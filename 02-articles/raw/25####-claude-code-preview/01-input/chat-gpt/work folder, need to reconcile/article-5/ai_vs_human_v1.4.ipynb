{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wtrekell/soylent_ops/blob/main/repository/notebooks/ai_vs_human_v1.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqu06J5MpdQS"
      },
      "outputs": [],
      "source": [
        "# MOUNT GOOGLE DRIVE\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "print(\"‚úì Drive mounted successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_8AIVCB-cME"
      },
      "outputs": [],
      "source": [
        "# CELL 1: SETUP AND CONFIGURATION\n",
        "\n",
        "# Core Python libraries\n",
        "import difflib\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data analysis libraries\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import seaborn as sns\n",
        "from matplotlib import patches\n",
        "\n",
        "# Machine learning and semantic analysis\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration settings for the analysis.\"\"\"\n",
        "\n",
        "    VERSION_PREFIXES = [\"draft-\", \"refined-\", \"edited-\", \"final-\"]\n",
        "    VERSION_ORDER = {prefix: i for i, prefix in enumerate(VERSION_PREFIXES)}\n",
        "\n",
        "    MIN_SENTENCE_LENGTH = 10  # Minimum characters for a sentence\n",
        "    MAX_SENTENCE_LENGTH = 200  # Maximum characters for a sentence\n",
        "\n",
        "\n",
        "def setup_output_directories(base_path):\n",
        "    \"\"\"Create necessary output directories.\"\"\"\n",
        "    output_dir = base_path\n",
        "    archive_dir = os.path.join(base_path, \"archive\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(archive_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"‚úì Output directory ready: {output_dir}\")\n",
        "    print(f\"‚úì Archive directory ready: {archive_dir}\")\n",
        "\n",
        "    return output_dir, archive_dir\n",
        "\n",
        "\n",
        "print(\"üìã Configuration loaded. Ready to process articles.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3ky_mO3Fr3D"
      },
      "outputs": [],
      "source": [
        "# CELL 2: DATA INGESTION & VALIDATION (STEP 1)\n",
        "\n",
        "\n",
        "class ArticleVersions:\n",
        "    \"\"\"Class to handle loading and validating article versions.\"\"\"\n",
        "\n",
        "    def __init__(self, article_name, input_path):\n",
        "        self.article_name = article_name\n",
        "        self.input_path = input_path\n",
        "        self.versions = {}\n",
        "        self.metadata = {\n",
        "            \"article_name\": article_name,\n",
        "            \"input_path\": input_path,\n",
        "            \"processing_timestamp\": datetime.now().isoformat(),\n",
        "            \"versions_found\": [],\n",
        "            \"validation_status\": \"pending\",\n",
        "        }\n",
        "\n",
        "    def load_versions(self):\n",
        "        \"\"\"Load all versions of an article from the specified path.\"\"\"\n",
        "        print(f\"\\nüìÅ Loading versions for article: {self.article_name}\")\n",
        "        print(f\"üìÇ Input path: {self.input_path}\")\n",
        "\n",
        "        for prefix in Config.VERSION_PREFIXES:\n",
        "            filename = f\"{prefix}{self.article_name}.md\"\n",
        "            filepath = os.path.join(self.input_path, filename)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                try:\n",
        "                    with open(filepath, encoding=\"utf-8\") as file:\n",
        "                        content = file.read()\n",
        "                        self.versions[prefix.rstrip(\"-\")] = {\n",
        "                            \"filename\": filename,\n",
        "                            \"filepath\": filepath,\n",
        "                            \"content\": content,\n",
        "                            \"loaded_at\": datetime.now().isoformat(),\n",
        "                            \"file_size\": len(content),\n",
        "                        }\n",
        "                        print(f\"  ‚úì Loaded: {filename} ({len(content)} characters)\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚úó Error loading {filename}: {e!s}\")\n",
        "            else:\n",
        "                print(f\"  - Not found: {filename}\")\n",
        "\n",
        "        self.metadata[\"versions_found\"] = list(self.versions.keys())\n",
        "        return self.versions\n",
        "\n",
        "    def validate_version_sequence(self):\n",
        "        \"\"\"Validate that we have the minimum required versions.\"\"\"\n",
        "        found_versions = set(self.versions.keys())\n",
        "        required_versions = [\"draft\", \"final\"]\n",
        "\n",
        "        missing_required = []\n",
        "        for version in required_versions:\n",
        "            if version not in found_versions:\n",
        "                missing_required.append(version)\n",
        "\n",
        "        validation_results = {\n",
        "            \"has_draft\": \"draft\" in found_versions,\n",
        "            \"has_final\": \"final\" in found_versions,\n",
        "            \"missing_required\": missing_required,\n",
        "            \"versions_found\": list(found_versions),\n",
        "            \"is_valid\": len(missing_required) == 0,\n",
        "        }\n",
        "\n",
        "        self.metadata[\"validation_results\"] = validation_results\n",
        "\n",
        "        if validation_results[\"is_valid\"]:\n",
        "            self.metadata[\"validation_status\"] = \"passed\"\n",
        "            print(f\"‚úì Validation passed: Found {len(found_versions)} versions\")\n",
        "        else:\n",
        "            self.metadata[\"validation_status\"] = \"failed\"\n",
        "            print(\"‚úó Validation failed: Missing required versions\")\n",
        "            print(f\"  Missing: {', '.join(missing_required)}\")\n",
        "\n",
        "        return validation_results\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"Get a summary of loaded versions.\"\"\"\n",
        "        summary = {\n",
        "            \"article_name\": self.article_name,\n",
        "            \"input_path\": self.input_path,\n",
        "            \"versions_count\": len(self.versions),\n",
        "            \"validation_status\": self.metadata[\"validation_status\"],\n",
        "            \"file_sizes\": {},\n",
        "        }\n",
        "\n",
        "        for version, data in self.versions.items():\n",
        "            summary[\"file_sizes\"][version] = data[\"file_size\"]\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "print(\"üìñ ArticleVersions class loaded. Ready for data ingestion.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhHY2pJqFvOr"
      },
      "outputs": [],
      "source": [
        "# CELL 3: TEXT PREPROCESSING (STEP 2)\n",
        "\n",
        "\n",
        "class TextPreprocessor:\n",
        "    \"\"\"Class to handle text preprocessing and segmentation.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.processed_versions = {}\n",
        "\n",
        "    def clean_markdown(self, text):\n",
        "        \"\"\"Clean markdown formatting while preserving content structure.\"\"\"\n",
        "        # Remove markdown formatting but keep the text\n",
        "\n",
        "        cleaned_text = text\n",
        "\n",
        "        # Apply patterns that need multiline flag\n",
        "        multiline_patterns = [\n",
        "            (r\"^\\s*[\\*\\-\\+]\\s+\", \"\"),  # Bullet points\n",
        "            (r\"^\\s*\\d+\\.\\s+\", \"\"),  # Numbered lists\n",
        "        ]\n",
        "\n",
        "        for pattern, replacement in multiline_patterns:\n",
        "            cleaned_text = re.sub(\n",
        "                pattern, replacement, cleaned_text, flags=re.MULTILINE\n",
        "            )\n",
        "\n",
        "        # Apply regular patterns\n",
        "        regular_patterns = [\n",
        "            (r\"^\\s*#{1,6}\\s+\", \"\"),  # Headers\n",
        "            (r\"\\*\\*(.*?)\\*\\*\", r\"\\1\"),  # Bold\n",
        "            (r\"\\*(.*?)\\*\", r\"\\1\"),  # Italic\n",
        "            (r\"`(.*?)`\", r\"\\1\"),  # Inline code\n",
        "            (r\"```.*?```\", \"\"),  # Code blocks\n",
        "            (r\"!\\[.*?\\]\\(.*?\\)\", \"\"),  # Images\n",
        "            (r\"\\[([^\\]]+)\\]\\([^\\)]+\\)\", r\"\\1\"),  # Links\n",
        "            (r\"\\n{3,}\", \"\\n\\n\"),  # Multiple newlines\n",
        "        ]\n",
        "\n",
        "        for pattern, replacement in regular_patterns:\n",
        "            cleaned_text = re.sub(pattern, replacement, cleaned_text)\n",
        "\n",
        "        return cleaned_text.strip()\n",
        "\n",
        "    def segment_into_sentences(self, text):\n",
        "        \"\"\"Segment text into sentences with basic filtering.\"\"\"\n",
        "        # Simple sentence segmentation (can be enhanced with spaCy later if needed)\n",
        "        sentences = re.split(r\"[.!?]+\\s+\", text)\n",
        "\n",
        "        # Filter sentences\n",
        "        filtered_sentences = []\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if (\n",
        "                Config.MIN_SENTENCE_LENGTH\n",
        "                <= len(sentence)\n",
        "                <= Config.MAX_SENTENCE_LENGTH\n",
        "                and sentence\n",
        "            ):\n",
        "                filtered_sentences.append(sentence)\n",
        "\n",
        "        return filtered_sentences\n",
        "\n",
        "    def segment_into_paragraphs(self, text):\n",
        "        \"\"\"Segment text into paragraphs.\"\"\"\n",
        "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "        return paragraphs\n",
        "\n",
        "    def process_version(self, version_name, raw_content):\n",
        "        \"\"\"Process a single version of the article.\"\"\"\n",
        "        print(f\"  Processing {version_name} version...\")\n",
        "\n",
        "        # Clean the markdown\n",
        "        cleaned_content = self.clean_markdown(raw_content)\n",
        "\n",
        "        # Segment into different units\n",
        "        sentences = self.segment_into_sentences(cleaned_content)\n",
        "        paragraphs = self.segment_into_paragraphs(cleaned_content)\n",
        "\n",
        "        # Calculate basic statistics\n",
        "        stats = {\n",
        "            \"character_count\": len(cleaned_content),\n",
        "            \"word_count\": len(cleaned_content.split()),\n",
        "            \"sentence_count\": len(sentences),\n",
        "            \"paragraph_count\": len(paragraphs),\n",
        "            \"avg_sentence_length\": sum(len(s) for s in sentences) / len(sentences)\n",
        "            if sentences\n",
        "            else 0,\n",
        "            \"avg_paragraph_length\": sum(len(p) for p in paragraphs) / len(paragraphs)\n",
        "            if paragraphs\n",
        "            else 0,\n",
        "        }\n",
        "\n",
        "        processed_data = {\n",
        "            \"version_name\": version_name,\n",
        "            \"raw_content\": raw_content,\n",
        "            \"cleaned_content\": cleaned_content,\n",
        "            \"sentences\": sentences,\n",
        "            \"paragraphs\": paragraphs,\n",
        "            \"statistics\": stats,\n",
        "            \"processed_at\": datetime.now().isoformat(),\n",
        "        }\n",
        "\n",
        "        self.processed_versions[version_name] = processed_data\n",
        "\n",
        "        print(\n",
        "            f\"    ‚úì {stats['sentence_count']} sentences, {stats['paragraph_count']} paragraphs\"\n",
        "        )\n",
        "        print(\n",
        "            f\"    ‚úì {stats['word_count']} words, {stats['character_count']} characters\"\n",
        "        )\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def process_all_versions(self, article_versions):\n",
        "        \"\"\"Process all versions of an article.\"\"\"\n",
        "        print(\"\\nüîÑ Preprocessing text for all versions...\")\n",
        "\n",
        "        for version_name, version_data in article_versions.versions.items():\n",
        "            self.process_version(version_name, version_data[\"content\"])\n",
        "\n",
        "        return self.processed_versions\n",
        "\n",
        "    def get_processing_summary(self):\n",
        "        \"\"\"Get a summary of processing results.\"\"\"\n",
        "        summary = {}\n",
        "        for version_name, data in self.processed_versions.items():\n",
        "            summary[version_name] = data[\"statistics\"]\n",
        "\n",
        "        return summary\n",
        "\n",
        "\n",
        "print(\"üîß TextPreprocessor class loaded. Ready for text processing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4bdYVJdFyQD"
      },
      "outputs": [],
      "source": [
        "# CELL 4: EXECUTION FUNCTIONS AND CHECKPOINT MANAGEMENT\n",
        "\n",
        "\n",
        "def save_checkpoint_data(\n",
        "    article_versions, preprocessor, output_path, checkpoint_name=\"steps_1_2\"\n",
        "):\n",
        "    \"\"\"Save checkpoint data for review.\"\"\"\n",
        "    checkpoint_data = {\n",
        "        \"checkpoint_name\": checkpoint_name,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"article_metadata\": article_versions.metadata,\n",
        "        \"processing_summary\": preprocessor.get_processing_summary(),\n",
        "        \"validation_results\": article_versions.metadata.get(\"validation_results\", {}),\n",
        "        \"article_summary\": article_versions.get_summary(),\n",
        "    }\n",
        "\n",
        "    checkpoint_file = f\"{output_path}/{article_versions.article_name}_checkpoint_{checkpoint_name}.json\"\n",
        "\n",
        "    with open(checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Checkpoint saved: {checkpoint_file}\")\n",
        "    return checkpoint_data\n",
        "\n",
        "\n",
        "def run_steps_1_2(article_name, input_path, base_output_path):\n",
        "    \"\"\"Run steps 1-2 for a given article.\"\"\"\n",
        "    print(f\"üöÄ Starting Steps 1-2 for article: {article_name}\")\n",
        "    print(f\"üìÇ Input path: {input_path}\")\n",
        "\n",
        "    output_dir, archive_dir = setup_output_directories(base_output_path)\n",
        "\n",
        "    # Step 1: Data Ingestion & Validation\n",
        "    article_versions = ArticleVersions(article_name, input_path)\n",
        "    article_versions.load_versions()\n",
        "    validation_results = article_versions.validate_version_sequence()\n",
        "\n",
        "    if not validation_results[\"is_valid\"]:\n",
        "        print(\"‚ùå Cannot proceed: Missing required versions (draft and final)\")\n",
        "        return None, None\n",
        "\n",
        "    # Step 2: Text Preprocessing\n",
        "    preprocessor = TextPreprocessor()\n",
        "    preprocessor.process_all_versions(article_versions)\n",
        "\n",
        "    # Save checkpoint\n",
        "    save_checkpoint_data(article_versions, preprocessor, output_dir)\n",
        "\n",
        "    print(\"\\n‚úÖ Steps 1-2 completed successfully!\")\n",
        "    print(\"üìä Processing Summary:\")\n",
        "    for version, stats in preprocessor.get_processing_summary().items():\n",
        "        print(\n",
        "            f\"  {version}: {stats['word_count']} words, {stats['sentence_count']} sentences\"\n",
        "        )\n",
        "\n",
        "    return article_versions, preprocessor\n",
        "\n",
        "\n",
        "def get_user_inputs():\n",
        "    \"\"\"Get user inputs for processing.\"\"\"\n",
        "    print(\"üìù Please provide the following information:\")\n",
        "\n",
        "    article_name = input(\"Enter article name (without .md extension): \").strip()\n",
        "    input_path = input(\n",
        "        \"Enter full path to input folder containing markdown files: \"\n",
        "    ).strip()\n",
        "    base_output_path = input(\"Enter full path to base output folder: \").strip()\n",
        "\n",
        "    print(\"\\nüìã Configuration:\")\n",
        "    print(f\"  Article name: {article_name}\")\n",
        "    print(f\"  Input path: {input_path}\")\n",
        "    print(f\"  Output path: {base_output_path}\")\n",
        "\n",
        "    confirm = input(\"\\nProceed with these settings? (y/n): \").strip().lower()\n",
        "\n",
        "    if confirm == \"y\":\n",
        "        return article_name, input_path, base_output_path\n",
        "    else:\n",
        "        print(\"‚ùå Cancelled. Run get_user_inputs() again to restart.\")\n",
        "        return None, None, None\n",
        "\n",
        "\n",
        "def process_article_interactive():\n",
        "    \"\"\"Process an article with interactive inputs.\"\"\"\n",
        "    article_name, input_path, base_output_path = get_user_inputs()\n",
        "\n",
        "    if article_name and input_path and base_output_path:\n",
        "        return run_steps_1_2(article_name, input_path, base_output_path)\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "print(\"üìã Ready to process your article!\")\n",
        "print(\"Run: article_versions, preprocessor = process_article_interactive()\")\n",
        "print(\"\\nMake sure your markdown files are named:\")\n",
        "print(\"- draft-your-article-name.md\")\n",
        "print(\"- refined-your-article-name.md\")\n",
        "print(\"- edited-your-article-name.md\")\n",
        "print(\"- final-your-article-name.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-hysVC7J3F0"
      },
      "outputs": [],
      "source": [
        "article_versions, preprocessor = process_article_interactive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGzjFLxSMguC"
      },
      "outputs": [],
      "source": [
        "# CELL 6: INSTALL ANALYSIS PACKAGES\n",
        "\n",
        "print(\"üì¶ Dependencies installed and imported successfully!\")\n",
        "print(\"ü§ñ Loading SentenceTransformer model (this may take a moment)...\")\n",
        "\n",
        "# Load the semantic similarity model\n",
        "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"‚úÖ Semantic model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0OQ62SiMjnr"
      },
      "outputs": [],
      "source": [
        "# CELL 7: SIMILARITY ANALYSIS\n",
        "\n",
        "\n",
        "class SimilarityAnalyzer:\n",
        "    \"\"\"Class to handle lexical and semantic similarity analysis.\"\"\"\n",
        "\n",
        "    def __init__(self, semantic_model):\n",
        "        self.semantic_model = semantic_model\n",
        "        self.similarity_results = {}\n",
        "\n",
        "    def calculate_lexical_similarity(self, text1, text2):\n",
        "        \"\"\"Calculate lexical similarity using multiple metrics.\"\"\"\n",
        "        # Jaccard similarity (word-level)\n",
        "        words1 = set(text1.lower().split())\n",
        "        words2 = set(text2.lower().split())\n",
        "        jaccard = (\n",
        "            len(words1.intersection(words2)) / len(words1.union(words2))\n",
        "            if words1.union(words2)\n",
        "            else 0\n",
        "        )\n",
        "\n",
        "        # Edit distance similarity (character-level)\n",
        "        sequence_matcher = difflib.SequenceMatcher(None, text1, text2)\n",
        "        edit_similarity = sequence_matcher.ratio()\n",
        "\n",
        "        # TF-IDF cosine similarity\n",
        "        vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "            tfidf_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[\n",
        "                0\n",
        "            ][0]\n",
        "        except:\n",
        "            tfidf_similarity = 0.0\n",
        "\n",
        "        return {\n",
        "            \"jaccard_similarity\": jaccard,\n",
        "            \"edit_similarity\": edit_similarity,\n",
        "            \"tfidf_similarity\": tfidf_similarity,\n",
        "            \"lexical_average\": (jaccard + edit_similarity + tfidf_similarity) / 3,\n",
        "        }\n",
        "\n",
        "    def calculate_semantic_similarity(self, text1, text2):\n",
        "        \"\"\"Calculate semantic similarity using sentence embeddings.\"\"\"\n",
        "        # Get embeddings\n",
        "        embeddings = self.semantic_model.encode([text1, text2])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "        return {\n",
        "            \"semantic_similarity\": float(similarity),\n",
        "            \"embedding_dim\": len(embeddings[0]),\n",
        "        }\n",
        "\n",
        "    def calculate_sentence_level_similarities(\n",
        "        self, sentences1, sentences2, version1, version2\n",
        "    ):\n",
        "        \"\"\"Calculate similarities at sentence level between two versions.\"\"\"\n",
        "        print(f\"    Analyzing {len(sentences1)} vs {len(sentences2)} sentences...\")\n",
        "\n",
        "        sentence_similarities = []\n",
        "\n",
        "        # Calculate all pairwise similarities\n",
        "        for i, sent1 in enumerate(sentences1):\n",
        "            best_match = {\"index\": -1, \"lexical\": 0, \"semantic\": 0, \"combined\": 0}\n",
        "\n",
        "            for j, sent2 in enumerate(sentences2):\n",
        "                # Calculate similarities\n",
        "                lexical = self.calculate_lexical_similarity(sent1, sent2)\n",
        "                semantic = self.calculate_semantic_similarity(sent1, sent2)\n",
        "\n",
        "                # Combined score (weighted average)\n",
        "                combined = (\n",
        "                    lexical[\"lexical_average\"] + semantic[\"semantic_similarity\"]\n",
        "                ) / 2\n",
        "\n",
        "                if combined > best_match[\"combined\"]:\n",
        "                    best_match = {\n",
        "                        \"index\": j,\n",
        "                        \"lexical\": lexical[\"lexical_average\"],\n",
        "                        \"semantic\": semantic[\"semantic_similarity\"],\n",
        "                        \"combined\": combined,\n",
        "                        \"target_sentence\": sent2,\n",
        "                    }\n",
        "\n",
        "            sentence_similarities.append(\n",
        "                {\"source_index\": i, \"source_sentence\": sent1, \"best_match\": best_match}\n",
        "            )\n",
        "\n",
        "        return sentence_similarities\n",
        "\n",
        "    def analyze_version_pair(\n",
        "        self, version1_data, version2_data, version1_name, version2_name\n",
        "    ):\n",
        "        \"\"\"Analyze similarities between two versions.\"\"\"\n",
        "        print(f\"  üîç Analyzing {version1_name} ‚Üí {version2_name}\")\n",
        "\n",
        "        # Full text similarity\n",
        "        full_text_lexical = self.calculate_lexical_similarity(\n",
        "            version1_data[\"cleaned_content\"], version2_data[\"cleaned_content\"]\n",
        "        )\n",
        "        full_text_semantic = self.calculate_semantic_similarity(\n",
        "            version1_data[\"cleaned_content\"], version2_data[\"cleaned_content\"]\n",
        "        )\n",
        "\n",
        "        # Sentence-level analysis\n",
        "        sentence_analysis = self.calculate_sentence_level_similarities(\n",
        "            version1_data[\"sentences\"],\n",
        "            version2_data[\"sentences\"],\n",
        "            version1_name,\n",
        "            version2_name,\n",
        "        )\n",
        "\n",
        "        # Paragraph-level similarity\n",
        "        para_lexical = self.calculate_lexical_similarity(\n",
        "            \" \".join(version1_data[\"paragraphs\"]), \" \".join(version2_data[\"paragraphs\"])\n",
        "        )\n",
        "        para_semantic = self.calculate_semantic_similarity(\n",
        "            \" \".join(version1_data[\"paragraphs\"]), \" \".join(version2_data[\"paragraphs\"])\n",
        "        )\n",
        "\n",
        "        # Aggregate sentence similarities\n",
        "        sentence_similarities = [s[\"best_match\"][\"combined\"] for s in sentence_analysis]\n",
        "        avg_sentence_similarity = (\n",
        "            np.mean(sentence_similarities) if sentence_similarities else 0\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"version_pair\": f\"{version1_name}_to_{version2_name}\",\n",
        "            \"full_text\": {\n",
        "                \"lexical\": full_text_lexical,\n",
        "                \"semantic\": full_text_semantic,\n",
        "                \"combined\": (\n",
        "                    full_text_lexical[\"lexical_average\"]\n",
        "                    + full_text_semantic[\"semantic_similarity\"]\n",
        "                )\n",
        "                / 2,\n",
        "            },\n",
        "            \"sentence_level\": {\n",
        "                \"average_similarity\": avg_sentence_similarity,\n",
        "                \"individual_similarities\": sentence_similarities,\n",
        "                \"detailed_analysis\": sentence_analysis,\n",
        "            },\n",
        "            \"paragraph_level\": {\n",
        "                \"lexical\": para_lexical,\n",
        "                \"semantic\": para_semantic,\n",
        "                \"combined\": (\n",
        "                    para_lexical[\"lexical_average\"]\n",
        "                    + para_semantic[\"semantic_similarity\"]\n",
        "                )\n",
        "                / 2,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def analyze_all_versions(self, processed_versions):\n",
        "        \"\"\"Analyze similarities between all version pairs.\"\"\"\n",
        "        print(\"\\nüîç Step 3: Similarity Analysis\")\n",
        "\n",
        "        version_names = list(processed_versions.keys())\n",
        "        version_order = [\"draft\", \"refined\", \"edited\", \"final\"]\n",
        "\n",
        "        # Sort versions by expected order\n",
        "        sorted_versions = []\n",
        "        for expected in version_order:\n",
        "            if expected in version_names:\n",
        "                sorted_versions.append(expected)\n",
        "\n",
        "        # Sequential analysis (draft‚Üírefined‚Üíedited‚Üífinal)\n",
        "        sequential_results = []\n",
        "        for i in range(len(sorted_versions) - 1):\n",
        "            current_version = sorted_versions[i]\n",
        "            next_version = sorted_versions[i + 1]\n",
        "\n",
        "            result = self.analyze_version_pair(\n",
        "                processed_versions[current_version],\n",
        "                processed_versions[next_version],\n",
        "                current_version,\n",
        "                next_version,\n",
        "            )\n",
        "            sequential_results.append(result)\n",
        "\n",
        "        # Draft to final comparison\n",
        "        draft_to_final = None\n",
        "        if \"draft\" in version_names and \"final\" in version_names:\n",
        "            print(\"  üîç Analyzing draft ‚Üí final (overall change)\")\n",
        "            draft_to_final = self.analyze_version_pair(\n",
        "                processed_versions[\"draft\"],\n",
        "                processed_versions[\"final\"],\n",
        "                \"draft\",\n",
        "                \"final\",\n",
        "            )\n",
        "\n",
        "        self.similarity_results = {\n",
        "            \"sequential_analysis\": sequential_results,\n",
        "            \"draft_to_final\": draft_to_final,\n",
        "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "            \"versions_analyzed\": sorted_versions,\n",
        "        }\n",
        "\n",
        "        return self.similarity_results\n",
        "\n",
        "\n",
        "print(\"üîç SimilarityAnalyzer class loaded. Ready for similarity analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIDJ9C50MrQ3"
      },
      "outputs": [],
      "source": [
        "# CELL 8: ATTRIBUTION MAPPING (STEP 4)\n",
        "\n",
        "\n",
        "class AttributionMapper:\n",
        "    \"\"\"Class to track content attribution across versions.\"\"\"\n",
        "\n",
        "    def __init__(self, semantic_model, similarity_threshold=0.3):\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.semantic_model = semantic_model\n",
        "        self.attribution_results = {}\n",
        "\n",
        "    def trace_sentence_origins(self, processed_versions, similarity_results):\n",
        "        \"\"\"Trace each final sentence back to its earliest appearance.\"\"\"\n",
        "        print(\"\\nüìç Step 4: Attribution Mapping\")\n",
        "\n",
        "        version_order = [\"draft\", \"refined\", \"edited\", \"final\"]\n",
        "        available_versions = [v for v in version_order if v in processed_versions]\n",
        "\n",
        "        if \"final\" not in available_versions:\n",
        "            print(\"‚ùå Cannot perform attribution - final version not found\")\n",
        "            return None\n",
        "\n",
        "        final_sentences = processed_versions[\"final\"][\"sentences\"]\n",
        "        sentence_attributions = []\n",
        "\n",
        "        print(f\"  üìù Tracing {len(final_sentences)} final sentences...\")\n",
        "\n",
        "        for final_idx, final_sentence in enumerate(final_sentences):\n",
        "            attribution = {\n",
        "                \"final_index\": final_idx,\n",
        "                \"final_sentence\": final_sentence,\n",
        "                \"origin_version\": None,\n",
        "                \"origin_index\": None,\n",
        "                \"similarity_scores\": {},\n",
        "                \"modification_path\": [],\n",
        "            }\n",
        "\n",
        "            # Check each previous version (in reverse order to find earliest origin)\n",
        "            for version in reversed(available_versions[:-1]):  # Exclude 'final'\n",
        "                version_sentences = processed_versions[version][\"sentences\"]\n",
        "\n",
        "                best_match = {\"index\": -1, \"similarity\": 0, \"sentence\": \"\"}\n",
        "\n",
        "                for sent_idx, version_sentence in enumerate(version_sentences):\n",
        "                    # Calculate similarity\n",
        "                    lexical = self._quick_lexical_similarity(\n",
        "                        final_sentence, version_sentence\n",
        "                    )\n",
        "                    semantic = self._quick_semantic_similarity(\n",
        "                        final_sentence, version_sentence\n",
        "                    )\n",
        "                    combined = (lexical + semantic) / 2\n",
        "\n",
        "                    if combined > best_match[\"similarity\"]:\n",
        "                        best_match = {\n",
        "                            \"index\": sent_idx,\n",
        "                            \"similarity\": combined,\n",
        "                            \"sentence\": version_sentence,\n",
        "                        }\n",
        "\n",
        "                attribution[\"similarity_scores\"][version] = best_match[\"similarity\"]\n",
        "\n",
        "                # If similarity is above threshold, this could be the origin\n",
        "                if best_match[\"similarity\"] >= self.similarity_threshold:\n",
        "                    if (\n",
        "                        attribution[\"origin_version\"] is None\n",
        "                    ):  # First match found (earliest version)\n",
        "                        attribution[\"origin_version\"] = version\n",
        "                        attribution[\"origin_index\"] = best_match[\"index\"]\n",
        "\n",
        "                    attribution[\"modification_path\"].append(\n",
        "                        {\n",
        "                            \"version\": version,\n",
        "                            \"similarity\": best_match[\"similarity\"],\n",
        "                            \"sentence\": best_match[\"sentence\"],\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "            # If no origin found, it's new content\n",
        "            if attribution[\"origin_version\"] is None:\n",
        "                attribution[\"origin_version\"] = \"new_in_final\"\n",
        "\n",
        "            sentence_attributions.append(attribution)\n",
        "\n",
        "        return sentence_attributions\n",
        "\n",
        "    def _quick_lexical_similarity(self, text1, text2):\n",
        "        \"\"\"Quick lexical similarity calculation.\"\"\"\n",
        "        words1 = set(text1.lower().split())\n",
        "        words2 = set(text2.lower().split())\n",
        "        if not words1 and not words2:\n",
        "            return 1.0\n",
        "        if not words1 or not words2:\n",
        "            return 0.0\n",
        "        return len(words1.intersection(words2)) / len(words1.union(words2))\n",
        "\n",
        "    def _quick_semantic_similarity(self, text1, text2):\n",
        "        \"\"\"Quick semantic similarity calculation.\"\"\"\n",
        "        embeddings = self.semantic_model.encode([text1, text2])\n",
        "        return float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])\n",
        "\n",
        "    def calculate_attribution_statistics(self, sentence_attributions):\n",
        "        \"\"\"Calculate overall attribution statistics.\"\"\"\n",
        "        total_sentences = len(sentence_attributions)\n",
        "\n",
        "        # Count by origin version\n",
        "        origin_counts = defaultdict(int)\n",
        "        for attribution in sentence_attributions:\n",
        "            origin_counts[attribution[\"origin_version\"]] += 1\n",
        "\n",
        "        # Calculate percentages\n",
        "        origin_percentages = {}\n",
        "        for version, count in origin_counts.items():\n",
        "            origin_percentages[version] = {\n",
        "                \"count\": count,\n",
        "                \"percentage\": (count / total_sentences) * 100,\n",
        "            }\n",
        "\n",
        "        # Calculate modification statistics\n",
        "        modification_stats = {\n",
        "            \"high_similarity\": 0,  # >0.8\n",
        "            \"medium_similarity\": 0,  # 0.5-0.8\n",
        "            \"low_similarity\": 0,  # 0.3-0.5\n",
        "            \"new_content\": 0,  # <0.3 or new_in_final\n",
        "        }\n",
        "\n",
        "        for attribution in sentence_attributions:\n",
        "            if attribution[\"origin_version\"] == \"new_in_final\":\n",
        "                modification_stats[\"new_content\"] += 1\n",
        "            else:\n",
        "                # Get highest similarity score\n",
        "                max_similarity = (\n",
        "                    max(attribution[\"similarity_scores\"].values())\n",
        "                    if attribution[\"similarity_scores\"]\n",
        "                    else 0\n",
        "                )\n",
        "\n",
        "                if max_similarity > 0.8:\n",
        "                    modification_stats[\"high_similarity\"] += 1\n",
        "                elif max_similarity > 0.5:\n",
        "                    modification_stats[\"medium_similarity\"] += 1\n",
        "                elif max_similarity > 0.3:\n",
        "                    modification_stats[\"low_similarity\"] += 1\n",
        "                else:\n",
        "                    modification_stats[\"new_content\"] += 1\n",
        "\n",
        "        # Convert to percentages\n",
        "        modification_percentages = {}\n",
        "        for category, count in modification_stats.items():\n",
        "            modification_percentages[category] = {\n",
        "                \"count\": count,\n",
        "                \"percentage\": (count / total_sentences) * 100,\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            \"total_sentences\": total_sentences,\n",
        "            \"origin_distribution\": origin_percentages,\n",
        "            \"modification_distribution\": modification_percentages,\n",
        "        }\n",
        "\n",
        "    def analyze_attribution(self, processed_versions, similarity_results):\n",
        "        \"\"\"Perform complete attribution analysis.\"\"\"\n",
        "        sentence_attributions = self.trace_sentence_origins(\n",
        "            processed_versions, similarity_results\n",
        "        )\n",
        "\n",
        "        if sentence_attributions is None:\n",
        "            return None\n",
        "\n",
        "        attribution_statistics = self.calculate_attribution_statistics(\n",
        "            sentence_attributions\n",
        "        )\n",
        "\n",
        "        self.attribution_results = {\n",
        "            \"sentence_attributions\": sentence_attributions,\n",
        "            \"statistics\": attribution_statistics,\n",
        "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "            \"similarity_threshold\": self.similarity_threshold,\n",
        "        }\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nüìä Attribution Summary:\")\n",
        "        print(\n",
        "            f\"  Total sentences in final: {attribution_statistics['total_sentences']}\"\n",
        "        )\n",
        "\n",
        "        print(\"\\n  Origin Distribution:\")\n",
        "        for version, data in attribution_statistics[\"origin_distribution\"].items():\n",
        "            print(\n",
        "                f\"    {version}: {data['count']} sentences ({data['percentage']:.1f}%)\"\n",
        "            )\n",
        "\n",
        "        print(\"\\n  Modification Levels:\")\n",
        "        for category, data in attribution_statistics[\n",
        "            \"modification_distribution\"\n",
        "        ].items():\n",
        "            print(\n",
        "                f\"    {category}: {data['count']} sentences ({data['percentage']:.1f}%)\"\n",
        "            )\n",
        "\n",
        "        return self.attribution_results\n",
        "\n",
        "\n",
        "print(\"üìç AttributionMapper class loaded. Ready for attribution analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbD5vnmTMvo5"
      },
      "outputs": [],
      "source": [
        "# CELL 9: COMBINED EXECUTION FUNCTION\n",
        "\n",
        "\n",
        "def run_steps_3_4(article_versions, preprocessor, output_path):\n",
        "    \"\"\"Run steps 3-4: Similarity Analysis and Attribution Mapping.\"\"\"\n",
        "    print(f\"üöÄ Starting Steps 3-4 for article: {article_versions.article_name}\")\n",
        "\n",
        "    # Step 3: Similarity Analysis\n",
        "    similarity_analyzer = SimilarityAnalyzer(semantic_model)\n",
        "    similarity_results = similarity_analyzer.analyze_all_versions(\n",
        "        preprocessor.processed_versions\n",
        "    )\n",
        "\n",
        "    # Step 4: Attribution Mapping\n",
        "    attribution_mapper = AttributionMapper(semantic_model, similarity_threshold=0.3)\n",
        "    attribution_results = attribution_mapper.analyze_attribution(\n",
        "        preprocessor.processed_versions, similarity_results\n",
        "    )\n",
        "\n",
        "    # Combine results\n",
        "    combined_results = {\n",
        "        \"article_name\": article_versions.article_name,\n",
        "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "        \"article_metadata\": article_versions.metadata,\n",
        "        \"processing_summary\": preprocessor.get_processing_summary(),\n",
        "        \"similarity_analysis\": similarity_results,\n",
        "        \"attribution_analysis\": attribution_results,\n",
        "    }\n",
        "\n",
        "    # Save comprehensive results\n",
        "    results_file = (\n",
        "        f\"{output_path}/{article_versions.article_name}_complete_analysis.json\"\n",
        "    )\n",
        "    with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(combined_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Complete analysis saved: {results_file}\")\n",
        "\n",
        "    # Generate summary metrics for article footer\n",
        "    footer_metrics = generate_footer_metrics(combined_results)\n",
        "\n",
        "    # Save footer metrics separately\n",
        "    footer_file = f\"{output_path}/{article_versions.article_name}_footer_metrics.json\"\n",
        "    with open(footer_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(footer_metrics, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"üìä Footer metrics saved: {footer_file}\")\n",
        "\n",
        "    return combined_results, footer_metrics\n",
        "\n",
        "\n",
        "def generate_footer_metrics(combined_results):\n",
        "    \"\"\"Generate clean metrics for article footer.\"\"\"\n",
        "    # Get processing stats\n",
        "    processing = combined_results[\"processing_summary\"]\n",
        "\n",
        "    # Get attribution stats\n",
        "    if combined_results[\"attribution_analysis\"]:\n",
        "        attribution = combined_results[\"attribution_analysis\"][\"statistics\"]\n",
        "        origin_dist = attribution[\"origin_distribution\"]\n",
        "        modification_dist = attribution[\"modification_distribution\"]\n",
        "    else:\n",
        "        origin_dist = {}\n",
        "        modification_dist = {}\n",
        "\n",
        "    # Get similarity stats (draft to final)\n",
        "    draft_to_final = combined_results[\"similarity_analysis\"][\"draft_to_final\"]\n",
        "    overall_similarity = (\n",
        "        draft_to_final[\"full_text\"][\"combined\"] if draft_to_final else 0\n",
        "    )\n",
        "\n",
        "    footer_metrics = {\n",
        "        \"article_name\": combined_results[\"article_name\"],\n",
        "        \"word_progression\": {\n",
        "            \"draft\": processing.get(\"draft\", {}).get(\"word_count\", 0),\n",
        "            \"final\": processing.get(\"final\", {}).get(\"word_count\", 0),\n",
        "            \"change_percentage\": 0,\n",
        "        },\n",
        "        \"content_retention\": {\n",
        "            \"overall_similarity\": round(overall_similarity * 100, 1),\n",
        "            \"content_origins\": {},\n",
        "        },\n",
        "        \"modification_summary\": {},\n",
        "        \"generated_at\": datetime.now().isoformat(),\n",
        "    }\n",
        "\n",
        "    # Calculate word change percentage\n",
        "    if footer_metrics[\"word_progression\"][\"draft\"] > 0:\n",
        "        draft_words = footer_metrics[\"word_progression\"][\"draft\"]\n",
        "        final_words = footer_metrics[\"word_progression\"][\"final\"]\n",
        "        change = ((final_words - draft_words) / draft_words) * 100\n",
        "        footer_metrics[\"word_progression\"][\"change_percentage\"] = round(change, 1)\n",
        "\n",
        "    # Simplify origin distribution for footer\n",
        "    for version, data in origin_dist.items():\n",
        "        if version != \"new_in_final\":\n",
        "            footer_metrics[\"content_retention\"][\"content_origins\"][version] = round(\n",
        "                data[\"percentage\"], 1\n",
        "            )\n",
        "\n",
        "    # Simplify modification distribution\n",
        "    for category, data in modification_dist.items():\n",
        "        clean_category = category.replace(\"_\", \" \").title()\n",
        "        footer_metrics[\"modification_summary\"][clean_category] = round(\n",
        "            data[\"percentage\"], 1\n",
        "        )\n",
        "\n",
        "    return footer_metrics\n",
        "\n",
        "\n",
        "print(\"üéØ Execution functions loaded. Ready to run complete analysis!\")\n",
        "print(\"\\nTo run the complete analysis:\")\n",
        "print(\n",
        "    \"combined_results, footer_metrics = run_steps_3_4(article_versions, preprocessor, 'your_output_path')\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7yqzZneMzgn"
      },
      "outputs": [],
      "source": [
        "# CELL 10: QUICK EXECUTION FOR EXISTING DATA\n",
        "\n",
        "\n",
        "def run_complete_analysis_from_existing(article_versions, preprocessor):\n",
        "    \"\"\"Run steps 3-4 using the output path from existing data.\"\"\"\n",
        "    # Find where the Step 1-2 checkpoint was actually saved\n",
        "    base_path = article_versions.input_path\n",
        "\n",
        "    # Check for the nested output structure that was created in Steps 1-2\n",
        "    nested_output_path = os.path.join(base_path, \"output\", \"output\")\n",
        "    regular_output_path = os.path.join(base_path, \"output\")\n",
        "\n",
        "    # Use the path where the checkpoint file exists\n",
        "    checkpoint_file = \"markup-languages_checkpoint_steps_1_2.json\"\n",
        "\n",
        "    if os.path.exists(os.path.join(nested_output_path, checkpoint_file)):\n",
        "        output_path = nested_output_path\n",
        "        print(f\"üìÇ Using nested output path: {output_path}\")\n",
        "    elif os.path.exists(os.path.join(regular_output_path, checkpoint_file)):\n",
        "        output_path = regular_output_path\n",
        "        print(f\"üìÇ Using regular output path: {output_path}\")\n",
        "    else:\n",
        "        # Create regular output path as fallback\n",
        "        output_path = regular_output_path\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        print(f\"üìÇ Created output path: {output_path}\")\n",
        "\n",
        "    return run_steps_3_4(article_versions, preprocessor, output_path)\n",
        "\n",
        "\n",
        "print(\"‚ö° Quick execution function available:\")\n",
        "print(\n",
        "    \"combined_results, footer_metrics = run_complete_analysis_from_existing(article_versions, preprocessor)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYliXyeZM17X"
      },
      "outputs": [],
      "source": [
        "combined_results, footer_metrics = run_complete_analysis_from_existing(\n",
        "    article_versions, preprocessor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vnKEAt-eglI"
      },
      "outputs": [],
      "source": [
        "# CELL 11: TREND ANALYSIS SETUP\n",
        "\n",
        "ARCHIVE_BASE_PATH = \"/content/drive/MyDrive/Google Drive/syntaxandempathy/99-past/\"\n",
        "\n",
        "\n",
        "class TrendAnalyzer:\n",
        "    \"\"\"Class to analyze trends across multiple articles over time periods.\"\"\"\n",
        "\n",
        "    def __init__(self, archive_path=ARCHIVE_BASE_PATH):\n",
        "        self.archive_path = archive_path\n",
        "        self.found_articles = []\n",
        "        self.period_data = {}\n",
        "        self.trend_results = {}\n",
        "\n",
        "    def parse_period(self, period):\n",
        "        \"\"\"Parse period string into date patterns.\"\"\"\n",
        "        period_patterns = []\n",
        "\n",
        "        if len(period) == 4:  # Year: \"2025\"\n",
        "            # All months in the year: 202501*, 202502*, etc.\n",
        "            for month in range(1, 13):\n",
        "                pattern = f\"{period}{month:02d}*\"\n",
        "                period_patterns.append(pattern)\n",
        "\n",
        "        elif len(period) == 7 and period[4] == \"-\":  # Month: \"2025-01\"\n",
        "            year, month = period.split(\"-\")\n",
        "            pattern = f\"{year}{month}*\"  # e.g., \"202501*\"\n",
        "            period_patterns.append(pattern)\n",
        "\n",
        "        elif period.endswith((\"Q1\", \"Q2\", \"Q3\", \"Q4\")):  # Quarter: \"2025-Q1\"\n",
        "            year, quarter = period.split(\"-\")\n",
        "            quarter_months = {\n",
        "                \"Q1\": [\"01\", \"02\", \"03\"],\n",
        "                \"Q2\": [\"04\", \"05\", \"06\"],\n",
        "                \"Q3\": [\"07\", \"08\", \"09\"],\n",
        "                \"Q4\": [\"10\", \"11\", \"12\"],\n",
        "            }\n",
        "\n",
        "            for month in quarter_months[quarter]:\n",
        "                pattern = f\"{year}{month}*\"  # e.g., \"202501*\", \"202502*\", \"202503*\"\n",
        "                period_patterns.append(pattern)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Invalid period format: {period}. Use 'YYYY', 'YYYY-MM', or 'YYYY-Q#'\"\n",
        "            )\n",
        "\n",
        "        return period_patterns\n",
        "\n",
        "    def find_articles_for_period(self, period):\n",
        "        \"\"\"Find all articles matching the period.\"\"\"\n",
        "        print(f\"üîç Searching for articles in period: {period}\")\n",
        "        print(f\"üìÇ Archive path: {self.archive_path}\")\n",
        "\n",
        "        if not os.path.exists(self.archive_path):\n",
        "            print(f\"‚ùå Archive path does not exist: {self.archive_path}\")\n",
        "            return []\n",
        "\n",
        "        patterns = self.parse_period(period)\n",
        "        found_articles = []\n",
        "\n",
        "        # Get all directories in archive\n",
        "        all_dirs = [\n",
        "            d\n",
        "            for d in os.listdir(self.archive_path)\n",
        "            if os.path.isdir(os.path.join(self.archive_path, d))\n",
        "        ]\n",
        "\n",
        "        print(f\"üìÅ Found {len(all_dirs)} directories in archive\")\n",
        "\n",
        "        # Match directories against patterns\n",
        "        for pattern in patterns:\n",
        "            # Convert shell pattern to regex: 202506* becomes ^202506.*\n",
        "            pattern_regex = f\"^{pattern.replace('*', '.*')}\"\n",
        "            regex = re.compile(pattern_regex)\n",
        "\n",
        "            for dir_name in all_dirs:\n",
        "                if regex.match(dir_name):\n",
        "                    article_path = os.path.join(self.archive_path, dir_name)\n",
        "\n",
        "                    # Look for analysis file\n",
        "                    analysis_files = glob.glob(\n",
        "                        os.path.join(article_path, \"output\", \"*_complete_analysis.json\")\n",
        "                    )\n",
        "\n",
        "                    if analysis_files:\n",
        "                        # Avoid duplicates\n",
        "                        if not any(\n",
        "                            item[\"folder_name\"] == dir_name for item in found_articles\n",
        "                        ):\n",
        "                            found_articles.append(\n",
        "                                {\n",
        "                                    \"folder_name\": dir_name,\n",
        "                                    \"article_path\": article_path,\n",
        "                                    \"analysis_file\": analysis_files[0],\n",
        "                                    \"date_prefix\": dir_name[:8],  # Extract YYYYMMDD\n",
        "                                    \"article_name\": dir_name[\n",
        "                                        9:\n",
        "                                    ],  # Extract name after date-\n",
        "                                }\n",
        "                            )\n",
        "                            print(f\"  ‚úì Found: {dir_name} (matches {pattern})\")\n",
        "                    else:\n",
        "                        print(f\"  ‚ö† No analysis file found in: {dir_name}\")\n",
        "\n",
        "        found_articles.sort(key=lambda x: x[\"date_prefix\"])  # Sort by date\n",
        "        self.found_articles = found_articles\n",
        "\n",
        "        print(f\"üìä Total articles found for {period}: {len(found_articles)}\")\n",
        "        return found_articles\n",
        "\n",
        "    def load_article_data(self, article_info):\n",
        "        \"\"\"Load analysis data for a single article.\"\"\"\n",
        "        try:\n",
        "            with open(article_info[\"analysis_file\"], encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Add metadata\n",
        "            data[\"folder_name\"] = article_info[\"folder_name\"]\n",
        "            data[\"publication_date\"] = article_info[\"date_prefix\"]\n",
        "\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {article_info['analysis_file']}: {e!s}\")\n",
        "            return None\n",
        "\n",
        "    def aggregate_attribution_trends(self, articles_data):\n",
        "        \"\"\"Aggregate attribution data across articles.\"\"\"\n",
        "        attribution_trends = {\n",
        "            \"by_article\": [],\n",
        "            \"averages\": {},\n",
        "            \"totals\": {},\n",
        "            \"trends_over_time\": [],\n",
        "        }\n",
        "\n",
        "        # Collect data for each article\n",
        "        for article in articles_data:\n",
        "            if not article or \"attribution_analysis\" not in article:\n",
        "                continue\n",
        "\n",
        "            attr_stats = article[\"attribution_analysis\"][\"statistics\"]\n",
        "            origin_dist = attr_stats[\"origin_distribution\"]\n",
        "            mod_dist = attr_stats[\"modification_distribution\"]\n",
        "\n",
        "            article_data = {\n",
        "                \"article_name\": article[\"article_name\"],\n",
        "                \"publication_date\": article[\"publication_date\"],\n",
        "                \"total_sentences\": attr_stats[\"total_sentences\"],\n",
        "                \"origin_percentages\": {},\n",
        "                \"modification_percentages\": {},\n",
        "            }\n",
        "\n",
        "            # Extract origin percentages\n",
        "            for version, data in origin_dist.items():\n",
        "                article_data[\"origin_percentages\"][version] = data[\"percentage\"]\n",
        "\n",
        "            # Extract modification percentages\n",
        "            for category, data in mod_dist.items():\n",
        "                article_data[\"modification_percentages\"][category] = data[\"percentage\"]\n",
        "\n",
        "            attribution_trends[\"by_article\"].append(article_data)\n",
        "\n",
        "        # Calculate averages across all articles\n",
        "        if attribution_trends[\"by_article\"]:\n",
        "            all_origins = set()\n",
        "            all_modifications = set()\n",
        "\n",
        "            for article in attribution_trends[\"by_article\"]:\n",
        "                all_origins.update(article[\"origin_percentages\"].keys())\n",
        "                all_modifications.update(article[\"modification_percentages\"].keys())\n",
        "\n",
        "            # Average origin percentages\n",
        "            for origin in all_origins:\n",
        "                values = [\n",
        "                    a[\"origin_percentages\"].get(origin, 0)\n",
        "                    for a in attribution_trends[\"by_article\"]\n",
        "                ]\n",
        "                attribution_trends[\"averages\"][f\"origin_{origin}\"] = np.mean(values)\n",
        "\n",
        "            # Average modification percentages\n",
        "            for mod in all_modifications:\n",
        "                values = [\n",
        "                    a[\"modification_percentages\"].get(mod, 0)\n",
        "                    for a in attribution_trends[\"by_article\"]\n",
        "                ]\n",
        "                attribution_trends[\"averages\"][f\"modification_{mod}\"] = np.mean(values)\n",
        "\n",
        "        return attribution_trends\n",
        "\n",
        "    def aggregate_similarity_trends(self, articles_data):\n",
        "        \"\"\"Aggregate similarity data across articles.\"\"\"\n",
        "        similarity_trends = {\n",
        "            \"draft_to_final\": [],\n",
        "            \"sequential_changes\": [],\n",
        "            \"averages\": {},\n",
        "        }\n",
        "\n",
        "        for article in articles_data:\n",
        "            if not article or \"similarity_analysis\" not in article:\n",
        "                continue\n",
        "\n",
        "            sim_analysis = article[\"similarity_analysis\"]\n",
        "\n",
        "            # Draft to final similarity\n",
        "            if sim_analysis[\"draft_to_final\"]:\n",
        "                draft_final = sim_analysis[\"draft_to_final\"][\"full_text\"]\n",
        "                similarity_trends[\"draft_to_final\"].append(\n",
        "                    {\n",
        "                        \"article_name\": article[\"article_name\"],\n",
        "                        \"publication_date\": article[\"publication_date\"],\n",
        "                        \"lexical_similarity\": draft_final[\"lexical\"][\"lexical_average\"],\n",
        "                        \"semantic_similarity\": draft_final[\"semantic\"][\n",
        "                            \"semantic_similarity\"\n",
        "                        ],\n",
        "                        \"combined_similarity\": draft_final[\"combined\"],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            # Sequential changes\n",
        "            for seq in sim_analysis[\"sequential_analysis\"]:\n",
        "                similarity_trends[\"sequential_changes\"].append(\n",
        "                    {\n",
        "                        \"article_name\": article[\"article_name\"],\n",
        "                        \"publication_date\": article[\"publication_date\"],\n",
        "                        \"version_pair\": seq[\"version_pair\"],\n",
        "                        \"combined_similarity\": seq[\"full_text\"][\"combined\"],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        # Calculate averages\n",
        "        if similarity_trends[\"draft_to_final\"]:\n",
        "            df_sims = similarity_trends[\"draft_to_final\"]\n",
        "            similarity_trends[\"averages\"][\"draft_to_final\"] = {\n",
        "                \"lexical\": np.mean([s[\"lexical_similarity\"] for s in df_sims]),\n",
        "                \"semantic\": np.mean([s[\"semantic_similarity\"] for s in df_sims]),\n",
        "                \"combined\": np.mean([s[\"combined_similarity\"] for s in df_sims]),\n",
        "            }\n",
        "\n",
        "        return similarity_trends\n",
        "\n",
        "    def aggregate_word_count_trends(self, articles_data):\n",
        "        \"\"\"Aggregate word count progression across articles.\"\"\"\n",
        "        word_trends = {\"by_article\": [], \"averages\": {}, \"progression_patterns\": []}\n",
        "\n",
        "        for article in articles_data:\n",
        "            if not article or \"processing_summary\" not in article:\n",
        "                continue\n",
        "\n",
        "            processing = article[\"processing_summary\"]\n",
        "\n",
        "            article_words = {\n",
        "                \"article_name\": article[\"article_name\"],\n",
        "                \"publication_date\": article[\"publication_date\"],\n",
        "                \"word_counts\": {},\n",
        "                \"progression\": [],\n",
        "            }\n",
        "\n",
        "            # Extract word counts for each version\n",
        "            version_order = [\"draft\", \"refined\", \"edited\", \"final\"]\n",
        "            for version in version_order:\n",
        "                if version in processing:\n",
        "                    count = processing[version][\"word_count\"]\n",
        "                    article_words[\"word_counts\"][version] = count\n",
        "                    article_words[\"progression\"].append(count)\n",
        "\n",
        "            word_trends[\"by_article\"].append(article_words)\n",
        "\n",
        "        # Calculate average progressions\n",
        "        if word_trends[\"by_article\"]:\n",
        "            version_order = [\"draft\", \"refined\", \"edited\", \"final\"]\n",
        "            for version in version_order:\n",
        "                counts = [\n",
        "                    a[\"word_counts\"].get(version, 0)\n",
        "                    for a in word_trends[\"by_article\"]\n",
        "                    if version in a[\"word_counts\"]\n",
        "                ]\n",
        "                if counts:\n",
        "                    word_trends[\"averages\"][version] = np.mean(counts)\n",
        "\n",
        "        return word_trends\n",
        "\n",
        "    def analyze_trends(self, period):\n",
        "        \"\"\"Perform complete trend analysis for a period.\"\"\"\n",
        "        print(f\"\\nüöÄ Starting trend analysis for period: {period}\")\n",
        "\n",
        "        # Find articles\n",
        "        articles = self.find_articles_for_period(period)\n",
        "\n",
        "        if not articles:\n",
        "            print(f\"‚ùå No articles found for period {period}\")\n",
        "            return None\n",
        "\n",
        "        # Load article data\n",
        "        print(f\"\\nüìñ Loading analysis data for {len(articles)} articles...\")\n",
        "        articles_data = []\n",
        "\n",
        "        for article_info in articles:\n",
        "            data = self.load_article_data(article_info)\n",
        "            if data:\n",
        "                articles_data.append(data)\n",
        "                print(f\"  ‚úì Loaded: {article_info['article_name']}\")\n",
        "            else:\n",
        "                print(f\"  ‚úó Failed: {article_info['article_name']}\")\n",
        "\n",
        "        print(f\"üìä Successfully loaded {len(articles_data)} articles\")\n",
        "\n",
        "        # Perform trend analysis\n",
        "        print(\"\\nüîç Analyzing trends...\")\n",
        "\n",
        "        attribution_trends = self.aggregate_attribution_trends(articles_data)\n",
        "        similarity_trends = self.aggregate_similarity_trends(articles_data)\n",
        "        word_count_trends = self.aggregate_word_count_trends(articles_data)\n",
        "\n",
        "        # Compile results\n",
        "        self.trend_results = {\n",
        "            \"period\": period,\n",
        "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
        "            \"articles_analyzed\": len(articles_data),\n",
        "            \"article_list\": [\n",
        "                {\"name\": a[\"article_name\"], \"date\": a[\"publication_date\"]}\n",
        "                for a in articles_data\n",
        "            ],\n",
        "            \"attribution_trends\": attribution_trends,\n",
        "            \"similarity_trends\": similarity_trends,\n",
        "            \"word_count_trends\": word_count_trends,\n",
        "            \"summary\": self.generate_trend_summary(\n",
        "                attribution_trends, similarity_trends, word_count_trends\n",
        "            ),\n",
        "        }\n",
        "\n",
        "        return self.trend_results\n",
        "\n",
        "    def generate_trend_summary(\n",
        "        self, attribution_trends, similarity_trends, word_count_trends\n",
        "    ):\n",
        "        \"\"\"Generate a summary of key trends.\"\"\"\n",
        "        summary = {\"key_metrics\": {}, \"patterns\": [], \"insights\": []}\n",
        "\n",
        "        # Key attribution metrics\n",
        "        if attribution_trends[\"averages\"]:\n",
        "            avg_draft = attribution_trends[\"averages\"].get(\"origin_draft\", 0)\n",
        "            avg_edited = attribution_trends[\"averages\"].get(\"origin_edited\", 0)\n",
        "            avg_new = attribution_trends[\"averages\"].get(\"origin_new_in_final\", 0)\n",
        "\n",
        "            summary[\"key_metrics\"][\"avg_draft_retention\"] = round(avg_draft, 1)\n",
        "            summary[\"key_metrics\"][\"avg_edited_dominance\"] = round(avg_edited, 1)\n",
        "            summary[\"key_metrics\"][\"avg_new_content\"] = round(avg_new, 1)\n",
        "\n",
        "        # Key similarity metrics\n",
        "        if similarity_trends[\"averages\"].get(\"draft_to_final\"):\n",
        "            df_sim = similarity_trends[\"averages\"][\"draft_to_final\"]\n",
        "            summary[\"key_metrics\"][\"avg_draft_final_similarity\"] = round(\n",
        "                df_sim[\"combined\"] * 100, 1\n",
        "            )\n",
        "\n",
        "        # Word count patterns\n",
        "        if word_count_trends[\"averages\"]:\n",
        "            avg_draft = word_count_trends[\"averages\"].get(\"draft\", 0)\n",
        "            avg_final = word_count_trends[\"averages\"].get(\"final\", 0)\n",
        "            if avg_draft > 0:\n",
        "                change_pct = ((avg_final - avg_draft) / avg_draft) * 100\n",
        "                summary[\"key_metrics\"][\"avg_word_change_pct\"] = round(change_pct, 1)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def save_trend_analysis(self, output_path, filename_prefix=\"trend_analysis\"):\n",
        "        \"\"\"Save trend analysis results.\"\"\"\n",
        "        if not self.trend_results:\n",
        "            print(\"‚ùå No trend results to save\")\n",
        "            return None\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Save comprehensive results\n",
        "        results_file = os.path.join(\n",
        "            output_path, f\"{filename_prefix}_{self.trend_results['period']}.json\"\n",
        "        )\n",
        "\n",
        "        with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.trend_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"üíæ Trend analysis saved: {results_file}\")\n",
        "\n",
        "        # Also save a summary report\n",
        "        summary_file = os.path.join(\n",
        "            output_path,\n",
        "            f\"{filename_prefix}_{self.trend_results['period']}_summary.json\",\n",
        "        )\n",
        "\n",
        "        summary_data = {\n",
        "            \"period\": self.trend_results[\"period\"],\n",
        "            \"articles_count\": self.trend_results[\"articles_analyzed\"],\n",
        "            \"key_metrics\": self.trend_results[\"summary\"][\"key_metrics\"],\n",
        "            \"article_list\": self.trend_results[\"article_list\"],\n",
        "        }\n",
        "\n",
        "        with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"üìã Summary report saved: {summary_file}\")\n",
        "\n",
        "        return results_file, summary_file\n",
        "\n",
        "\n",
        "print(\"üìà TrendAnalyzer class loaded. Ready for trend analysis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKz52N5oel4w"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 12: EXECUTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def analyze_period_trends(period, output_path=None):\n",
        "    \"\"\"Analyze trends for a specific period.\"\"\"\n",
        "    print(f\"üìà Analyzing trends for period: {period}\")\n",
        "\n",
        "    # Create analyzer\n",
        "    analyzer = TrendAnalyzer()\n",
        "\n",
        "    # Run analysis\n",
        "    results = analyzer.analyze_trends(period)\n",
        "\n",
        "    if not results:\n",
        "        return None\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nüìä TREND ANALYSIS SUMMARY for {period}\")\n",
        "    print(f\"{'=' * 50}\")\n",
        "\n",
        "    summary = results[\"summary\"][\"key_metrics\"]\n",
        "\n",
        "    if \"avg_draft_retention\" in summary:\n",
        "        print(\n",
        "            f\"üìù Average content retention from draft: {summary['avg_draft_retention']}%\"\n",
        "        )\n",
        "\n",
        "    if \"avg_edited_dominance\" in summary:\n",
        "        print(\n",
        "            f\"‚úèÔ∏è  Average content from editing phase: {summary['avg_edited_dominance']}%\"\n",
        "        )\n",
        "\n",
        "    if \"avg_new_content\" in summary:\n",
        "        print(f\"üÜï Average new content in final: {summary['avg_new_content']}%\")\n",
        "\n",
        "    if \"avg_draft_final_similarity\" in summary:\n",
        "        print(\n",
        "            f\"üîÑ Average draft-to-final similarity: {summary['avg_draft_final_similarity']}%\"\n",
        "        )\n",
        "\n",
        "    if \"avg_word_change_pct\" in summary:\n",
        "        change = summary[\"avg_word_change_pct\"]\n",
        "        direction = \"increase\" if change > 0 else \"decrease\"\n",
        "        print(f\"üìè Average word count {direction}: {abs(change):.1f}%\")\n",
        "\n",
        "    print(f\"\\nüìö Articles analyzed: {results['articles_analyzed']}\")\n",
        "    for article in results[\"article_list\"]:\n",
        "        print(f\"  ‚Ä¢ {article['date']}: {article['name']}\")\n",
        "\n",
        "    # Save results if output path provided\n",
        "    if output_path:\n",
        "        analyzer.save_trend_analysis(output_path)\n",
        "\n",
        "    return results, analyzer\n",
        "\n",
        "\n",
        "def quick_trend_check(period):\n",
        "    \"\"\"Quick trend check without saving files.\"\"\"\n",
        "    results, analyzer = analyze_period_trends(period)\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"üéØ Execution functions loaded!\")\n",
        "print(\"\\nUsage examples:\")\n",
        "print(\"# Quick check (no files saved)\")\n",
        "print(\"results = quick_trend_check('2025-Q1')\")\n",
        "print(\"\\n# Full analysis with file output\")\n",
        "print(\"results, analyzer = analyze_period_trends('2025-01', '/your/output/path')\")\n",
        "print(\"\\n# Supported period formats:\")\n",
        "print(\"  ‚Ä¢ '2025' (full year)\")\n",
        "print(\"  ‚Ä¢ '2025-01' (specific month)\")\n",
        "print(\"  ‚Ä¢ '2025-Q1' (quarter)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZnFq9pfjhdL"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1: VISUALIZATION SETUP AND DEPENDENCIES\n",
        "# =============================================================================\n",
        "\n",
        "# Install required packages\n",
        "plt.style.use(\"default\")\n",
        "sns.set_palette(\"husl\")\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "# Custom color palette for consistency\n",
        "COLORS = {\n",
        "    \"draft\": \"#FF6B6B\",  # Red\n",
        "    \"refined\": \"#4ECDC4\",  # Teal\n",
        "    \"edited\": \"#45B7D1\",  # Blue\n",
        "    \"final\": \"#96CEB4\",  # Green\n",
        "    \"new_content\": \"#FECA57\",  # Yellow\n",
        "    \"high_similarity\": \"#48CAE4\",  # Light blue\n",
        "    \"medium_similarity\": \"#FFB3BA\",  # Light pink\n",
        "    \"low_similarity\": \"#FFDFBA\",  # Light orange\n",
        "    \"background\": \"#F8F9FA\",\n",
        "}\n",
        "\n",
        "print(\"üìä Visualization dependencies loaded successfully!\")\n",
        "print(\"üé® Custom color palette configured\")\n",
        "\n",
        "\n",
        "class VisualizationEngine:\n",
        "    \"\"\"Unified visualization engine for individual articles and trend analysis.\"\"\"\n",
        "\n",
        "    def __init__(self, output_path=None):\n",
        "        self.output_path = output_path\n",
        "        self.figures = {}\n",
        "\n",
        "    def create_content_flow_chart(\n",
        "        self, data, title=\"Content Flow Analysis\", mode=\"individual\"\n",
        "    ):\n",
        "        \"\"\"Create a content flow visualization showing version progression.\"\"\"\n",
        "        if mode == \"individual\":\n",
        "            # Individual article - show attribution percentages\n",
        "            attribution_stats = data[\"attribution_analysis\"][\"statistics\"]\n",
        "            origin_dist = attribution_stats[\"origin_distribution\"]\n",
        "\n",
        "            # Prepare data for flow chart\n",
        "            flow_data = []\n",
        "            for version, stats in origin_dist.items():\n",
        "                if version != \"new_in_final\":\n",
        "                    flow_data.append(\n",
        "                        {\n",
        "                            \"source\": version.title(),\n",
        "                            \"target\": \"Final Article\",\n",
        "                            \"value\": stats[\"percentage\"],\n",
        "                            \"count\": stats[\"count\"],\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "            # Add new content\n",
        "            if \"new_in_final\" in origin_dist:\n",
        "                flow_data.append(\n",
        "                    {\n",
        "                        \"source\": \"New Content\",\n",
        "                        \"target\": \"Final Article\",\n",
        "                        \"value\": origin_dist[\"new_in_final\"][\"percentage\"],\n",
        "                        \"count\": origin_dist[\"new_in_final\"][\"count\"],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            # Trend analysis - show average percentages\n",
        "            avg_data = data[\"attribution_trends\"][\"averages\"]\n",
        "            flow_data = []\n",
        "\n",
        "            for key, value in avg_data.items():\n",
        "                if key.startswith(\"origin_\") and not key.endswith(\"new_in_final\"):\n",
        "                    version = key.replace(\"origin_\", \"\").title()\n",
        "                    flow_data.append(\n",
        "                        {\n",
        "                            \"source\": version,\n",
        "                            \"target\": \"Final Articles\",\n",
        "                            \"value\": value,\n",
        "                            \"count\": f\"{value:.1f}% avg\",\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "            # Add new content average\n",
        "            if \"origin_new_in_final\" in avg_data:\n",
        "                flow_data.append(\n",
        "                    {\n",
        "                        \"source\": \"New Content\",\n",
        "                        \"target\": \"Final Articles\",\n",
        "                        \"value\": avg_data[\"origin_new_in_final\"],\n",
        "                        \"count\": f\"{avg_data['origin_new_in_final']:.1f}% avg\",\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        # Create sankey-style visualization using matplotlib\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        ax.set_xlim(0, 10)\n",
        "        ax.set_ylim(0, 10)\n",
        "\n",
        "        # Draw flow connections\n",
        "        y_positions = np.linspace(8, 2, len(flow_data))\n",
        "\n",
        "        for i, item in enumerate(flow_data):\n",
        "            # Source box\n",
        "            source_color = COLORS.get(item[\"source\"].lower(), \"#CCCCCC\")\n",
        "            source_rect = patches.Rectangle(\n",
        "                (0.5, y_positions[i] - 0.3),\n",
        "                2,\n",
        "                0.6,\n",
        "                facecolor=source_color,\n",
        "                alpha=0.7,\n",
        "                edgecolor=\"black\",\n",
        "            )\n",
        "            ax.add_patch(source_rect)\n",
        "            ax.text(\n",
        "                1.5,\n",
        "                y_positions[i],\n",
        "                item[\"source\"],\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                fontweight=\"bold\",\n",
        "            )\n",
        "\n",
        "            # Flow arrow\n",
        "            arrow_width = item[\"value\"] / 100 * 0.4  # Scale arrow width by percentage\n",
        "            arrow = patches.FancyArrowPatch(\n",
        "                (2.5, y_positions[i]),\n",
        "                (6.5, 5),\n",
        "                arrowstyle=\"->\",\n",
        "                mutation_scale=20,\n",
        "                linewidth=arrow_width * 10,\n",
        "                alpha=0.6,\n",
        "                color=source_color,\n",
        "            )\n",
        "            ax.add_patch(arrow)\n",
        "\n",
        "            # Percentage label\n",
        "            ax.text(\n",
        "                4.5,\n",
        "                y_positions[i] + 0.2,\n",
        "                f\"{item['value']:.1f}%\",\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                fontsize=10,\n",
        "                fontweight=\"bold\",\n",
        "            )\n",
        "\n",
        "        # Target box\n",
        "        target_rect = patches.Rectangle(\n",
        "            (7, 4.5), 2, 1, facecolor=COLORS[\"final\"], alpha=0.7, edgecolor=\"black\"\n",
        "        )\n",
        "        ax.add_patch(target_rect)\n",
        "        target_text = \"Final Article\" if mode == \"individual\" else \"Final Articles\"\n",
        "        ax.text(8, 5, target_text, ha=\"center\", va=\"center\", fontweight=\"bold\")\n",
        "\n",
        "        ax.set_title(title, fontsize=16, fontweight=\"bold\", pad=20)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_modification_intensity_chart(\n",
        "        self, data, title=\"Content Modification Intensity\", mode=\"individual\"\n",
        "    ):\n",
        "        \"\"\"Create a chart showing modification intensity levels.\"\"\"\n",
        "        if mode == \"individual\":\n",
        "            mod_dist = data[\"attribution_analysis\"][\"statistics\"][\n",
        "                \"modification_distribution\"\n",
        "            ]\n",
        "            categories = list(mod_dist.keys())\n",
        "            values = [mod_dist[cat][\"percentage\"] for cat in categories]\n",
        "        else:\n",
        "            # Trend analysis\n",
        "            avg_data = data[\"attribution_trends\"][\"averages\"]\n",
        "            categories = []\n",
        "            values = []\n",
        "            for key, value in avg_data.items():\n",
        "                if key.startswith(\"modification_\"):\n",
        "                    cat = key.replace(\"modification_\", \"\").replace(\"_\", \" \").title()\n",
        "                    categories.append(cat)\n",
        "                    values.append(value)\n",
        "\n",
        "        # Create horizontal bar chart\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        # Color mapping for modification levels\n",
        "        mod_colors = [\n",
        "            COLORS.get(cat.lower().replace(\" \", \"_\"), \"#CCCCCC\") for cat in categories\n",
        "        ]\n",
        "\n",
        "        bars = ax.barh(\n",
        "            categories,\n",
        "            values,\n",
        "            color=mod_colors,\n",
        "            alpha=0.8,\n",
        "            edgecolor=\"black\",\n",
        "            linewidth=1,\n",
        "        )\n",
        "\n",
        "        # Add percentage labels on bars\n",
        "        for bar, value in zip(bars, values, strict=False):\n",
        "            width = bar.get_width()\n",
        "            ax.text(\n",
        "                width + 1,\n",
        "                bar.get_y() + bar.get_height() / 2,\n",
        "                f\"{value:.1f}%\",\n",
        "                ha=\"left\",\n",
        "                va=\"center\",\n",
        "                fontweight=\"bold\",\n",
        "            )\n",
        "\n",
        "        ax.set_xlabel(\"Percentage of Content\", fontsize=12, fontweight=\"bold\")\n",
        "        ax.set_title(title, fontsize=14, fontweight=\"bold\", pad=20)\n",
        "        ax.set_xlim(0, max(values) * 1.2)\n",
        "\n",
        "        # Add grid for readability\n",
        "        ax.grid(axis=\"x\", alpha=0.3, linestyle=\"--\")\n",
        "        ax.set_axisbelow(True)\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_word_count_progression(\n",
        "        self, data, title=\"Word Count Progression\", mode=\"individual\"\n",
        "    ):\n",
        "        \"\"\"Create a line chart showing word count changes across versions.\"\"\"\n",
        "        if mode == \"individual\":\n",
        "            processing = data[\"processing_summary\"]\n",
        "            versions = [\"draft\", \"refined\", \"edited\", \"final\"]\n",
        "            word_counts = []\n",
        "            version_labels = []\n",
        "\n",
        "            for version in versions:\n",
        "                if version in processing:\n",
        "                    word_counts.append(processing[version][\"word_count\"])\n",
        "                    version_labels.append(version.title())\n",
        "        else:\n",
        "            # Trend analysis - show averages\n",
        "            avg_data = data[\"word_count_trends\"][\"averages\"]\n",
        "            versions = [\"draft\", \"refined\", \"edited\", \"final\"]\n",
        "            word_counts = []\n",
        "            version_labels = []\n",
        "\n",
        "            for version in versions:\n",
        "                if version in avg_data:\n",
        "                    word_counts.append(avg_data[version])\n",
        "                    version_labels.append(version.title())\n",
        "\n",
        "        # Create line chart\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        ax.plot(\n",
        "            version_labels,\n",
        "            word_counts,\n",
        "            marker=\"o\",\n",
        "            linewidth=3,\n",
        "            markersize=8,\n",
        "            color=COLORS[\"draft\"],\n",
        "            markerfacecolor=COLORS[\"final\"],\n",
        "        )\n",
        "\n",
        "        # Add value labels on points\n",
        "        for i, (_label, count) in enumerate(\n",
        "            zip(version_labels, word_counts, strict=False)\n",
        "        ):\n",
        "            ax.annotate(\n",
        "                f\"{int(count)}\",\n",
        "                (i, count),\n",
        "                textcoords=\"offset points\",\n",
        "                xytext=(0, 10),\n",
        "                ha=\"center\",\n",
        "                fontweight=\"bold\",\n",
        "            )\n",
        "\n",
        "        ax.set_ylabel(\"Word Count\", fontsize=12, fontweight=\"bold\")\n",
        "        ax.set_title(title, fontsize=14, fontweight=\"bold\", pad=20)\n",
        "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
        "        ax.set_axisbelow(True)\n",
        "\n",
        "        # Calculate and show percentage change\n",
        "        if len(word_counts) >= 2:\n",
        "            change = ((word_counts[-1] - word_counts[0]) / word_counts[0]) * 100\n",
        "            change_text = f\"Overall change: {change:+.1f}%\"\n",
        "            ax.text(\n",
        "                0.02,\n",
        "                0.98,\n",
        "                change_text,\n",
        "                transform=ax.transAxes,\n",
        "                fontsize=11,\n",
        "                fontweight=\"bold\",\n",
        "                verticalalignment=\"top\",\n",
        "                bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
        "            )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_similarity_heatmap(\n",
        "        self, data, title=\"Version Similarity Matrix\", mode=\"individual\"\n",
        "    ):\n",
        "        \"\"\"Create a heatmap showing similarities between versions.\"\"\"\n",
        "        if mode == \"individual\":\n",
        "            similarity_data = data[\"similarity_analysis\"][\"sequential_analysis\"]\n",
        "\n",
        "            # Build similarity matrix\n",
        "            versions = [\"Draft\", \"Refined\", \"Edited\", \"Final\"]\n",
        "            matrix = np.zeros((len(versions), len(versions)))\n",
        "            np.fill_diagonal(matrix, 1.0)  # Perfect similarity with self\n",
        "\n",
        "            # Fill in the sequential similarities\n",
        "            for seq in similarity_data:\n",
        "                pair = seq[\"version_pair\"]\n",
        "                similarity = seq[\"full_text\"][\"combined\"]\n",
        "\n",
        "                # Parse version pair (e.g., \"draft_to_refined\")\n",
        "                from_version, to_version = pair.split(\"_to_\")\n",
        "                from_idx = versions.index(from_version.title())\n",
        "                to_idx = versions.index(to_version.title())\n",
        "\n",
        "                matrix[from_idx][to_idx] = similarity\n",
        "                matrix[to_idx][from_idx] = similarity  # Make symmetric\n",
        "\n",
        "        else:\n",
        "            # For trend analysis, show average similarities\n",
        "            similarity_trends = data[\"similarity_trends\"]\n",
        "\n",
        "            # Create a simplified matrix for trends\n",
        "            versions = [\"Draft\", \"Final\"]\n",
        "            matrix = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
        "\n",
        "            if similarity_trends[\"averages\"].get(\"draft_to_final\"):\n",
        "                avg_sim = similarity_trends[\"averages\"][\"draft_to_final\"][\"combined\"]\n",
        "                matrix[0][1] = avg_sim\n",
        "                matrix[1][0] = avg_sim\n",
        "\n",
        "        # Create heatmap\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "        im = ax.imshow(matrix, cmap=\"RdYlGn\", aspect=\"equal\", vmin=0, vmax=1)\n",
        "\n",
        "        # Add text annotations\n",
        "        for i in range(len(versions)):\n",
        "            for j in range(len(versions)):\n",
        "                ax.text(\n",
        "                    j,\n",
        "                    i,\n",
        "                    f\"{matrix[i, j]:.2f}\",\n",
        "                    ha=\"center\",\n",
        "                    va=\"center\",\n",
        "                    color=\"black\",\n",
        "                    fontweight=\"bold\",\n",
        "                )\n",
        "\n",
        "        ax.set_xticks(range(len(versions)))\n",
        "        ax.set_yticks(range(len(versions)))\n",
        "        ax.set_xticklabels(versions)\n",
        "        ax.set_yticklabels(versions)\n",
        "        ax.set_title(title, fontsize=14, fontweight=\"bold\", pad=20)\n",
        "\n",
        "        # Add colorbar\n",
        "        cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "        cbar.set_label(\"Similarity Score\", rotation=270, labelpad=20, fontweight=\"bold\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_summary_dashboard(\n",
        "        self, data, title_prefix=\"Article Analysis\", mode=\"individual\"\n",
        "    ):\n",
        "        \"\"\"Create a comprehensive dashboard with multiple visualizations.\"\"\"\n",
        "        # Create subplots\n",
        "        fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "        # Title\n",
        "        main_title = f\"{title_prefix} Dashboard\"\n",
        "        if mode == \"individual\":\n",
        "            main_title += f\" - {data['article_name']}\"\n",
        "\n",
        "        fig.suptitle(main_title, fontsize=18, fontweight=\"bold\", y=0.95)\n",
        "\n",
        "        # Create individual charts and save them\n",
        "        charts = {}\n",
        "\n",
        "        # Content Flow Chart\n",
        "        charts[\"flow\"] = self.create_content_flow_chart(\n",
        "            data, \"Content Attribution\", mode\n",
        "        )\n",
        "\n",
        "        # Modification Intensity\n",
        "        charts[\"modification\"] = self.create_modification_intensity_chart(\n",
        "            data, \"Modification Intensity\", mode\n",
        "        )\n",
        "\n",
        "        # Word Count Progression\n",
        "        charts[\"word_count\"] = self.create_word_count_progression(\n",
        "            data, \"Word Count Evolution\", mode\n",
        "        )\n",
        "\n",
        "        # Similarity Heatmap\n",
        "        charts[\"similarity\"] = self.create_similarity_heatmap(\n",
        "            data, \"Version Similarities\", mode\n",
        "        )\n",
        "\n",
        "        return charts\n",
        "\n",
        "    def save_visualizations(self, charts, prefix=\"analysis\", formats=None):\n",
        "        \"\"\"Save all visualizations in multiple formats.\"\"\"\n",
        "        if formats is None:\n",
        "            formats = [\"png\", \"svg\"]\n",
        "        if not self.output_path:\n",
        "            print(\"‚ö†Ô∏è No output path specified - visualizations not saved\")\n",
        "            return {}\n",
        "\n",
        "        os.makedirs(self.output_path, exist_ok=True)\n",
        "        saved_files = {}\n",
        "\n",
        "        for chart_name, fig in charts.items():\n",
        "            for fmt in formats:\n",
        "                filename = f\"{prefix}_{chart_name}.{fmt}\"\n",
        "                filepath = os.path.join(self.output_path, filename)\n",
        "\n",
        "                fig.savefig(\n",
        "                    filepath,\n",
        "                    dpi=300,\n",
        "                    bbox_inches=\"tight\",\n",
        "                    facecolor=\"white\",\n",
        "                    edgecolor=\"none\",\n",
        "                )\n",
        "\n",
        "                if chart_name not in saved_files:\n",
        "                    saved_files[chart_name] = []\n",
        "                saved_files[chart_name].append(filepath)\n",
        "\n",
        "        # Close figures to free memory\n",
        "        for fig in charts.values():\n",
        "            plt.close(fig)\n",
        "\n",
        "        print(f\"üíæ Visualizations saved to: {self.output_path}\")\n",
        "        for chart_name, files in saved_files.items():\n",
        "            print(f\"  üìä {chart_name}: {len(files)} formats\")\n",
        "\n",
        "        return saved_files\n",
        "\n",
        "\n",
        "print(\"üé® VisualizationEngine class loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlHV6DxKjrLW"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 2: INTERACTIVE VISUALIZATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def create_interactive_flow_chart(data, mode=\"individual\"):\n",
        "    \"\"\"Create an interactive Plotly flow/sankey diagram.\"\"\"\n",
        "    if mode == \"individual\":\n",
        "        attribution_stats = data[\"attribution_analysis\"][\"statistics\"]\n",
        "        origin_dist = attribution_stats[\"origin_distribution\"]\n",
        "\n",
        "        # Prepare data for Sankey diagram\n",
        "        sources = []\n",
        "        targets = []\n",
        "        values = []\n",
        "        labels = []\n",
        "\n",
        "        # Add all source versions\n",
        "        for version in [\"draft\", \"refined\", \"edited\"]:\n",
        "            if version in origin_dist:\n",
        "                labels.append(version.title())\n",
        "\n",
        "        # Add new content and final\n",
        "        if \"new_in_final\" in origin_dist:\n",
        "            labels.append(\"New Content\")\n",
        "        labels.append(\"Final Article\")\n",
        "\n",
        "        # Create connections\n",
        "        final_idx = len(labels) - 1\n",
        "\n",
        "        for version, stats in origin_dist.items():\n",
        "            if version != \"new_in_final\":\n",
        "                source_idx = labels.index(version.title())\n",
        "                sources.append(source_idx)\n",
        "                targets.append(final_idx)\n",
        "                values.append(stats[\"percentage\"])\n",
        "\n",
        "        # Add new content if exists\n",
        "        if \"new_in_final\" in origin_dist:\n",
        "            new_idx = labels.index(\"New Content\")\n",
        "            sources.append(new_idx)\n",
        "            targets.append(final_idx)\n",
        "            values.append(origin_dist[\"new_in_final\"][\"percentage\"])\n",
        "\n",
        "    else:\n",
        "        # Trend analysis mode\n",
        "        avg_data = data[\"attribution_trends\"][\"averages\"]\n",
        "        labels = []\n",
        "        sources = []\n",
        "        targets = []\n",
        "        values = []\n",
        "\n",
        "        # Build labels and connections for trend data\n",
        "        for key, value in avg_data.items():\n",
        "            if key.startswith(\"origin_\") and value > 0:\n",
        "                version = key.replace(\"origin_\", \"\")\n",
        "                if version != \"new_in_final\":\n",
        "                    labels.append(version.title())\n",
        "                else:\n",
        "                    labels.append(\"New Content\")\n",
        "\n",
        "        labels.append(\"Final Articles\")\n",
        "        final_idx = len(labels) - 1\n",
        "\n",
        "        # Create connections\n",
        "        i = 0\n",
        "        for key, value in avg_data.items():\n",
        "            if key.startswith(\"origin_\") and value > 0:\n",
        "                sources.append(i)\n",
        "                targets.append(final_idx)\n",
        "                values.append(value)\n",
        "                i += 1\n",
        "\n",
        "    # Create Sankey diagram\n",
        "    fig = go.Figure(\n",
        "        data=[\n",
        "            go.Sankey(\n",
        "                node=dict(\n",
        "                    pad=15,\n",
        "                    thickness=20,\n",
        "                    line=dict(color=\"black\", width=0.5),\n",
        "                    label=labels,\n",
        "                    color=[\n",
        "                        COLORS.get(label.lower().replace(\" \", \"_\"), \"#CCCCCC\")\n",
        "                        for label in labels\n",
        "                    ],\n",
        "                ),\n",
        "                link=dict(\n",
        "                    source=sources,\n",
        "                    target=targets,\n",
        "                    value=values,\n",
        "                    color=[\n",
        "                        \"rgba(255,107,107,0.4)\"\n",
        "                        if i < len(sources)\n",
        "                        else \"rgba(150,206,180,0.4)\"\n",
        "                        for i in range(len(sources))\n",
        "                    ],\n",
        "                ),\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    title = \"Content Flow Analysis\"\n",
        "    if mode == \"individual\":\n",
        "        title += f\" - {data['article_name']}\"\n",
        "\n",
        "    fig.update_layout(title_text=title, font_size=12, height=600)\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def create_interactive_trends_chart(trend_data):\n",
        "    \"\"\"Create interactive trend charts for multiple articles over time.\"\"\"\n",
        "    # Extract data for trends over time\n",
        "    attribution_by_article = trend_data[\"attribution_trends\"][\"by_article\"]\n",
        "\n",
        "    if not attribution_by_article:\n",
        "        print(\"No trend data available\")\n",
        "        return None\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    dates = []\n",
        "    draft_retention = []\n",
        "    edited_dominance = []\n",
        "    new_content = []\n",
        "    article_names = []\n",
        "\n",
        "    for article in attribution_by_article:\n",
        "        dates.append(article[\"publication_date\"])\n",
        "        article_names.append(article[\"article_name\"])\n",
        "\n",
        "        # Extract percentages\n",
        "        draft_retention.append(article[\"origin_percentages\"].get(\"draft\", 0))\n",
        "        edited_dominance.append(article[\"origin_percentages\"].get(\"edited\", 0))\n",
        "        new_content.append(article[\"origin_percentages\"].get(\"new_in_final\", 0))\n",
        "\n",
        "    # Create multi-line chart\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add traces for each metric\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=dates,\n",
        "            y=draft_retention,\n",
        "            mode=\"lines+markers\",\n",
        "            name=\"Draft Retention %\",\n",
        "            line=dict(color=COLORS[\"draft\"], width=3),\n",
        "            marker=dict(size=8),\n",
        "            hovertemplate=\"<b>%{text}</b><br>Date: %{x}<br>Draft Retention: %{y:.1f}%<extra></extra>\",\n",
        "            text=article_names,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=dates,\n",
        "            y=edited_dominance,\n",
        "            mode=\"lines+markers\",\n",
        "            name=\"Edited Content %\",\n",
        "            line=dict(color=COLORS[\"edited\"], width=3),\n",
        "            marker=dict(size=8),\n",
        "            hovertemplate=\"<b>%{text}</b><br>Date: %{x}<br>Edited Content: %{y:.1f}%<extra></extra>\",\n",
        "            text=article_names,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=dates,\n",
        "            y=new_content,\n",
        "            mode=\"lines+markers\",\n",
        "            name=\"New Content %\",\n",
        "            line=dict(color=COLORS[\"new_content\"], width=3),\n",
        "            marker=dict(size=8),\n",
        "            hovertemplate=\"<b>%{text}</b><br>Date: %{x}<br>New Content: %{y:.1f}%<extra></extra>\",\n",
        "            text=article_names,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Content Attribution Trends Over Time\",\n",
        "        xaxis_title=\"Publication Date\",\n",
        "        yaxis_title=\"Percentage of Final Content\",\n",
        "        hovermode=\"x unified\",\n",
        "        height=600,\n",
        "        showlegend=True,\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "print(\"üöÄ Interactive visualization functions loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6l2aIb8j5DK"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 3: EXECUTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def visualize_individual_article(analysis_data, output_path=None, save_files=True):\n",
        "    \"\"\"Create visualizations for a single article analysis.\"\"\"\n",
        "    print(f\"üé® Creating visualizations for: {analysis_data['article_name']}\")\n",
        "\n",
        "    # Initialize visualization engine\n",
        "    viz_engine = VisualizationEngine(output_path)\n",
        "\n",
        "    # Create static charts\n",
        "    print(\"üìä Generating static visualizations...\")\n",
        "    charts = viz_engine.create_summary_dashboard(analysis_data, mode=\"individual\")\n",
        "\n",
        "    # Create interactive chart\n",
        "    print(\"üîß Creating interactive flow chart...\")\n",
        "    interactive_flow = create_interactive_flow_chart(analysis_data, mode=\"individual\")\n",
        "\n",
        "    # Save files if requested\n",
        "    saved_files = {}\n",
        "    if save_files and output_path:\n",
        "        saved_files = viz_engine.save_visualizations(\n",
        "            charts,\n",
        "            prefix=f\"{analysis_data['article_name']}_analysis\",\n",
        "            formats=[\"png\", \"svg\"],\n",
        "        )\n",
        "\n",
        "        # Save interactive chart\n",
        "        interactive_file = os.path.join(\n",
        "            output_path, f\"{analysis_data['article_name']}_interactive_flow.html\"\n",
        "        )\n",
        "        interactive_flow.write_html(interactive_file)\n",
        "        saved_files[\"interactive_flow\"] = [interactive_file]\n",
        "        print(f\"üíæ Interactive chart saved: {interactive_file}\")\n",
        "\n",
        "    # Display interactive chart\n",
        "    interactive_flow.show()\n",
        "\n",
        "    return charts, interactive_flow, saved_files\n",
        "\n",
        "\n",
        "def visualize_trend_analysis(trend_data, output_path=None, save_files=True):\n",
        "    \"\"\"Create visualizations for trend analysis.\"\"\"\n",
        "    period = trend_data[\"period\"]\n",
        "    print(f\"üìà Creating trend visualizations for period: {period}\")\n",
        "\n",
        "    # Initialize visualization engine\n",
        "    viz_engine = VisualizationEngine(output_path)\n",
        "\n",
        "    # Create static charts\n",
        "    print(\"üìä Generating static trend visualizations...\")\n",
        "    charts = viz_engine.create_summary_dashboard(\n",
        "        trend_data, f\"Trend Analysis - {period}\", mode=\"trend\"\n",
        "    )\n",
        "\n",
        "    # Create interactive charts\n",
        "    print(\"üîß Creating interactive trend charts...\")\n",
        "    interactive_flow = create_interactive_flow_chart(trend_data, mode=\"trend\")\n",
        "    interactive_trends = create_interactive_trends_chart(trend_data)\n",
        "\n",
        "    # Save files if requested\n",
        "    saved_files = {}\n",
        "    if save_files and output_path:\n",
        "        saved_files = viz_engine.save_visualizations(\n",
        "            charts, prefix=f\"trend_analysis_{period}\", formats=[\"png\", \"svg\"]\n",
        "        )\n",
        "\n",
        "        # Save interactive charts\n",
        "        if output_path:\n",
        "            flow_file = os.path.join(output_path, f\"trend_flow_{period}.html\")\n",
        "            interactive_flow.write_html(flow_file)\n",
        "            saved_files[\"interactive_flow\"] = [flow_file]\n",
        "\n",
        "            if interactive_trends:\n",
        "                trends_file = os.path.join(output_path, f\"trend_timeline_{period}.html\")\n",
        "                interactive_trends.write_html(trends_file)\n",
        "                saved_files[\"interactive_trends\"] = [trends_file]\n",
        "                print(f\"üíæ Interactive charts saved to: {output_path}\")\n",
        "\n",
        "    # Display interactive charts\n",
        "    interactive_flow.show()\n",
        "    if interactive_trends:\n",
        "        interactive_trends.show()\n",
        "\n",
        "    return charts, {\"flow\": interactive_flow, \"trends\": interactive_trends}, saved_files\n",
        "\n",
        "\n",
        "def quick_visualize(data, data_type=\"individual\", output_path=None):\n",
        "    \"\"\"Quick visualization function for any analysis data.\"\"\"\n",
        "    if data_type == \"individual\":\n",
        "        return visualize_individual_article(\n",
        "            data, output_path, save_files=bool(output_path)\n",
        "        )\n",
        "    elif data_type == \"trend\":\n",
        "        return visualize_trend_analysis(data, output_path, save_files=bool(output_path))\n",
        "    else:\n",
        "        print(f\"‚ùå Unknown data type: {data_type}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"üéØ Execution functions loaded!\")\n",
        "print(\"\\nUsage examples:\")\n",
        "print(\"# Visualize individual article (using existing analysis data)\")\n",
        "print(\n",
        "    \"charts, interactive, files = visualize_individual_article(combined_results, '/your/output/path')\"\n",
        ")\n",
        "print(\"\\n# Visualize trend analysis\")\n",
        "print(\n",
        "    \"charts, interactive, files = visualize_trend_analysis(trend_results, '/your/output/path')\"\n",
        ")\n",
        "print(\"\\n# Quick visualization without saving\")\n",
        "print(\"quick_visualize(combined_results, 'individual')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv070Iw0j-wV"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 4: FOOTER METRICS GENERATOR\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def generate_article_footer_graphics(analysis_data, output_path=None):\n",
        "    \"\"\"Generate clean, publication-ready graphics for article footers.\"\"\"\n",
        "    print(f\"üìÑ Generating footer graphics for: {analysis_data['article_name']}\")\n",
        "\n",
        "    # Create a clean, minimal style for footer graphics\n",
        "    plt.style.use(\"default\")\n",
        "\n",
        "    # Footer-specific color palette (more muted)\n",
        "    footer_colors = {\n",
        "        \"primary\": \"#2E86AB\",\n",
        "        \"secondary\": \"#A23B72\",\n",
        "        \"accent\": \"#F18F01\",\n",
        "        \"neutral\": \"#C73E1D\",\n",
        "    }\n",
        "\n",
        "    # Create a compact summary visualization\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    fig.suptitle(\n",
        "        f\"Content Analysis Summary - {analysis_data['article_name']}\",\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "    # 1. Content Origins (Pie Chart)\n",
        "    attribution_stats = analysis_data[\"attribution_analysis\"][\"statistics\"]\n",
        "    origin_dist = attribution_stats[\"origin_distribution\"]\n",
        "\n",
        "    labels = []\n",
        "    sizes = []\n",
        "    colors = []\n",
        "\n",
        "    for version, stats in origin_dist.items():\n",
        "        if version != \"new_in_final\":\n",
        "            labels.append(f\"{version.title()}\\n{stats['percentage']:.1f}%\")\n",
        "            sizes.append(stats[\"percentage\"])\n",
        "            colors.append(footer_colors[\"primary\"])\n",
        "\n",
        "    if \"new_in_final\" in origin_dist:\n",
        "        labels.append(f\"New Content\\n{origin_dist['new_in_final']['percentage']:.1f}%\")\n",
        "        sizes.append(origin_dist[\"new_in_final\"][\"percentage\"])\n",
        "        colors.append(footer_colors[\"accent\"])\n",
        "\n",
        "    ax1.pie(sizes, labels=labels, colors=colors, autopct=\"\", startangle=90)\n",
        "    ax1.set_title(\"Content Origins\", fontweight=\"bold\")\n",
        "\n",
        "    # 2. Word Count Progression (Bar Chart)\n",
        "    processing = analysis_data[\"processing_summary\"]\n",
        "    versions = [\"Draft\", \"Refined\", \"Edited\", \"Final\"]\n",
        "    word_counts = []\n",
        "\n",
        "    for version in [\"draft\", \"refined\", \"edited\", \"final\"]:\n",
        "        if version in processing:\n",
        "            word_counts.append(processing[version][\"word_count\"])\n",
        "        else:\n",
        "            word_counts.append(0)\n",
        "\n",
        "    bars = ax2.bar(versions, word_counts, color=footer_colors[\"secondary\"], alpha=0.7)\n",
        "    ax2.set_title(\"Word Count Evolution\", fontweight=\"bold\")\n",
        "    ax2.set_ylabel(\"Words\")\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, word_counts, strict=False):\n",
        "        if count > 0:\n",
        "            height = bar.get_height()\n",
        "            ax2.text(\n",
        "                bar.get_x() + bar.get_width() / 2.0,\n",
        "                height + 5,\n",
        "                f\"{int(count)}\",\n",
        "                ha=\"center\",\n",
        "                va=\"bottom\",\n",
        "                fontsize=9,\n",
        "            )\n",
        "\n",
        "    # 3. Modification Intensity (Horizontal Bar)\n",
        "    mod_dist = attribution_stats[\"modification_distribution\"]\n",
        "    mod_categories = []\n",
        "    mod_values = []\n",
        "\n",
        "    for category, stats in mod_dist.items():\n",
        "        clean_cat = category.replace(\"_\", \" \").title()\n",
        "        mod_categories.append(clean_cat)\n",
        "        mod_values.append(stats[\"percentage\"])\n",
        "\n",
        "    bars = ax3.barh(\n",
        "        mod_categories, mod_values, color=footer_colors[\"neutral\"], alpha=0.7\n",
        "    )\n",
        "    ax3.set_title(\"Modification Levels\", fontweight=\"bold\")\n",
        "    ax3.set_xlabel(\"Percentage\")\n",
        "\n",
        "    # Add percentage labels\n",
        "    for bar, value in zip(bars, mod_values, strict=False):\n",
        "        width = bar.get_width()\n",
        "        ax3.text(\n",
        "            width + 1,\n",
        "            bar.get_y() + bar.get_height() / 2.0,\n",
        "            f\"{value:.1f}%\",\n",
        "            ha=\"left\",\n",
        "            va=\"center\",\n",
        "            fontsize=9,\n",
        "        )\n",
        "\n",
        "    # 4. Key Metrics Summary (Text)\n",
        "    ax4.axis(\"off\")\n",
        "\n",
        "    # Calculate key metrics\n",
        "    draft_to_final = analysis_data[\"similarity_analysis\"][\"draft_to_final\"]\n",
        "    similarity_pct = (\n",
        "        draft_to_final[\"full_text\"][\"combined\"] * 100 if draft_to_final else 0\n",
        "    )\n",
        "\n",
        "    word_change = 0\n",
        "    if \"draft\" in processing and \"final\" in processing:\n",
        "        draft_words = processing[\"draft\"][\"word_count\"]\n",
        "        final_words = processing[\"final\"][\"word_count\"]\n",
        "        word_change = ((final_words - draft_words) / draft_words) * 100\n",
        "\n",
        "    # Display key metrics as text\n",
        "    metrics_text = f\"\"\"Key Metrics:\n",
        "\n",
        "Draft-Final Similarity: {similarity_pct:.1f}%\n",
        "\n",
        "Word Count Change: {word_change:+.1f}%\n",
        "\n",
        "Total Sentences: {attribution_stats[\"total_sentences\"]}\n",
        "\n",
        "Analysis Date: {datetime.now().strftime(\"%Y-%m-%d\")}\"\"\"\n",
        "\n",
        "    ax4.text(\n",
        "        0.1,\n",
        "        0.9,\n",
        "        metrics_text,\n",
        "        transform=ax4.transAxes,\n",
        "        fontsize=11,\n",
        "        verticalalignment=\"top\",\n",
        "        fontweight=\"bold\",\n",
        "        bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.3),\n",
        "    )\n",
        "\n",
        "    ax4.set_title(\"Summary Statistics\", fontweight=\"bold\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save footer graphic\n",
        "    footer_files = {}\n",
        "    if output_path:\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Save in multiple formats for footer use\n",
        "        for fmt in [\"png\", \"svg\", \"pdf\"]:\n",
        "            footer_file = os.path.join(\n",
        "                output_path, f\"{analysis_data['article_name']}_footer.{fmt}\"\n",
        "            )\n",
        "            fig.savefig(\n",
        "                footer_file,\n",
        "                dpi=300,\n",
        "                bbox_inches=\"tight\",\n",
        "                facecolor=\"white\",\n",
        "                edgecolor=\"none\",\n",
        "            )\n",
        "            footer_files[fmt] = footer_file\n",
        "\n",
        "        print(f\"üìÑ Footer graphics saved: {len(footer_files)} formats\")\n",
        "\n",
        "    return fig, footer_files\n",
        "\n",
        "\n",
        "def create_minimal_attribution_chart(analysis_data, output_path=None):\n",
        "    \"\"\"Create a minimal, clean chart suitable for article footers.\"\"\"\n",
        "    # Very simple pie chart for content attribution\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "\n",
        "    attribution_stats = analysis_data[\"attribution_analysis\"][\"statistics\"]\n",
        "    origin_dist = attribution_stats[\"origin_distribution\"]\n",
        "\n",
        "    # Aggregate into simple categories\n",
        "    ai_generated = 0  # draft + refined\n",
        "    human_edited = 0  # edited\n",
        "    new_content = 0  # new_in_final\n",
        "\n",
        "    for version, stats in origin_dist.items():\n",
        "        if version in [\"draft\", \"refined\"]:\n",
        "            ai_generated += stats[\"percentage\"]\n",
        "        elif version == \"edited\":\n",
        "            human_edited = stats[\"percentage\"]\n",
        "        elif version == \"new_in_final\":\n",
        "            new_content = stats[\"percentage\"]\n",
        "\n",
        "    # Create simple pie chart\n",
        "    labels = []\n",
        "    sizes = []\n",
        "    colors = [\"#FF6B6B\", \"#4ECDC4\", \"#FECA57\"]\n",
        "\n",
        "    if ai_generated > 0:\n",
        "        labels.append(f\"AI Generated\\n{ai_generated:.1f}%\")\n",
        "        sizes.append(ai_generated)\n",
        "\n",
        "    if human_edited > 0:\n",
        "        labels.append(f\"Human Edited\\n{human_edited:.1f}%\")\n",
        "        sizes.append(human_edited)\n",
        "\n",
        "    if new_content > 0:\n",
        "        labels.append(f\"New Content\\n{new_content:.1f}%\")\n",
        "        sizes.append(new_content)\n",
        "\n",
        "    pie_result = ax.pie(\n",
        "        sizes,\n",
        "        labels=labels,\n",
        "        colors=colors[: len(sizes)],\n",
        "        autopct=\"\",\n",
        "        startangle=90,\n",
        "        textprops={\"fontsize\": 10},\n",
        "    )\n",
        "\n",
        "    # ax.pie returns different number of values based on parameters\n",
        "    # We only need the wedges for our purposes\n",
        "    pie_result[0] if isinstance(pie_result, tuple) else pie_result\n",
        "\n",
        "    ax.set_title(\n",
        "        f\"Content Attribution - {analysis_data['article_name']}\",\n",
        "        fontsize=12,\n",
        "        fontweight=\"bold\",\n",
        "        pad=10,\n",
        "    )\n",
        "\n",
        "    # Save minimal chart\n",
        "    minimal_files = {}\n",
        "    if output_path:\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        for fmt in [\"png\", \"svg\"]:\n",
        "            minimal_file = os.path.join(\n",
        "                output_path, f\"{analysis_data['article_name']}_minimal.{fmt}\"\n",
        "            )\n",
        "            fig.savefig(\n",
        "                minimal_file,\n",
        "                dpi=300,\n",
        "                bbox_inches=\"tight\",\n",
        "                facecolor=\"white\",\n",
        "                edgecolor=\"none\",\n",
        "            )\n",
        "            minimal_files[fmt] = minimal_file\n",
        "\n",
        "        print(f\"üìä Minimal chart saved: {len(minimal_files)} formats\")\n",
        "\n",
        "    return fig, minimal_files\n",
        "\n",
        "\n",
        "print(\"üìÑ Footer graphics functions loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW4CPVrvkD0-"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 5: COMPLETE VISUALIZATION WORKFLOW\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def create_complete_visualization_suite(\n",
        "    analysis_data, output_path=None, data_type=\"individual\"\n",
        "):\n",
        "    \"\"\"Create a complete suite of visualizations for any analysis data.\"\"\"\n",
        "    print(\"üé® Creating complete visualization suite...\")\n",
        "    print(f\"üìä Data type: {data_type}\")\n",
        "\n",
        "    if not output_path:\n",
        "        print(\"‚ö†Ô∏è No output path provided - visualizations will not be saved\")\n",
        "\n",
        "    all_outputs = {\n",
        "        \"static_charts\": {},\n",
        "        \"interactive_charts\": {},\n",
        "        \"footer_graphics\": {},\n",
        "        \"saved_files\": {},\n",
        "    }\n",
        "\n",
        "    # Create main visualizations\n",
        "    if data_type == \"individual\":\n",
        "        # Individual article visualizations\n",
        "        charts, interactive, files = visualize_individual_article(\n",
        "            analysis_data, output_path, save_files=bool(output_path)\n",
        "        )\n",
        "\n",
        "        all_outputs[\"static_charts\"] = charts\n",
        "        all_outputs[\"interactive_charts\"] = {\"flow\": interactive}\n",
        "        all_outputs[\"saved_files\"].update(files)\n",
        "\n",
        "        # Create footer graphics\n",
        "        if output_path:\n",
        "            footer_fig, footer_files = generate_article_footer_graphics(\n",
        "                analysis_data, output_path\n",
        "            )\n",
        "            minimal_fig, minimal_files = create_minimal_attribution_chart(\n",
        "                analysis_data, output_path\n",
        "            )\n",
        "\n",
        "            all_outputs[\"footer_graphics\"][\"full\"] = footer_fig\n",
        "            all_outputs[\"footer_graphics\"][\"minimal\"] = minimal_fig\n",
        "            all_outputs[\"saved_files\"][\"footer\"] = footer_files\n",
        "            all_outputs[\"saved_files\"][\"minimal\"] = minimal_files\n",
        "\n",
        "            # Close footer figures\n",
        "            plt.close(footer_fig)\n",
        "            plt.close(minimal_fig)\n",
        "\n",
        "    elif data_type == \"trend\":\n",
        "        # Trend analysis visualizations\n",
        "        charts, interactive, files = visualize_trend_analysis(\n",
        "            analysis_data, output_path, save_files=bool(output_path)\n",
        "        )\n",
        "\n",
        "        all_outputs[\"static_charts\"] = charts\n",
        "        all_outputs[\"interactive_charts\"] = interactive\n",
        "        all_outputs[\"saved_files\"].update(files)\n",
        "\n",
        "    print(\"‚úÖ Visualization suite complete!\")\n",
        "    if output_path:\n",
        "        print(f\"üìÅ All files saved to: {output_path}\")\n",
        "\n",
        "    return all_outputs\n",
        "\n",
        "\n",
        "def visualize_from_existing_data(\n",
        "    analysis_data_or_file, output_path=None, data_type=\"auto\"\n",
        "):\n",
        "    \"\"\"Load and visualize from existing analysis data or file.\"\"\"\n",
        "    # Handle file input\n",
        "    if isinstance(analysis_data_or_file, str):\n",
        "        print(f\"üìñ Loading analysis data from: {analysis_data_or_file}\")\n",
        "        with open(analysis_data_or_file, encoding=\"utf-8\") as f:\n",
        "            analysis_data = json.load(f)\n",
        "    else:\n",
        "        analysis_data = analysis_data_or_file\n",
        "\n",
        "    # Auto-detect data type if not specified\n",
        "    if data_type == \"auto\":\n",
        "        if \"article_name\" in analysis_data and \"attribution_analysis\" in analysis_data:\n",
        "            data_type = \"individual\"\n",
        "        elif \"period\" in analysis_data and \"attribution_trends\" in analysis_data:\n",
        "            data_type = \"trend\"\n",
        "        else:\n",
        "            print(\"‚ùå Could not auto-detect data type\")\n",
        "            return None\n",
        "\n",
        "    print(f\"üîç Detected data type: {data_type}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    return create_complete_visualization_suite(analysis_data, output_path, data_type)\n",
        "\n",
        "\n",
        "# Quick access functions\n",
        "def viz_article(analysis_data, output_path=None):\n",
        "    \"\"\"Quick function to visualize individual article.\"\"\"\n",
        "    # Use the same output path structure as the analysis if not provided\n",
        "    if output_path is None and \"article_metadata\" in analysis_data:\n",
        "        # Try to derive output path from the analysis data\n",
        "        input_path = analysis_data[\"article_metadata\"].get(\"input_path\")\n",
        "        if input_path:\n",
        "            output_path = os.path.join(input_path, \"output\")\n",
        "\n",
        "    return create_complete_visualization_suite(analysis_data, output_path, \"individual\")\n",
        "\n",
        "\n",
        "def viz_trends(trend_data, output_path=None):\n",
        "    \"\"\"Quick function to visualize trend analysis.\"\"\"\n",
        "    return create_complete_visualization_suite(trend_data, output_path, \"trend\")\n",
        "\n",
        "\n",
        "print(\"üöÄ Complete visualization workflow loaded!\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä VISUALIZATION SYSTEM READY!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nQuick usage:\")\n",
        "print(\"# Visualize your existing analysis data\")\n",
        "print(\"viz_outputs = viz_article(combined_results, '/your/output/path')\")\n",
        "print(\"\\n# Visualize trend analysis\")\n",
        "print(\"trend_viz = viz_trends(trend_results, '/your/output/path')\")\n",
        "print(\"\\n# Create footer graphics only\")\n",
        "print(\"footer_fig, files = generate_article_footer_graphics(combined_results, '/path')\")\n",
        "print(\"\\nAll visualizations include:\")\n",
        "print(\"  üìà Static charts (PNG, SVG)\")\n",
        "print(\"  üîß Interactive charts (HTML)\")\n",
        "print(\"  üìÑ Footer graphics (publication-ready)\")\n",
        "print(\"  üíæ Multiple file formats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdR7tDxWkSJk"
      },
      "outputs": [],
      "source": [
        "viz_outputs = viz_article(combined_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}