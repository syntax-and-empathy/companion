{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wtrekell/soylent-army/blob/main/colab/ai_vs_human_v1.4c.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_8AIVCB-cME",
    "outputId": "d4c7d2db-8390-4015-8c7d-1dc4a233d7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration loaded. Ready to process articles.\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: SETUP AND CONFIGURATION\n",
    "\n",
    "# Core Python libraries\n",
    "import difflib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Data analysis libraries\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning and semantic analysis\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for the analysis.\"\"\"\n",
    "\n",
    "    VERSION_PREFIXES = [\"draft-\", \"refined-\", \"edited-\", \"final-\"]\n",
    "    VERSION_ORDER = {prefix: i for i, prefix in enumerate(VERSION_PREFIXES)}\n",
    "\n",
    "    MIN_SENTENCE_LENGTH = 10  # Minimum characters for a sentence\n",
    "    MAX_SENTENCE_LENGTH = 200  # Maximum characters for a sentence\n",
    "\n",
    "\n",
    "def setup_output_directories(base_path):\n",
    "    \"\"\"Create necessary output directories.\"\"\"\n",
    "    output_dir = base_path\n",
    "    archive_dir = os.path.join(base_path, \"archive\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(archive_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"‚úì Output directory ready: {output_dir}\")\n",
    "    print(f\"‚úì Archive directory ready: {archive_dir}\")\n",
    "\n",
    "    return output_dir, archive_dir\n",
    "\n",
    "\n",
    "print(\"üìã Configuration loaded. Ready to process articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3ky_mO3Fr3D",
    "outputId": "32b9e46a-5eea-4b9f-f48d-5a966d0146cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ ArticleVersions class loaded. Ready for data ingestion.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: DATA INGESTION & VALIDATION (STEP 1)\n",
    "\n",
    "\n",
    "class ArticleVersions:\n",
    "    \"\"\"Class to handle loading and validating article versions.\"\"\"\n",
    "\n",
    "    def __init__(self, article_name, input_path):\n",
    "        self.article_name = article_name\n",
    "        self.input_path = input_path\n",
    "        self.versions = {}\n",
    "        self.metadata = {\n",
    "            \"article_name\": article_name,\n",
    "            \"input_path\": input_path,\n",
    "            \"processing_timestamp\": datetime.now().isoformat(),\n",
    "            \"versions_found\": [],\n",
    "            \"validation_status\": \"pending\",\n",
    "        }\n",
    "\n",
    "    def load_versions(self):\n",
    "        \"\"\"Load all versions of an article from the specified path.\"\"\"\n",
    "        print(f\"\\nüìÅ Loading versions for article: {self.article_name}\")\n",
    "        print(f\"üìÇ Input path: {self.input_path}\")\n",
    "\n",
    "        for prefix in Config.VERSION_PREFIXES:\n",
    "            filename = f\"{prefix}{self.article_name}.md\"\n",
    "            filepath = os.path.join(self.input_path, filename)\n",
    "\n",
    "            if os.path.exists(filepath):\n",
    "                try:\n",
    "                    with open(filepath, encoding=\"utf-8\") as file:\n",
    "                        content = file.read()\n",
    "                        self.versions[prefix.rstrip(\"-\")] = {\n",
    "                            \"filename\": filename,\n",
    "                            \"filepath\": filepath,\n",
    "                            \"content\": content,\n",
    "                            \"loaded_at\": datetime.now().isoformat(),\n",
    "                            \"file_size\": len(content),\n",
    "                        }\n",
    "                        print(f\"  ‚úì Loaded: {filename} ({len(content)} characters)\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚úó Error loading {filename}: {e!s}\")\n",
    "            else:\n",
    "                print(f\"  - Not found: {filename}\")\n",
    "\n",
    "        self.metadata[\"versions_found\"] = list(self.versions.keys())\n",
    "        return self.versions\n",
    "\n",
    "    def validate_version_sequence(self):\n",
    "        \"\"\"Validate that we have the minimum required versions.\"\"\"\n",
    "        found_versions = set(self.versions.keys())\n",
    "        required_versions = [\"draft\", \"final\"]\n",
    "\n",
    "        missing_required = []\n",
    "        for version in required_versions:\n",
    "            if version not in found_versions:\n",
    "                missing_required.append(version)\n",
    "\n",
    "        validation_results = {\n",
    "            \"has_draft\": \"draft\" in found_versions,\n",
    "            \"has_final\": \"final\" in found_versions,\n",
    "            \"missing_required\": missing_required,\n",
    "            \"versions_found\": list(found_versions),\n",
    "            \"is_valid\": len(missing_required) == 0,\n",
    "        }\n",
    "\n",
    "        self.metadata[\"validation_results\"] = validation_results\n",
    "\n",
    "        if validation_results[\"is_valid\"]:\n",
    "            self.metadata[\"validation_status\"] = \"passed\"\n",
    "            print(f\"‚úì Validation passed: Found {len(found_versions)} versions\")\n",
    "        else:\n",
    "            self.metadata[\"validation_status\"] = \"failed\"\n",
    "            print(\"‚úó Validation failed: Missing required versions\")\n",
    "            print(f\"  Missing: {', '.join(missing_required)}\")\n",
    "\n",
    "        return validation_results\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a summary of loaded versions.\"\"\"\n",
    "        summary = {\n",
    "            \"article_name\": self.article_name,\n",
    "            \"input_path\": self.input_path,\n",
    "            \"versions_count\": len(self.versions),\n",
    "            \"validation_status\": self.metadata[\"validation_status\"],\n",
    "            \"file_sizes\": {},\n",
    "        }\n",
    "\n",
    "        for version, data in self.versions.items():\n",
    "            summary[\"file_sizes\"][version] = data[\"file_size\"]\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "print(\"üìñ ArticleVersions class loaded. Ready for data ingestion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhHY2pJqFvOr",
    "outputId": "f8166a46-9e06-4b45-d771-8331990b0f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TextPreprocessor class loaded. Ready for text processing.\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: TEXT PREPROCESSING (STEP 2)\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Class to handle text preprocessing and segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processed_versions = {}\n",
    "\n",
    "    def clean_markdown(self, text):\n",
    "        \"\"\"Clean markdown formatting while preserving content structure.\"\"\"\n",
    "        # Remove markdown formatting but keep the text\n",
    "\n",
    "        cleaned_text = text\n",
    "\n",
    "        # Apply patterns that need multiline flag\n",
    "        multiline_patterns = [\n",
    "            (r\"^\\s*[\\*\\-\\+]\\s+\", \"\"),  # Bullet points\n",
    "            (r\"^\\s*\\d+\\.\\s+\", \"\"),  # Numbered lists\n",
    "        ]\n",
    "\n",
    "        for pattern, replacement in multiline_patterns:\n",
    "            cleaned_text = re.sub(\n",
    "                pattern, replacement, cleaned_text, flags=re.MULTILINE\n",
    "            )\n",
    "\n",
    "        # Apply regular patterns\n",
    "        regular_patterns = [\n",
    "            (r\"^\\s*#{1,6}\\s+\", \"\"),  # Headers\n",
    "            (r\"\\*\\*(.*?)\\*\\*\", r\"\\1\"),  # Bold\n",
    "            (r\"\\*(.*?)\\*\", r\"\\1\"),  # Italic\n",
    "            (r\"`(.*?)`\", r\"\\1\"),  # Inline code\n",
    "            (r\"```.*?```\", \"\"),  # Code blocks\n",
    "            (r\"!\\[.*?\\]\\(.*?\\)\", \"\"),  # Images\n",
    "            (r\"\\[([^\\]]+)\\]\\([^\\)]+\\)\", r\"\\1\"),  # Links\n",
    "            (r\"\\n{3,}\", \"\\n\\n\"),  # Multiple newlines\n",
    "        ]\n",
    "\n",
    "        for pattern, replacement in regular_patterns:\n",
    "            cleaned_text = re.sub(pattern, replacement, cleaned_text)\n",
    "\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def segment_into_sentences(self, text):\n",
    "        \"\"\"Segment text into sentences with basic filtering.\"\"\"\n",
    "        # Simple sentence segmentation (can be enhanced with spaCy later if needed)\n",
    "        sentences = re.split(r\"[.!?]+\\s+\", text)\n",
    "\n",
    "        # Filter sentences\n",
    "        filtered_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if (\n",
    "                Config.MIN_SENTENCE_LENGTH\n",
    "                <= len(sentence)\n",
    "                <= Config.MAX_SENTENCE_LENGTH\n",
    "                and sentence\n",
    "            ):\n",
    "                filtered_sentences.append(sentence)\n",
    "\n",
    "        return filtered_sentences\n",
    "\n",
    "    def segment_into_paragraphs(self, text):\n",
    "        \"\"\"Segment text into paragraphs.\"\"\"\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "        return paragraphs\n",
    "\n",
    "    def process_version(self, version_name, raw_content):\n",
    "        \"\"\"Process a single version of the article.\"\"\"\n",
    "        print(f\"  Processing {version_name} version...\")\n",
    "\n",
    "        # Clean the markdown\n",
    "        cleaned_content = self.clean_markdown(raw_content)\n",
    "\n",
    "        # Segment into different units\n",
    "        sentences = self.segment_into_sentences(cleaned_content)\n",
    "        paragraphs = self.segment_into_paragraphs(cleaned_content)\n",
    "\n",
    "        # Calculate basic statistics\n",
    "        stats = {\n",
    "            \"character_count\": len(cleaned_content),\n",
    "            \"word_count\": len(cleaned_content.split()),\n",
    "            \"sentence_count\": len(sentences),\n",
    "            \"paragraph_count\": len(paragraphs),\n",
    "            \"avg_sentence_length\": sum(len(s) for s in sentences) / len(sentences)\n",
    "            if sentences\n",
    "            else 0,\n",
    "            \"avg_paragraph_length\": sum(len(p) for p in paragraphs) / len(paragraphs)\n",
    "            if paragraphs\n",
    "            else 0,\n",
    "        }\n",
    "\n",
    "        processed_data = {\n",
    "            \"version_name\": version_name,\n",
    "            \"raw_content\": raw_content,\n",
    "            \"cleaned_content\": cleaned_content,\n",
    "            \"sentences\": sentences,\n",
    "            \"paragraphs\": paragraphs,\n",
    "            \"statistics\": stats,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        self.processed_versions[version_name] = processed_data\n",
    "\n",
    "        print(\n",
    "            f\"    ‚úì {stats['sentence_count']} sentences, {stats['paragraph_count']} paragraphs\"\n",
    "        )\n",
    "        print(\n",
    "            f\"    ‚úì {stats['word_count']} words, {stats['character_count']} characters\"\n",
    "        )\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "    def process_all_versions(self, article_versions):\n",
    "        \"\"\"Process all versions of an article.\"\"\"\n",
    "        print(\"\\nüîÑ Preprocessing text for all versions...\")\n",
    "\n",
    "        for version_name, version_data in article_versions.versions.items():\n",
    "            self.process_version(version_name, version_data[\"content\"])\n",
    "\n",
    "        return self.processed_versions\n",
    "\n",
    "    def get_processing_summary(self):\n",
    "        \"\"\"Get a summary of processing results.\"\"\"\n",
    "        summary = {}\n",
    "        for version_name, data in self.processed_versions.items():\n",
    "            summary[version_name] = data[\"statistics\"]\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "print(\"üîß TextPreprocessor class loaded. Ready for text processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4bdYVJdFyQD",
    "outputId": "b9398167-5eac-4bae-9045-40e57bcf79e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Ready to process your article!\n",
      "Run: article_versions, preprocessor = process_article_interactive()\n",
      "\n",
      "Make sure your markdown files are named:\n",
      "- draft-your-article-name.md\n",
      "- refined-your-article-name.md\n",
      "- edited-your-article-name.md\n",
      "- final-your-article-name.md\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: EXECUTION FUNCTIONS AND CHECKPOINT MANAGEMENT\n",
    "\n",
    "\n",
    "def save_checkpoint_data(\n",
    "    article_versions, preprocessor, output_path, checkpoint_name=\"steps_1_2\"\n",
    "):\n",
    "    \"\"\"Save checkpoint data for review.\"\"\"\n",
    "    checkpoint_data = {\n",
    "        \"checkpoint_name\": checkpoint_name,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"article_metadata\": article_versions.metadata,\n",
    "        \"processing_summary\": preprocessor.get_processing_summary(),\n",
    "        \"validation_results\": article_versions.metadata.get(\"validation_results\", {}),\n",
    "        \"article_summary\": article_versions.get_summary(),\n",
    "    }\n",
    "\n",
    "    checkpoint_file = f\"{output_path}/{article_versions.article_name}_checkpoint_{checkpoint_name}.json\"\n",
    "\n",
    "    with open(checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nüíæ Checkpoint saved: {checkpoint_file}\")\n",
    "    return checkpoint_data\n",
    "\n",
    "\n",
    "def run_steps_1_2(article_name, input_path, base_output_path):\n",
    "    \"\"\"Run steps 1-2 for a given article.\"\"\"\n",
    "    print(f\"üöÄ Starting Steps 1-2 for article: {article_name}\")\n",
    "    print(f\"üìÇ Input path: {input_path}\")\n",
    "\n",
    "    output_dir, archive_dir = setup_output_directories(base_output_path)\n",
    "\n",
    "    # Step 1: Data Ingestion & Validation\n",
    "    article_versions = ArticleVersions(article_name, input_path)\n",
    "    article_versions.load_versions()\n",
    "    validation_results = article_versions.validate_version_sequence()\n",
    "\n",
    "    if not validation_results[\"is_valid\"]:\n",
    "        print(\"‚ùå Cannot proceed: Missing required versions (draft and final)\")\n",
    "        return None, None\n",
    "\n",
    "    # Step 2: Text Preprocessing\n",
    "    preprocessor = TextPreprocessor()\n",
    "    preprocessor.process_all_versions(article_versions)\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint_data(article_versions, preprocessor, output_dir)\n",
    "\n",
    "    print(\"\\n‚úÖ Steps 1-2 completed successfully!\")\n",
    "    print(\"üìä Processing Summary:\")\n",
    "    for version, stats in preprocessor.get_processing_summary().items():\n",
    "        print(\n",
    "            f\"  {version}: {stats['word_count']} words, {stats['sentence_count']} sentences\"\n",
    "        )\n",
    "\n",
    "    return article_versions, preprocessor\n",
    "\n",
    "\n",
    "def get_user_inputs():\n",
    "    \"\"\"Get user inputs for processing.\"\"\"\n",
    "    print(\"üìù Please provide the following information:\")\n",
    "\n",
    "    article_name = input(\"Enter article name (without .md extension): \").strip()\n",
    "    input_path = input(\n",
    "        \"Enter full path to input folder containing markdown files: \"\n",
    "    ).strip()\n",
    "    base_output_path = input(\"Enter full path to base output folder: \").strip()\n",
    "\n",
    "    print(\"\\nüìã Configuration:\")\n",
    "    print(f\"  Article name: {article_name}\")\n",
    "    print(f\"  Input path: {input_path}\")\n",
    "    print(f\"  Output path: {base_output_path}\")\n",
    "\n",
    "    confirm = input(\"\\nProceed with these settings? (y/n): \").strip().lower()\n",
    "\n",
    "    if confirm == \"y\":\n",
    "        return article_name, input_path, base_output_path\n",
    "    else:\n",
    "        print(\"‚ùå Cancelled. Run get_user_inputs() again to restart.\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def process_article_interactive():\n",
    "    \"\"\"Process an article with interactive inputs.\"\"\"\n",
    "    article_name, input_path, base_output_path = get_user_inputs()\n",
    "\n",
    "    if article_name and input_path and base_output_path:\n",
    "        return run_steps_1_2(article_name, input_path, base_output_path)\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "print(\"üìã Ready to process your article!\")\n",
    "print(\"Run: article_versions, preprocessor = process_article_interactive()\")\n",
    "print(\"\\nMake sure your markdown files are named:\")\n",
    "print(\"- draft-your-article-name.md\")\n",
    "print(\"- refined-your-article-name.md\")\n",
    "print(\"- edited-your-article-name.md\")\n",
    "print(\"- final-your-article-name.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-hysVC7J3F0",
    "outputId": "aacafeac-bf55-471a-9a63-4fd5c182aa46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Please provide the following information:\n",
      "Enter article name (without .md extension): markup-languages\n",
      "Enter full path to input folder containing markdown files: /content/\n",
      "Enter full path to base output folder: /content/output/\n",
      "\n",
      "üìã Configuration:\n",
      "  Article name: markup-languages\n",
      "  Input path: /content/\n",
      "  Output path: /content/output/\n",
      "\n",
      "Proceed with these settings? (y/n): y\n",
      "üöÄ Starting Steps 1-2 for article: markup-languages\n",
      "üìÇ Input path: /content/\n",
      "‚úì Output directory ready: /content/output/\n",
      "‚úì Archive directory ready: /content/output/archive\n",
      "\n",
      "üìÅ Loading versions for article: markup-languages\n",
      "üìÇ Input path: /content/\n",
      "  ‚úì Loaded: draft-markup-languages.md (5667 characters)\n",
      "  ‚úì Loaded: refined-markup-languages.md (7301 characters)\n",
      "  ‚úì Loaded: edited-markup-languages.md (7575 characters)\n",
      "  ‚úì Loaded: final-markup-languages.md (6327 characters)\n",
      "‚úì Validation passed: Found 4 versions\n",
      "\n",
      "üîÑ Preprocessing text for all versions...\n",
      "  Processing draft version...\n",
      "    ‚úì 42 sentences, 29 paragraphs\n",
      "    ‚úì 863 words, 5535 characters\n",
      "  Processing refined version...\n",
      "    ‚úì 55 sentences, 31 paragraphs\n",
      "    ‚úì 1128 words, 7213 characters\n",
      "  Processing edited version...\n",
      "    ‚úì 39 sentences, 35 paragraphs\n",
      "    ‚úì 1066 words, 7479 characters\n",
      "  Processing final version...\n",
      "    ‚úì 41 sentences, 38 paragraphs\n",
      "    ‚úì 892 words, 6258 characters\n",
      "\n",
      "üíæ Checkpoint saved: /content/output//markup-languages_checkpoint_steps_1_2.json\n",
      "\n",
      "‚úÖ Steps 1-2 completed successfully!\n",
      "üìä Processing Summary:\n",
      "  draft: 863 words, 42 sentences\n",
      "  refined: 1128 words, 55 sentences\n",
      "  edited: 1066 words, 39 sentences\n",
      "  final: 892 words, 41 sentences\n"
     ]
    }
   ],
   "source": [
    "article_versions, preprocessor = process_article_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGzjFLxSMguC",
    "outputId": "fe745658-4f1e-4d08-9f6d-ba9e45230f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Dependencies installed and imported successfully!\n",
      "ü§ñ Loading SentenceTransformer model (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semantic model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: INSTALL ANALYSIS PACKAGES\n",
    "\n",
    "print(\"üì¶ Dependencies installed and imported successfully!\")\n",
    "print(\"ü§ñ Loading SentenceTransformer model (this may take a moment)...\")\n",
    "\n",
    "# Load the semantic similarity model\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"‚úÖ Semantic model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0OQ62SiMjnr",
    "outputId": "fb524127-26c1-414e-b2d7-c5eef9a848fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SimilarityAnalyzer class loaded. Ready for similarity analysis.\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: SIMILARITY ANALYSIS\n",
    "\n",
    "\n",
    "class SimilarityAnalyzer:\n",
    "    \"\"\"Class to handle lexical and semantic similarity analysis.\"\"\"\n",
    "\n",
    "    def __init__(self, semantic_model):\n",
    "        self.semantic_model = semantic_model\n",
    "        self.similarity_results = {}\n",
    "\n",
    "    def calculate_lexical_similarity(self, text1, text2):\n",
    "        \"\"\"Calculate lexical similarity using multiple metrics.\"\"\"\n",
    "        # Jaccard similarity (word-level)\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        jaccard = (\n",
    "            len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "            if words1.union(words2)\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        # Edit distance similarity (character-level)\n",
    "        sequence_matcher = difflib.SequenceMatcher(None, text1, text2)\n",
    "        edit_similarity = sequence_matcher.ratio()\n",
    "\n",
    "        # TF-IDF cosine similarity\n",
    "        vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "            tfidf_similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[\n",
    "                0\n",
    "            ][0]\n",
    "        except:\n",
    "            tfidf_similarity = 0.0\n",
    "\n",
    "        return {\n",
    "            \"jaccard_similarity\": jaccard,\n",
    "            \"edit_similarity\": edit_similarity,\n",
    "            \"tfidf_similarity\": tfidf_similarity,\n",
    "            \"lexical_average\": (jaccard + edit_similarity + tfidf_similarity) / 3,\n",
    "        }\n",
    "\n",
    "    def calculate_semantic_similarity(self, text1, text2):\n",
    "        \"\"\"Calculate semantic similarity using sentence embeddings.\"\"\"\n",
    "        # Get embeddings\n",
    "        embeddings = self.semantic_model.encode([text1, text2])\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "\n",
    "        return {\n",
    "            \"semantic_similarity\": float(similarity),\n",
    "            \"embedding_dim\": len(embeddings[0]),\n",
    "        }\n",
    "\n",
    "    def calculate_sentence_level_similarities(\n",
    "        self, sentences1, sentences2, version1, version2\n",
    "    ):\n",
    "        \"\"\"Calculate similarities at sentence level between two versions.\"\"\"\n",
    "        print(f\"    Analyzing {len(sentences1)} vs {len(sentences2)} sentences...\")\n",
    "\n",
    "        sentence_similarities = []\n",
    "\n",
    "        # Calculate all pairwise similarities\n",
    "        for i, sent1 in enumerate(sentences1):\n",
    "            best_match = {\"index\": -1, \"lexical\": 0, \"semantic\": 0, \"combined\": 0}\n",
    "\n",
    "            for j, sent2 in enumerate(sentences2):\n",
    "                # Calculate similarities\n",
    "                lexical = self.calculate_lexical_similarity(sent1, sent2)\n",
    "                semantic = self.calculate_semantic_similarity(sent1, sent2)\n",
    "\n",
    "                # Combined score (weighted average)\n",
    "                combined = (\n",
    "                    lexical[\"lexical_average\"] + semantic[\"semantic_similarity\"]\n",
    "                ) / 2\n",
    "\n",
    "                if combined > best_match[\"combined\"]:\n",
    "                    best_match = {\n",
    "                        \"index\": j,\n",
    "                        \"lexical\": lexical[\"lexical_average\"],\n",
    "                        \"semantic\": semantic[\"semantic_similarity\"],\n",
    "                        \"combined\": combined,\n",
    "                        \"target_sentence\": sent2,\n",
    "                    }\n",
    "\n",
    "            sentence_similarities.append(\n",
    "                {\"source_index\": i, \"source_sentence\": sent1, \"best_match\": best_match}\n",
    "            )\n",
    "\n",
    "        return sentence_similarities\n",
    "\n",
    "    def analyze_version_pair(\n",
    "        self, version1_data, version2_data, version1_name, version2_name\n",
    "    ):\n",
    "        \"\"\"Analyze similarities between two versions.\"\"\"\n",
    "        print(f\"  üîç Analyzing {version1_name} ‚Üí {version2_name}\")\n",
    "\n",
    "        # Full text similarity\n",
    "        full_text_lexical = self.calculate_lexical_similarity(\n",
    "            version1_data[\"cleaned_content\"], version2_data[\"cleaned_content\"]\n",
    "        )\n",
    "        full_text_semantic = self.calculate_semantic_similarity(\n",
    "            version1_data[\"cleaned_content\"], version2_data[\"cleaned_content\"]\n",
    "        )\n",
    "\n",
    "        # Sentence-level analysis\n",
    "        sentence_analysis = self.calculate_sentence_level_similarities(\n",
    "            version1_data[\"sentences\"],\n",
    "            version2_data[\"sentences\"],\n",
    "            version1_name,\n",
    "            version2_name,\n",
    "        )\n",
    "\n",
    "        # Paragraph-level similarity\n",
    "        para_lexical = self.calculate_lexical_similarity(\n",
    "            \" \".join(version1_data[\"paragraphs\"]), \" \".join(version2_data[\"paragraphs\"])\n",
    "        )\n",
    "        para_semantic = self.calculate_semantic_similarity(\n",
    "            \" \".join(version1_data[\"paragraphs\"]), \" \".join(version2_data[\"paragraphs\"])\n",
    "        )\n",
    "\n",
    "        # Aggregate sentence similarities\n",
    "        sentence_similarities = [s[\"best_match\"][\"combined\"] for s in sentence_analysis]\n",
    "        avg_sentence_similarity = (\n",
    "            np.mean(sentence_similarities) if sentence_similarities else 0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"version_pair\": f\"{version1_name}_to_{version2_name}\",\n",
    "            \"full_text\": {\n",
    "                \"lexical\": full_text_lexical,\n",
    "                \"semantic\": full_text_semantic,\n",
    "                \"combined\": (\n",
    "                    full_text_lexical[\"lexical_average\"]\n",
    "                    + full_text_semantic[\"semantic_similarity\"]\n",
    "                )\n",
    "                / 2,\n",
    "            },\n",
    "            \"sentence_level\": {\n",
    "                \"average_similarity\": avg_sentence_similarity,\n",
    "                \"individual_similarities\": sentence_similarities,\n",
    "                \"detailed_analysis\": sentence_analysis,\n",
    "            },\n",
    "            \"paragraph_level\": {\n",
    "                \"lexical\": para_lexical,\n",
    "                \"semantic\": para_semantic,\n",
    "                \"combined\": (\n",
    "                    para_lexical[\"lexical_average\"]\n",
    "                    + para_semantic[\"semantic_similarity\"]\n",
    "                )\n",
    "                / 2,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def analyze_all_versions(self, processed_versions):\n",
    "        \"\"\"Analyze similarities between all version pairs.\"\"\"\n",
    "        print(\"\\nüîç Step 3: Similarity Analysis\")\n",
    "\n",
    "        version_names = list(processed_versions.keys())\n",
    "        version_order = [\"draft\", \"refined\", \"edited\", \"final\"]\n",
    "\n",
    "        # Sort versions by expected order\n",
    "        sorted_versions = []\n",
    "        for expected in version_order:\n",
    "            if expected in version_names:\n",
    "                sorted_versions.append(expected)\n",
    "\n",
    "        # Sequential analysis (draft‚Üírefined‚Üíedited‚Üífinal)\n",
    "        sequential_results = []\n",
    "        for i in range(len(sorted_versions) - 1):\n",
    "            current_version = sorted_versions[i]\n",
    "            next_version = sorted_versions[i + 1]\n",
    "\n",
    "            result = self.analyze_version_pair(\n",
    "                processed_versions[current_version],\n",
    "                processed_versions[next_version],\n",
    "                current_version,\n",
    "                next_version,\n",
    "            )\n",
    "            sequential_results.append(result)\n",
    "\n",
    "        # Draft to final comparison\n",
    "        draft_to_final = None\n",
    "        if \"draft\" in version_names and \"final\" in version_names:\n",
    "            print(\"  üîç Analyzing draft ‚Üí final (overall change)\")\n",
    "            draft_to_final = self.analyze_version_pair(\n",
    "                processed_versions[\"draft\"],\n",
    "                processed_versions[\"final\"],\n",
    "                \"draft\",\n",
    "                \"final\",\n",
    "            )\n",
    "\n",
    "        self.similarity_results = {\n",
    "            \"sequential_analysis\": sequential_results,\n",
    "            \"draft_to_final\": draft_to_final,\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"versions_analyzed\": sorted_versions,\n",
    "        }\n",
    "\n",
    "        return self.similarity_results\n",
    "\n",
    "\n",
    "print(\"üîç SimilarityAnalyzer class loaded. Ready for similarity analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIDJ9C50MrQ3",
    "outputId": "c9c305df-1a8c-4885-99b1-b5679e54ec28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç AttributionMapper class loaded. Ready for attribution analysis.\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: ATTRIBUTION MAPPING (STEP 4)\n",
    "\n",
    "\n",
    "class AttributionMapper:\n",
    "    \"\"\"Class to track content attribution across versions.\"\"\"\n",
    "\n",
    "    def __init__(self, semantic_model, similarity_threshold=0.3):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.semantic_model = semantic_model\n",
    "        self.attribution_results = {}\n",
    "\n",
    "    def trace_sentence_origins(self, processed_versions, similarity_results):\n",
    "        \"\"\"Trace each final sentence back to its earliest appearance.\"\"\"\n",
    "        print(\"\\nüìç Step 4: Attribution Mapping\")\n",
    "\n",
    "        version_order = [\"draft\", \"refined\", \"edited\", \"final\"]\n",
    "        available_versions = [v for v in version_order if v in processed_versions]\n",
    "\n",
    "        if \"final\" not in available_versions:\n",
    "            print(\"‚ùå Cannot perform attribution - final version not found\")\n",
    "            return None\n",
    "\n",
    "        final_sentences = processed_versions[\"final\"][\"sentences\"]\n",
    "        sentence_attributions = []\n",
    "\n",
    "        print(f\"  üìù Tracing {len(final_sentences)} final sentences...\")\n",
    "\n",
    "        for final_idx, final_sentence in enumerate(final_sentences):\n",
    "            attribution = {\n",
    "                \"final_index\": final_idx,\n",
    "                \"final_sentence\": final_sentence,\n",
    "                \"origin_version\": None,\n",
    "                \"origin_index\": None,\n",
    "                \"similarity_scores\": {},\n",
    "                \"modification_path\": [],\n",
    "            }\n",
    "\n",
    "            # Check each previous version (in reverse order to find earliest origin)\n",
    "            for version in reversed(available_versions[:-1]):  # Exclude 'final'\n",
    "                version_sentences = processed_versions[version][\"sentences\"]\n",
    "\n",
    "                best_match = {\"index\": -1, \"similarity\": 0, \"sentence\": \"\"}\n",
    "\n",
    "                for sent_idx, version_sentence in enumerate(version_sentences):\n",
    "                    # Calculate similarity\n",
    "                    lexical = self._quick_lexical_similarity(\n",
    "                        final_sentence, version_sentence\n",
    "                    )\n",
    "                    semantic = self._quick_semantic_similarity(\n",
    "                        final_sentence, version_sentence\n",
    "                    )\n",
    "                    combined = (lexical + semantic) / 2\n",
    "\n",
    "                    if combined > best_match[\"similarity\"]:\n",
    "                        best_match = {\n",
    "                            \"index\": sent_idx,\n",
    "                            \"similarity\": combined,\n",
    "                            \"sentence\": version_sentence,\n",
    "                        }\n",
    "\n",
    "                attribution[\"similarity_scores\"][version] = best_match[\"similarity\"]\n",
    "\n",
    "                # If similarity is above threshold, this could be the origin\n",
    "                if best_match[\"similarity\"] >= self.similarity_threshold:\n",
    "                    if (\n",
    "                        attribution[\"origin_version\"] is None\n",
    "                    ):  # First match found (earliest version)\n",
    "                        attribution[\"origin_version\"] = version\n",
    "                        attribution[\"origin_index\"] = best_match[\"index\"]\n",
    "\n",
    "                    attribution[\"modification_path\"].append(\n",
    "                        {\n",
    "                            \"version\": version,\n",
    "                            \"similarity\": best_match[\"similarity\"],\n",
    "                            \"sentence\": best_match[\"sentence\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # If no origin found, it's new content\n",
    "            if attribution[\"origin_version\"] is None:\n",
    "                attribution[\"origin_version\"] = \"new_in_final\"\n",
    "\n",
    "            sentence_attributions.append(attribution)\n",
    "\n",
    "        return sentence_attributions\n",
    "\n",
    "    def _quick_lexical_similarity(self, text1, text2):\n",
    "        \"\"\"Quick lexical similarity calculation.\"\"\"\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        if not words1 and not words2:\n",
    "            return 1.0\n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        return len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "    def _quick_semantic_similarity(self, text1, text2):\n",
    "        \"\"\"Quick semantic similarity calculation.\"\"\"\n",
    "        embeddings = self.semantic_model.encode([text1, text2])\n",
    "        return float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])\n",
    "\n",
    "    def calculate_attribution_statistics(self, sentence_attributions):\n",
    "        \"\"\"Calculate overall attribution statistics.\"\"\"\n",
    "        total_sentences = len(sentence_attributions)\n",
    "\n",
    "        # Count by origin version\n",
    "        origin_counts = defaultdict(int)\n",
    "        for attribution in sentence_attributions:\n",
    "            origin_counts[attribution[\"origin_version\"]] += 1\n",
    "\n",
    "        # Calculate percentages\n",
    "        origin_percentages = {}\n",
    "        for version, count in origin_counts.items():\n",
    "            origin_percentages[version] = {\n",
    "                \"count\": count,\n",
    "                \"percentage\": (count / total_sentences) * 100,\n",
    "            }\n",
    "\n",
    "        # Calculate modification statistics\n",
    "        modification_stats = {\n",
    "            \"high_similarity\": 0,  # >0.8\n",
    "            \"medium_similarity\": 0,  # 0.5-0.8\n",
    "            \"low_similarity\": 0,  # 0.3-0.5\n",
    "            \"new_content\": 0,  # <0.3 or new_in_final\n",
    "        }\n",
    "\n",
    "        for attribution in sentence_attributions:\n",
    "            if attribution[\"origin_version\"] == \"new_in_final\":\n",
    "                modification_stats[\"new_content\"] += 1\n",
    "            else:\n",
    "                # Get highest similarity score\n",
    "                max_similarity = (\n",
    "                    max(attribution[\"similarity_scores\"].values())\n",
    "                    if attribution[\"similarity_scores\"]\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "                if max_similarity > 0.8:\n",
    "                    modification_stats[\"high_similarity\"] += 1\n",
    "                elif max_similarity > 0.5:\n",
    "                    modification_stats[\"medium_similarity\"] += 1\n",
    "                elif max_similarity > 0.3:\n",
    "                    modification_stats[\"low_similarity\"] += 1\n",
    "                else:\n",
    "                    modification_stats[\"new_content\"] += 1\n",
    "\n",
    "        # Convert to percentages\n",
    "        modification_percentages = {}\n",
    "        for category, count in modification_stats.items():\n",
    "            modification_percentages[category] = {\n",
    "                \"count\": count,\n",
    "                \"percentage\": (count / total_sentences) * 100,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"total_sentences\": total_sentences,\n",
    "            \"origin_distribution\": origin_percentages,\n",
    "            \"modification_distribution\": modification_percentages,\n",
    "        }\n",
    "\n",
    "    def analyze_attribution(self, processed_versions, similarity_results):\n",
    "        \"\"\"Perform complete attribution analysis.\"\"\"\n",
    "        sentence_attributions = self.trace_sentence_origins(\n",
    "            processed_versions, similarity_results\n",
    "        )\n",
    "\n",
    "        if sentence_attributions is None:\n",
    "            return None\n",
    "\n",
    "        attribution_statistics = self.calculate_attribution_statistics(\n",
    "            sentence_attributions\n",
    "        )\n",
    "\n",
    "        self.attribution_results = {\n",
    "            \"sentence_attributions\": sentence_attributions,\n",
    "            \"statistics\": attribution_statistics,\n",
    "            \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "            \"similarity_threshold\": self.similarity_threshold,\n",
    "        }\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\nüìä Attribution Summary:\")\n",
    "        print(\n",
    "            f\"  Total sentences in final: {attribution_statistics['total_sentences']}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n  Origin Distribution:\")\n",
    "        for version, data in attribution_statistics[\"origin_distribution\"].items():\n",
    "            print(\n",
    "                f\"    {version}: {data['count']} sentences ({data['percentage']:.1f}%)\"\n",
    "            )\n",
    "\n",
    "        print(\"\\n  Modification Levels:\")\n",
    "        for category, data in attribution_statistics[\n",
    "            \"modification_distribution\"\n",
    "        ].items():\n",
    "            print(\n",
    "                f\"    {category}: {data['count']} sentences ({data['percentage']:.1f}%)\"\n",
    "            )\n",
    "\n",
    "        return self.attribution_results\n",
    "\n",
    "\n",
    "print(\"üìç AttributionMapper class loaded. Ready for attribution analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbD5vnmTMvo5",
    "outputId": "8a16f45c-7c3d-4394-ce9b-fd367f578220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Execution functions loaded. Ready to run complete analysis!\n",
      "\n",
      "To run the complete analysis:\n",
      "combined_results, footer_metrics = run_steps_3_4(article_versions, preprocessor, 'your_output_path')\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: COMBINED EXECUTION FUNCTION\n",
    "\n",
    "\n",
    "def run_steps_3_4(article_versions, preprocessor, output_path):\n",
    "    \"\"\"Run steps 3-4: Similarity Analysis and Attribution Mapping.\"\"\"\n",
    "    print(f\"üöÄ Starting Steps 3-4 for article: {article_versions.article_name}\")\n",
    "\n",
    "    # Step 3: Similarity Analysis\n",
    "    similarity_analyzer = SimilarityAnalyzer(semantic_model)\n",
    "    similarity_results = similarity_analyzer.analyze_all_versions(\n",
    "        preprocessor.processed_versions\n",
    "    )\n",
    "\n",
    "    # Step 4: Attribution Mapping\n",
    "    attribution_mapper = AttributionMapper(semantic_model, similarity_threshold=0.3)\n",
    "    attribution_results = attribution_mapper.analyze_attribution(\n",
    "        preprocessor.processed_versions, similarity_results\n",
    "    )\n",
    "\n",
    "    # Combine results\n",
    "    combined_results = {\n",
    "        \"article_name\": article_versions.article_name,\n",
    "        \"analysis_timestamp\": datetime.now().isoformat(),\n",
    "        \"article_metadata\": article_versions.metadata,\n",
    "        \"processing_summary\": preprocessor.get_processing_summary(),\n",
    "        \"similarity_analysis\": similarity_results,\n",
    "        \"attribution_analysis\": attribution_results,\n",
    "    }\n",
    "\n",
    "    # Save comprehensive results\n",
    "    results_file = (\n",
    "        f\"{output_path}/{article_versions.article_name}_complete_analysis.json\"\n",
    "    )\n",
    "    with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nüíæ Complete analysis saved: {results_file}\")\n",
    "\n",
    "    # Generate summary metrics for article footer\n",
    "    footer_metrics = generate_footer_metrics(combined_results)\n",
    "\n",
    "    # Save footer metrics separately\n",
    "    footer_file = f\"{output_path}/{article_versions.article_name}_footer_metrics.json\"\n",
    "    with open(footer_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(footer_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"üìä Footer metrics saved: {footer_file}\")\n",
    "\n",
    "    return combined_results, footer_metrics\n",
    "\n",
    "\n",
    "def generate_footer_metrics(combined_results):\n",
    "    \"\"\"Generate clean metrics for article footer.\"\"\"\n",
    "    # Get processing stats\n",
    "    processing = combined_results[\"processing_summary\"]\n",
    "\n",
    "    # Get attribution stats\n",
    "    if combined_results[\"attribution_analysis\"]:\n",
    "        attribution = combined_results[\"attribution_analysis\"][\"statistics\"]\n",
    "        origin_dist = attribution[\"origin_distribution\"]\n",
    "        modification_dist = attribution[\"modification_distribution\"]\n",
    "    else:\n",
    "        origin_dist = {}\n",
    "        modification_dist = {}\n",
    "\n",
    "    # Get similarity stats (draft to final)\n",
    "    draft_to_final = combined_results[\"similarity_analysis\"][\"draft_to_final\"]\n",
    "    overall_similarity = (\n",
    "        draft_to_final[\"full_text\"][\"combined\"] if draft_to_final else 0\n",
    "    )\n",
    "\n",
    "    footer_metrics = {\n",
    "        \"article_name\": combined_results[\"article_name\"],\n",
    "        \"word_progression\": {\n",
    "            \"draft\": processing.get(\"draft\", {}).get(\"word_count\", 0),\n",
    "            \"final\": processing.get(\"final\", {}).get(\"word_count\", 0),\n",
    "            \"change_percentage\": 0,\n",
    "        },\n",
    "        \"content_retention\": {\n",
    "            \"overall_similarity\": round(overall_similarity * 100, 1),\n",
    "            \"content_origins\": {},\n",
    "        },\n",
    "        \"modification_summary\": {},\n",
    "        \"generated_at\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    # Calculate word change percentage\n",
    "    if footer_metrics[\"word_progression\"][\"draft\"] > 0:\n",
    "        draft_words = footer_metrics[\"word_progression\"][\"draft\"]\n",
    "        final_words = footer_metrics[\"word_progression\"][\"final\"]\n",
    "        change = ((final_words - draft_words) / draft_words) * 100\n",
    "        footer_metrics[\"word_progression\"][\"change_percentage\"] = round(change, 1)\n",
    "\n",
    "    # Simplify origin distribution for footer\n",
    "    for version, data in origin_dist.items():\n",
    "        if version != \"new_in_final\":\n",
    "            footer_metrics[\"content_retention\"][\"content_origins\"][version] = round(\n",
    "                data[\"percentage\"], 1\n",
    "            )\n",
    "\n",
    "    # Simplify modification distribution\n",
    "    for category, data in modification_dist.items():\n",
    "        clean_category = category.replace(\"_\", \" \").title()\n",
    "        footer_metrics[\"modification_summary\"][clean_category] = round(\n",
    "            data[\"percentage\"], 1\n",
    "        )\n",
    "\n",
    "    return footer_metrics\n",
    "\n",
    "\n",
    "print(\"üéØ Execution functions loaded. Ready to run complete analysis!\")\n",
    "print(\"\\nTo run the complete analysis:\")\n",
    "print(\n",
    "    \"combined_results, footer_metrics = run_steps_3_4(article_versions, preprocessor, 'your_output_path')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7yqzZneMzgn",
    "outputId": "12c65ad0-06df-4595-b9d7-9e7e6d5bc394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Quick execution function available:\n",
      "combined_results, footer_metrics = run_complete_analysis_from_existing(article_versions, preprocessor)\n"
     ]
    }
   ],
   "source": [
    "# CELL 10: QUICK EXECUTION FOR EXISTING DATA\n",
    "\n",
    "\n",
    "def run_complete_analysis_from_existing(article_versions, preprocessor):\n",
    "    \"\"\"Run steps 3-4 using the output path from existing data.\"\"\"\n",
    "    # Find where the Step 1-2 checkpoint was actually saved\n",
    "    base_path = article_versions.input_path\n",
    "\n",
    "    # Check for the nested output structure that was created in Steps 1-2\n",
    "    nested_output_path = os.path.join(base_path, \"output\", \"output\")\n",
    "    regular_output_path = os.path.join(base_path, \"output\")\n",
    "\n",
    "    # Use the path where the checkpoint file exists\n",
    "    checkpoint_file = \"markup-languages_checkpoint_steps_1_2.json\"\n",
    "\n",
    "    if os.path.exists(os.path.join(nested_output_path, checkpoint_file)):\n",
    "        output_path = nested_output_path\n",
    "        print(f\"üìÇ Using nested output path: {output_path}\")\n",
    "    elif os.path.exists(os.path.join(regular_output_path, checkpoint_file)):\n",
    "        output_path = regular_output_path\n",
    "        print(f\"üìÇ Using regular output path: {output_path}\")\n",
    "    else:\n",
    "        # Create regular output path as fallback\n",
    "        output_path = regular_output_path\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        print(f\"üìÇ Created output path: {output_path}\")\n",
    "\n",
    "    return run_steps_3_4(article_versions, preprocessor, output_path)\n",
    "\n",
    "\n",
    "print(\"‚ö° Quick execution function available:\")\n",
    "print(\n",
    "    \"combined_results, footer_metrics = run_complete_analysis_from_existing(article_versions, preprocessor)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYliXyeZM17X",
    "outputId": "a87ff862-caa9-4030-af4e-bb908a8cd12c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Using regular output path: /content/output\n",
      "üöÄ Starting Steps 3-4 for article: markup-languages\n",
      "\n",
      "üîç Step 3: Similarity Analysis\n",
      "  üîç Analyzing draft ‚Üí refined\n",
      "    Analyzing 42 vs 55 sentences...\n",
      "  üîç Analyzing refined ‚Üí edited\n",
      "    Analyzing 55 vs 39 sentences...\n",
      "  üîç Analyzing edited ‚Üí final\n",
      "    Analyzing 39 vs 41 sentences...\n",
      "  üîç Analyzing draft ‚Üí final (overall change)\n",
      "  üîç Analyzing draft ‚Üí final\n",
      "    Analyzing 42 vs 41 sentences...\n",
      "\n",
      "üìç Step 4: Attribution Mapping\n",
      "  üìù Tracing 41 final sentences...\n",
      "\n",
      "üìä Attribution Summary:\n",
      "  Total sentences in final: 41\n",
      "\n",
      "  Origin Distribution:\n",
      "    edited: 33 sentences (80.5%)\n",
      "    draft: 1 sentences (2.4%)\n",
      "    new_in_final: 7 sentences (17.1%)\n",
      "\n",
      "  Modification Levels:\n",
      "    high_similarity: 3 sentences (7.3%)\n",
      "    medium_similarity: 15 sentences (36.6%)\n",
      "    low_similarity: 16 sentences (39.0%)\n",
      "    new_content: 7 sentences (17.1%)\n",
      "\n",
      "üíæ Complete analysis saved: /content/output/markup-languages_complete_analysis.json\n",
      "üìä Footer metrics saved: /content/output/markup-languages_footer_metrics.json\n"
     ]
    }
   ],
   "source": [
    "combined_results, footer_metrics = run_complete_analysis_from_existing(\n",
    "    article_versions, preprocessor\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}