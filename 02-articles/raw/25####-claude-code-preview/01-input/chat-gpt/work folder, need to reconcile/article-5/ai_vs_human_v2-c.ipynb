{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wtrekell/soylent-army/blob/main/colab/ai_vs_human_v2-c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h5RvQTtrfWf9",
   "metadata": {
    "id": "h5RvQTtrfWf9"
   },
   "source": [
    "**Cell 0.1 with have an error to âš  Restart to reload dependencies**\n",
    "\n",
    "1. Run Cell 0.1a\n",
    "2. Error\n",
    "3. Restart session\n",
    "4. Sellect Cell 0.1b\n",
    "2. Run cell and below  (in runtime menu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9bac6",
   "metadata": {
    "id": "bdc9bac6"
   },
   "outputs": [],
   "source": [
    "# Cell 0.1a â€” Dependencies (Colab setup)\n",
    "\n",
    "%pip install -q \"pandas\" \"numpy\" \"beautifulsoup4\" \"markdown-it-py[linkify]\" \"lxml\" \\\n",
    "                \"scikit-learn\" \"sentence-transformers\" \"transformers\" \"torch\" \\\n",
    "                \"textstat\" \"tqdm\" \"spacy\" \"rapidfuzz\"\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aOW3YmEmcklv",
   "metadata": {
    "id": "aOW3YmEmcklv"
   },
   "outputs": [],
   "source": [
    "# Cell 0.1b - Confirm 'restart session' worked\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"spaCy version: {spacy.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# You can also test the spaCy model loading\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"\\nâœ… spaCy model 'en_core_web_sm' loaded successfully.\")\n",
    "except OSError:\n",
    "    print(\"\\nâŒ spaCy model 'en_core_web_sm' not found. Make sure you downloaded it in the first cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927ddc1",
   "metadata": {
    "id": "9927ddc1"
   },
   "outputs": [],
   "source": [
    "# Cell 0.2 â€” Imports & Global Config\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown_it import MarkdownIt\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = \"./data\"\n",
    "    out_dir: str = \"./output\"\n",
    "\n",
    "    patterns: dict[str, str] = None\n",
    "\n",
    "    model_name: str = \"all-MiniLM-L6-v2\"\n",
    "    batch_size: int = 64\n",
    "\n",
    "    ngram_range: tuple[int,int] = (1, 3)\n",
    "    max_features: int | None = 50000\n",
    "    sublinear_tf: bool = True\n",
    "\n",
    "    topk_candidates: int = 10\n",
    "    weight_semantic: float = 0.5\n",
    "    weight_tfidf: float = 0.5\n",
    "\n",
    "    percentile_minor: float = 0.60\n",
    "    percentile_unchanged: float = 0.85\n",
    "    new_floor: float = 0.25\n",
    "    new_floor_tfidf: float = 0.20\n",
    "\n",
    "    USE_SPACY: bool = True\n",
    "    COMPUTE_SENT_PERPLEXITY: bool = False  # per-sentence perplexity (costly)\n",
    "\n",
    "    ppl_model_name: str = \"gpt2\"\n",
    "    ppl_stride: int = 512\n",
    "\n",
    "cfg = Config(patterns={\"draft\":\"draft*.md\",\"refined\":\"refined*.md\",\"edited\":\"edited*.md\",\"final\":\"final*.md\"})\n",
    "print(\"v2 config loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e612e",
   "metadata": {
    "id": "4a1e612e"
   },
   "outputs": [],
   "source": [
    "# Cell 1.1 â€” File Discovery (user-paste path, single or batch)\n",
    "\n",
    "\n",
    "def _has_article_files(folder: Path) -> bool:\n",
    "    # â€œAny of the 4â€ is enough to treat as an article folder\n",
    "    return any(folder.glob(pat) for pat in cfg.patterns.values())\n",
    "\n",
    "in_path = input(\"ðŸ“ Paste path to an article folder OR a parent directory: \").strip()\n",
    "base = Path(in_path).expanduser().resolve()\n",
    "if not base.exists():\n",
    "    raise FileNotFoundError(f\"Path not found: {base}\")\n",
    "\n",
    "# 1) Prefer the base directory itself, if it contains any of the 4 files\n",
    "if _has_article_files(base):\n",
    "    articles = [base]\n",
    "    print(f\"Detected article files in base â†’ using SINGLE article: {base}\")\n",
    "\n",
    "# 2) Otherwise, look one level down for subfolders that contain any of the 4 files\n",
    "else:\n",
    "    candidates = [p for p in base.iterdir() if p.is_dir() and _has_article_files(p)]\n",
    "    if candidates:\n",
    "        articles = sorted(candidates)\n",
    "        print(f\"No article files in base. Using {len(articles)} subfolder article(s):\")\n",
    "        for a in articles: print(\" -\", a)\n",
    "    else:\n",
    "        # Nothing matched anywhere\n",
    "        patterns_list = \", \".join(cfg.patterns.values())\n",
    "        raise FileNotFoundError(\n",
    "            f\"No article files found.\\n\"\n",
    "            f\"Looked for [{patterns_list}] in:\\n\"\n",
    "            f\" - {base}\\n\"\n",
    "            f\" - its immediate subfolders\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b984bb",
   "metadata": {
    "id": "d5b984bb"
   },
   "outputs": [],
   "source": [
    "# Cell 1.2 â€” Markdown Read + Cleanup (md â†’ text)\n",
    "\n",
    "md = MarkdownIt(\"commonmark\", {})\n",
    "\n",
    "def md_to_text(md_str: str) -> str:\n",
    "    html = md.render(md_str or \"\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    text = soup.get_text(\" \")\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def read_first_match(folder: Path, pattern: str) -> str | None:\n",
    "    files = sorted(folder.glob(pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "    return files[0].read_text(encoding=\"utf-8\", errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6f253",
   "metadata": {
    "id": "22d6f253"
   },
   "outputs": [],
   "source": [
    "# Cell 1.3 â€” Sentence Segmentation\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+(?=[\\(\\\"\\'A-Z0-9])\", text)\n",
    "    return [p.strip() for p in parts if p and not p.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee90a2a",
   "metadata": {
    "id": "5ee90a2a"
   },
   "outputs": [],
   "source": [
    "# Cell 1.4 â€” Tokenization Utilities\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    text = unicodedata.normalize(\"NFKC\", text).lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text, flags=re.UNICODE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb27951",
   "metadata": {
    "id": "6cb27951"
   },
   "outputs": [],
   "source": [
    "# Cell 2.1 â€” TF-IDF Helpers\n",
    "\n",
    "def fit_tfidf(corpus: list[str]) -> tuple[TfidfVectorizer, any]:\n",
    "    vec = TfidfVectorizer(ngram_range=cfg.ngram_range, max_features=cfg.max_features, sublinear_tf=cfg.sublinear_tf)\n",
    "    mat = vec.fit_transform(corpus)\n",
    "    return vec, mat\n",
    "\n",
    "def tfidf_matrix(vec: TfidfVectorizer, texts: list[str]):\n",
    "    return vec.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dda8be",
   "metadata": {
    "id": "a7dda8be"
   },
   "outputs": [],
   "source": [
    "# Cell 2.2 â€” Embeddings\n",
    "\n",
    "_EMB_MODEL: SentenceTransformer | None = None\n",
    "\n",
    "def get_embedder() -> SentenceTransformer:\n",
    "    global _EMB_MODEL\n",
    "    if _EMB_MODEL is None:\n",
    "        print(f\"Loading embedding model: {cfg.model_name}\")\n",
    "        _EMB_MODEL = SentenceTransformer(cfg.model_name)\n",
    "    return _EMB_MODEL\n",
    "\n",
    "def embed_sentences(sents: list[str]) -> np.ndarray:\n",
    "    model = get_embedder()\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(sents), cfg.batch_size), desc=\"Embedding\", unit=\"batch\"):\n",
    "        batch = sents[i:i+cfg.batch_size]\n",
    "        embs.append(model.encode(batch, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True))\n",
    "    return np.vstack(embs) if embs else np.zeros((0, 384), dtype=np.float32)  # 384 for MiniLM; safe default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62e554",
   "metadata": {
    "id": "5c62e554"
   },
   "outputs": [],
   "source": [
    "# Cell 2.3 â€” Perplexity\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "_PPL = {\"model\": None, \"tok\": None}\n",
    "\n",
    "def get_ppl_model():\n",
    "    if _PPL[\"model\"] is None:\n",
    "        print(f\"Loading perplexity model: {cfg.ppl_model_name}\")\n",
    "        _PPL[\"tok\"] = AutoTokenizer.from_pretrained(cfg.ppl_model_name)\n",
    "        _PPL[\"model\"] = AutoModelForCausalLM.from_pretrained(cfg.ppl_model_name)\n",
    "        _PPL[\"model\"].eval()\n",
    "    return _PPL[\"model\"], _PPL[\"tok\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_perplexity(text: str) -> float:\n",
    "    model, tokenizer = get_ppl_model()\n",
    "    if not text.strip():\n",
    "        return 0.0\n",
    "    enc = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = enc.input_ids\n",
    "    seq_len = input_ids.size(1)\n",
    "    max_len = getattr(model.config, \"n_positions\", 1024)\n",
    "    stride = cfg.ppl_stride\n",
    "\n",
    "    nlls = []\n",
    "    prev_end = 0\n",
    "    for begin in range(0, seq_len, stride):\n",
    "        end = min(begin + max_len, seq_len)\n",
    "        trg_len = end - prev_end\n",
    "        input_chunk = input_ids[:, begin:end]\n",
    "        target_ids = input_chunk.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        outputs = model(input_chunk, labels=target_ids)\n",
    "        nlls.append(outputs.loss * trg_len)\n",
    "        prev_end = end\n",
    "        if end == seq_len:\n",
    "            break\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / seq_len)\n",
    "    return float(ppl.item())\n",
    "\n",
    "def calc_sentence_ppl(sentences: list[str]) -> list[float]:\n",
    "    if not cfg.COMPUTE_SENT_PERPLEXITY:\n",
    "        return [None] * len(sentences)\n",
    "    return [calc_perplexity(s) for s in tqdm(sentences, desc=\"Per-sentence PPL\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6930e48",
   "metadata": {
    "id": "d6930e48"
   },
   "outputs": [],
   "source": [
    "# Cell 2.4 â€” Burstiness & Fano\n",
    "\n",
    "def sentence_token_lengths(sentences: list[str], tokenizer=None) -> list[int]:\n",
    "    # use embedding tokenization surrogate: whitespace word count (robust & fast)\n",
    "    return [len(tokenize(s)) for s in sentences]\n",
    "\n",
    "def calc_burstiness(sentences: list[str]) -> float:\n",
    "    lens = sentence_token_lengths(sentences)\n",
    "    return float(np.std(lens)) if lens else 0.0\n",
    "\n",
    "def calc_fano(sentences: list[str]) -> float:\n",
    "    lens = sentence_token_lengths(sentences)\n",
    "    if not lens:\n",
    "        return 0.0\n",
    "    mean = np.mean(lens)\n",
    "    var = np.var(lens)\n",
    "    return float(var / mean) if mean > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91200060",
   "metadata": {
    "id": "91200060"
   },
   "outputs": [],
   "source": [
    "# Cell 2.5 â€” Stylometrics & Readability\n",
    "\n",
    "def type_token_ratio(text: str) -> float:\n",
    "    toks = tokenize(text)\n",
    "    return float(len(set(toks)) / max(1, len(toks)))\n",
    "\n",
    "_SPACY_NLP = None\n",
    "def get_spacy():\n",
    "    global _SPACY_NLP\n",
    "    if not cfg.USE_SPACY:\n",
    "        return None\n",
    "    if spacy is None:\n",
    "        print(\"spaCy not available. Set USE_SPACY=False or install spaCy + en_core_web_sm.\")\n",
    "        return None\n",
    "    if _SPACY_NLP is None:\n",
    "        _SPACY_NLP = spacy.load(\"en_core_web_sm\")\n",
    "    return _SPACY_NLP\n",
    "\n",
    "def pos_ratios(text: str) -> dict[str, float]:\n",
    "    nlp = get_spacy()\n",
    "    if nlp is None:\n",
    "        return {}\n",
    "    doc = nlp(text)\n",
    "    counts = {}\n",
    "    total = 0\n",
    "    for t in doc:\n",
    "        if t.is_space: continue\n",
    "        pos = t.pos_\n",
    "        counts[pos] = counts.get(pos, 0) + 1\n",
    "        total += 1\n",
    "    return {k: v/total for k, v in counts.items()} if total else {}\n",
    "\n",
    "def punctuation_rates(text: str) -> dict[str, float]:\n",
    "    total = max(1, len(text))\n",
    "    return {\n",
    "        \"comma_rate\": text.count(\",\")/total,\n",
    "        \"semicolon_rate\": text.count(\";\")/total,\n",
    "        \"colon_rate\": text.count(\":\")/total,\n",
    "    }\n",
    "\n",
    "def readability_scores(text: str) -> dict[str, float]:\n",
    "    # textstat guards internally; returns 0 if cannot compute\n",
    "    try:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": float(textstat.flesch_reading_ease(text)),\n",
    "            \"flesch_kincaid_grade\": float(textstat.flesch_kincaid_grade(text)),\n",
    "            \"gunning_fog\": float(textstat.gunning_fog(text)),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def compute_stylometrics(text: str, sentences: list[str]) -> dict[str, float]:\n",
    "    toks = tokenize(text)\n",
    "    lens = [len(tokenize(s)) for s in sentences] if sentences else []\n",
    "    base = {\n",
    "        \"ttr\": type_token_ratio(text),\n",
    "        \"avg_word_len\": (sum(len(t) for t in toks)/len(toks)) if toks else 0.0,\n",
    "        \"avg_sent_len_tokens\": (np.mean(lens) if lens else 0.0),\n",
    "        \"var_sent_len_tokens\": (np.var(lens) if lens else 0.0),\n",
    "    }\n",
    "    base.update(punctuation_rates(text))\n",
    "    base.update(readability_scores(text))\n",
    "    base.update(pos_ratios(text))\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f03bf1",
   "metadata": {
    "id": "f8f03bf1"
   },
   "outputs": [],
   "source": [
    "# Cell 3.1 â€” Version Loader\n",
    "\n",
    "VERSION_ORDER = [\"draft\",\"refined\",\"edited\",\"final\"]\n",
    "\n",
    "def load_versions(article_folder: Path) -> dict[str, str | None]:\n",
    "    out = {}\n",
    "    for v in VERSION_ORDER:\n",
    "        pat = cfg.patterns.get(v)\n",
    "        out[v] = read_first_match(article_folder, pat) if pat else None\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c95c8",
   "metadata": {
    "id": "7e0c95c8"
   },
   "outputs": [],
   "source": [
    "# Cell 3.2 â€” Sentence Banks\n",
    "\n",
    "def to_sentences(md_content: str | None) -> list[str]:\n",
    "    return split_sentences(md_to_text(md_content or \"\"))\n",
    "\n",
    "def build_sentence_banks(versions: dict[str,str]) -> tuple[list[str], list[str], list[tuple[str,int]]]:\n",
    "    sents = {v: to_sentences(versions.get(v)) for v in VERSION_ORDER}\n",
    "    final_sents = sents[\"final\"] or []\n",
    "    candidates, cand_meta = [], []\n",
    "    for v in [\"draft\",\"refined\",\"edited\"]:\n",
    "        for idx, s in enumerate(sents.get(v) or []):\n",
    "            candidates.append(s); cand_meta.append((v, idx))\n",
    "    return final_sents, candidates, cand_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46f438",
   "metadata": {
    "id": "8a46f438"
   },
   "outputs": [],
   "source": [
    "# Cell 3.3 â€” Fast Similarities\n",
    "\n",
    "def fast_similarities(final_sents: list[str], candidates: list[str]):\n",
    "    if not final_sents or not candidates:\n",
    "        return None, None, None\n",
    "\n",
    "    tfidf_corpus = final_sents + candidates\n",
    "    vec, mat = fit_tfidf(tfidf_corpus)\n",
    "    tfidf_final = mat[:len(final_sents)]\n",
    "    tfidf_cands = mat[len(final_sents):]\n",
    "    sim_tfidf = cosine_similarity(tfidf_final, tfidf_cands)\n",
    "\n",
    "    emb_final = embed_sentences(final_sents)\n",
    "    emb_cands = embed_sentences(candidates)\n",
    "    sim_sem = cosine_similarity(emb_final, emb_cands)\n",
    "\n",
    "    combined_fast = cfg.weight_semantic * sim_sem + cfg.weight_tfidf * sim_tfidf\n",
    "    return combined_fast, sim_sem, sim_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e81254",
   "metadata": {
    "id": "a6e81254"
   },
   "outputs": [],
   "source": [
    "# Cell 3.4 â€” Shortlist Top-K\n",
    "\n",
    "def shortlist_indices(combined_fast: np.ndarray, k: int) -> list[np.ndarray]:\n",
    "    k = max(1, min(k, combined_fast.shape[1])) if combined_fast is not None else 1\n",
    "    idx_list = []\n",
    "    for i in range(combined_fast.shape[0]):\n",
    "        row = combined_fast[i]\n",
    "        take = np.argpartition(-row, kth=k-1)[:k]\n",
    "        idx_list.append(take)\n",
    "    return idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f611a4",
   "metadata": {
    "id": "21f611a4"
   },
   "outputs": [],
   "source": [
    "# Cell 4.1 â€” Heavy Metrics (Jaccard + Levenshtein)\n",
    "\n",
    "def jaccard_tokens(a: str, b: str) -> float:\n",
    "    A, B = set(tokenize(a)), set(tokenize(b))\n",
    "    if not A and not B: return 1.0\n",
    "    if not A or not B:  return 0.0\n",
    "    return len(A & B) / float(len(A | B))\n",
    "\n",
    "def lev_ratio(a: str, b: str) -> float:\n",
    "    # normalized similarity in [0,1]\n",
    "    if not a and not b: return 1.0\n",
    "    max_len = max(len(a), len(b)) or 1\n",
    "    dist = Levenshtein.distance(a, b)\n",
    "    return 1.0 - (dist / max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b6d9f",
   "metadata": {
    "id": "610b6d9f"
   },
   "outputs": [],
   "source": [
    "# Cell 4.2 â€” Final Scoring & Attribution\n",
    "\n",
    "def score_and_attribute(final_sents, candidates, cand_meta, combined_fast, sim_sem, sim_tfidf):\n",
    "    rows = []\n",
    "    heavy_calls = 0\n",
    "    if combined_fast is None or combined_fast.size == 0:\n",
    "        # No candidates; mark all as \"new/major\"\n",
    "        for i, f in enumerate(final_sents):\n",
    "            rows.append({\n",
    "                \"final_index\": i,\n",
    "                \"final_sentence\": f,\n",
    "                \"origin_version\": \"none\",\n",
    "                \"origin_index\": None,\n",
    "                \"origin_sentence\": None,\n",
    "                \"semantic_sim\": None,\n",
    "                \"tfidf_sim\": None,\n",
    "                \"jaccard\": None,\n",
    "                \"levenshtein\": None,\n",
    "                \"combined_sim\": 0.0,\n",
    "                \"modification_label\": \"major\"\n",
    "            })\n",
    "        return rows, heavy_calls\n",
    "\n",
    "    topk_idx = shortlist_indices(combined_fast, cfg.topk_candidates)\n",
    "\n",
    "    for i, f in enumerate(final_sents):\n",
    "        cand_ids = topk_idx[i]\n",
    "        best = None\n",
    "        best_score = -1.0\n",
    "        for ci in cand_ids:\n",
    "            c = candidates[ci]\n",
    "            # lexical_avg = mean(jaccard, lev_ratio, tfidf_sim)\n",
    "            jac = jaccard_tokens(f, c)\n",
    "            lev = lev_ratio(f, c)\n",
    "            tf  = float(sim_tfidf[i, ci])\n",
    "            lexical_avg = (jac + lev + tf) / 3.0\n",
    "            sem = float(sim_sem[i, ci])\n",
    "            score = 0.5*sem + 0.5*lexical_avg\n",
    "            heavy_calls += 1\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best = (ci, jac, lev, tf, sem, score)\n",
    "\n",
    "        if best is None:\n",
    "            rows.append({\n",
    "                \"final_index\": i,\n",
    "                \"final_sentence\": f,\n",
    "                \"origin_version\": \"none\",\n",
    "                \"origin_index\": None,\n",
    "                \"origin_sentence\": None,\n",
    "                \"semantic_sim\": None,\n",
    "                \"tfidf_sim\": None,\n",
    "                \"jaccard\": None,\n",
    "                \"levenshtein\": None,\n",
    "                \"combined_sim\": 0.0,\n",
    "                \"modification_label\": \"major\"\n",
    "            })\n",
    "        else:\n",
    "            ci, jac, lev, tf, sem, score = best\n",
    "            o_v, o_i = cand_meta[ci]\n",
    "            rows.append({\n",
    "                \"final_index\": i,\n",
    "                \"final_sentence\": f,\n",
    "                \"origin_version\": o_v,\n",
    "                \"origin_index\": int(o_i),\n",
    "                \"origin_sentence\": candidates[ci],\n",
    "                \"semantic_sim\": float(sem),\n",
    "                \"tfidf_sim\": float(tf),\n",
    "                \"jaccard\": float(jac),\n",
    "                \"levenshtein\": float(lev),\n",
    "                \"combined_sim\": float(score),\n",
    "                \"modification_label\": None  # filled next cell\n",
    "            })\n",
    "\n",
    "    return rows, heavy_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b28b3b",
   "metadata": {
    "id": "36b28b3b"
   },
   "outputs": [],
   "source": [
    "# Cell 4.3 â€” Labeling by Percentiles (+ â€œnewâ€ rule)\n",
    "\n",
    "def label_rows(rows: list[dict]) -> None:\n",
    "    scores = np.array([r[\"combined_sim\"] for r in rows], dtype=float)\n",
    "    if scores.size == 0:\n",
    "        return\n",
    "    p60 = float(np.percentile(scores, cfg.percentile_minor*100))\n",
    "    p85 = float(np.percentile(scores, cfg.percentile_unchanged*100))\n",
    "    for r in rows:\n",
    "        score = r[\"combined_sim\"]\n",
    "        tfidf = (r[\"tfidf_sim\"] or 0.0)\n",
    "        if score < cfg.new_floor and tfidf < cfg.new_floor_tfidf:\n",
    "            r[\"modification_label\"] = \"major\"  # effectively â€œnewâ€/major\n",
    "        elif score >= p85:\n",
    "            r[\"modification_label\"] = \"unchanged\"\n",
    "        elif score >= p60:\n",
    "            r[\"modification_label\"] = \"minor\"\n",
    "        else:\n",
    "            r[\"modification_label\"] = \"major\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c636d1",
   "metadata": {
    "id": "b3c636d1"
   },
   "outputs": [],
   "source": [
    "# Cell 4.4 â€” Summaries & Doc Metrics\n",
    "\n",
    "def distributions_from_rows(rows: list[dict]) -> tuple[dict,str]:\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        origin_dist = dict.fromkeys([\"draft\", \"refined\", \"edited\", \"none\"], 0.0)\n",
    "        mod_dist = dict.fromkeys([\"unchanged\", \"minor\", \"major\"], 0.0)\n",
    "        return origin_dist, mod_dist\n",
    "\n",
    "    origin_dist = (df[\"origin_version\"].fillna(\"none\")\n",
    "                   .value_counts(normalize=True)\n",
    "                   .reindex([\"draft\",\"refined\",\"edited\",\"none\"], fill_value=0.0)\n",
    "                   .to_dict())\n",
    "    mod_dist = (df[\"modification_label\"].fillna(\"major\")\n",
    "                .value_counts(normalize=True)\n",
    "                .reindex([\"unchanged\",\"minor\",\"major\"], fill_value=0.0)\n",
    "                .to_dict())\n",
    "    return origin_dist, mod_dist\n",
    "\n",
    "def doc_metrics(text_final: str, final_sents: list[str]) -> dict:\n",
    "    return {\n",
    "        \"perplexity\": calc_perplexity(text_final),\n",
    "        \"burstiness_std\": calc_burstiness(final_sents),\n",
    "        \"fano_factor\": calc_fano(final_sents),\n",
    "        \"stylometrics\": compute_stylometrics(text_final, final_sents),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a1d97",
   "metadata": {
    "id": "ee4a1d97"
   },
   "outputs": [],
   "source": [
    "# Cell 4.5 â€” Analyzer (per article) + Diagnostics\n",
    "\n",
    "def analyze_article(article_path: Path):\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    versions = load_versions(article_path)\n",
    "    final_text = md_to_text(versions.get(\"final\") or \"\")\n",
    "    final_sents, candidates, cand_meta = build_sentence_banks(versions)\n",
    "\n",
    "    if not final_sents:\n",
    "        print(f\"[SKIP] No final sentences: {article_path}\")\n",
    "        return None\n",
    "\n",
    "    combined_fast, sim_sem, sim_tfidf = fast_similarities(final_sents, candidates)\n",
    "    rows, heavy_calls = score_and_attribute(final_sents, candidates, cand_meta,\n",
    "                                            combined_fast, sim_sem, sim_tfidf)\n",
    "    label_rows(rows)\n",
    "    origin_dist, mod_dist = distributions_from_rows(rows)\n",
    "\n",
    "    metrics = doc_metrics(final_text, final_sents)\n",
    "    if cfg.COMPUTE_SENT_PERPLEXITY:\n",
    "        sent_ppl = calc_sentence_ppl(final_sents)\n",
    "        for r, ppl in zip(rows, sent_ppl, strict=False):\n",
    "            r[\"perplexity\"] = ppl\n",
    "\n",
    "    summary = {\n",
    "        \"origin_distribution\": origin_dist,\n",
    "        \"modification_distribution\": mod_dist,\n",
    "        \"counts\": {\n",
    "            \"sentences_final\": int(len(final_sents)),\n",
    "            \"heavy_comparisons\": int(heavy_calls),\n",
    "            \"avg_heavy_per_final\": float(heavy_calls / max(1, len(final_sents))),\n",
    "            \"topk\": int(cfg.topk_candidates)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"version\": \"2.0-analysis\",\n",
    "        \"article_name\": article_path.name,\n",
    "        \"config\": {\n",
    "            \"model_name\": cfg.model_name,\n",
    "            \"topk_candidates\": cfg.topk_candidates,\n",
    "            \"weights\": {\"semantic\": cfg.weight_semantic, \"tfidf\": cfg.weight_tfidf},\n",
    "            \"percentiles\": {\"minor\": cfg.percentile_minor, \"unchanged\": cfg.percentile_unchanged},\n",
    "            \"new_floor\": {\"combined\": cfg.new_floor, \"tfidf\": cfg.new_floor_tfidf},\n",
    "        },\n",
    "        \"summary\": summary,\n",
    "        \"doc_metrics\": metrics,\n",
    "        \"rows\": rows,\n",
    "        # Legacy block for backward compatibility (thin but enough for old visualizers)\n",
    "        \"attribution_analysis\": {\n",
    "            \"statistics\": {\n",
    "                \"origin_distribution\": origin_dist,\n",
    "                \"modification_distribution\": mod_dist\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"\\nAnalysis: {article_path.name}\")\n",
    "    print(f\"- Final sentences analyzed: {len(final_sents)}\")\n",
    "    print(f\"- Heavy comparisons executed: {heavy_calls} (expected â‰ˆ topK={cfg.topk_candidates} per final)\")\n",
    "    print(f\"- Avg heavy comps per final: {summary['counts']['avg_heavy_per_final']:.2f}\")\n",
    "    print(f\"- Unchanged/minor/major (%): \"\n",
    "          f\"{mod_dist.get('unchanged',0):.2%} / {mod_dist.get('minor',0):.2%} / {mod_dist.get('major',0):.2%}\")\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    footer = {\"elapsed_sec\": float(t1 - t0)}\n",
    "    return payload, footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ccd75e",
   "metadata": {
    "id": "66ccd75e"
   },
   "outputs": [],
   "source": [
    "# Cell 4.6 â€” Integrate Optional Detectors into Results\n",
    "\n",
    "def integrate_probs(rows: list[dict], final_text: str):\n",
    "    # rows must already exist (from score_and_attribute / label_rows)\n",
    "    sentences = [r[\"final_sentence\"] for r in rows]\n",
    "    probs = apply_detectors_to_rows(sentences, final_text)\n",
    "    # Attach per-sentence if available\n",
    "    for name, arr in probs.items():\n",
    "        if arr is None:\n",
    "            continue\n",
    "        for r, p in zip(rows, arr, strict=False):\n",
    "            r[name] = float(p)\n",
    "    # Return doc-level summaries\n",
    "    doc_levels = {}\n",
    "    if probs.get(\"p_ai_feature\") is not None:\n",
    "        doc_levels[\"p_ai_feature\"] = float(np.mean(probs[\"p_ai_feature\"]))\n",
    "    if probs.get(\"p_ai_transformer\") is not None:\n",
    "        doc_levels[\"p_ai_transformer\"] = float(np.mean(probs[\"p_ai_transformer\"]))\n",
    "    if probs.get(\"p_ai_ensemble\") is not None:\n",
    "        doc_levels[\"p_ai\"] = float(np.mean(probs[\"p_ai_ensemble\"]))\n",
    "    return doc_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8d6f1",
   "metadata": {
    "id": "76b8d6f1"
   },
   "outputs": [],
   "source": [
    "# Cell 5.1 â€” Feature-Engineered Detector (sklearn; optional)\n",
    "\n",
    "# ðŸ›‘ will occur unless we install and train our own models.\n",
    "\n",
    "import pickle\n",
    "\n",
    "_FE = {\"vec\": None, \"model\": None}\n",
    "\n",
    "def _try_load_fe_artifacts(model_dir: Path = Path(\"./models\")):\n",
    "    try:\n",
    "        vec_path = model_dir / \"fe_vectorizer.pkl\"\n",
    "        mdl_path = model_dir / \"fe_model.pkl\"\n",
    "        if vec_path.exists() and mdl_path.exists():\n",
    "            with open(vec_path, \"rb\") as f: _FE[\"vec\"] = pickle.load(f)\n",
    "            with open(mdl_path, \"rb\") as f: _FE[\"model\"] = pickle.load(f)\n",
    "            print(f\"[FE] Loaded vectorizer+model from {model_dir}\")\n",
    "        else:\n",
    "            print(\"ðŸ›‘ Artifacts not found; skipping feature-engineered detector.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FE] Failed to load artifacts: {e}\")\n",
    "\n",
    "def _fe_features(texts: list[str], doc_text: str) -> np.ndarray:\n",
    "    if _FE[\"vec\"] is None:\n",
    "        return None\n",
    "    X = _FE[\"vec\"].transform(texts)  # TF-IDF / ngram features inside the vectorizer\n",
    "    return X\n",
    "\n",
    "def fe_predict_proba(sentences: list[str], doc_text: str) -> np.ndarray | None:\n",
    "    if _FE[\"model\"] is None:\n",
    "        return None\n",
    "    X = _fe_features(sentences, doc_text)\n",
    "    if X is None:\n",
    "        return None\n",
    "    mdl = _FE[\"model\"]\n",
    "    # Support both probabilistic and decision-function models\n",
    "    if hasattr(mdl, \"predict_proba\"):\n",
    "        proba = mdl.predict_proba(X)\n",
    "        # assume binary [human, ai]; return ai column\n",
    "        return proba[:, 1]\n",
    "    elif hasattr(mdl, \"decision_function\"):\n",
    "        scores = mdl.decision_function(X)\n",
    "        # map decision scores to (0,1) via logistic\n",
    "        return 1.0 / (1.0 + np.exp(-scores))\n",
    "    else:\n",
    "        # fallback to labels {0,1}\n",
    "        preds = mdl.predict(X)\n",
    "        return preds.astype(float)\n",
    "\n",
    "_try_load_fe_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588f7e7",
   "metadata": {
    "id": "1588f7e7"
   },
   "outputs": [],
   "source": [
    "# Cell 4.1 â€” collect_doc_metrics()\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "_TR = {\"tok\": None, \"mdl\": None, \"id2label\": None}\n",
    "\n",
    "PRIMARY_MODEL  = \"fakespot-ai/roberta-base-ai-text-detection-v1\"   # recommended\n",
    "FALLBACK_MODEL = \"Hello-SimpleAI/chatgpt-detector-roberta\"         # HC3-based\n",
    "MODEL_DIR = Path(\"./models/roberta-detector\")\n",
    "\n",
    "def _download_and_save(model_id: str, target: Path) -> bool:\n",
    "    try:\n",
    "        print(f\"â¬ Attempting to download: {model_id}\")\n",
    "        tok = AutoTokenizer.from_pretrained(model_id)\n",
    "        mdl = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "        if target.exists():\n",
    "            shutil.rmtree(target)\n",
    "        target.mkdir(parents=True, exist_ok=True)\n",
    "        tok.save_pretrained(target)\n",
    "        mdl.save_pretrained(target)\n",
    "        print(f\"âœ… Saved detector to {target} ({model_id})\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to download {model_id}: {e}\")\n",
    "        return False\n",
    "\n",
    "def _ensure_checkpoint():\n",
    "    if MODEL_DIR.exists() and any(MODEL_DIR.iterdir()):\n",
    "        return True\n",
    "    ok = _download_and_save(PRIMARY_MODEL, MODEL_DIR)\n",
    "    if not ok:\n",
    "        ok = _download_and_save(FALLBACK_MODEL, MODEL_DIR)\n",
    "    if not ok:\n",
    "        print(\"âš  No detector downloaded. Transformer-based detection will noop gracefully.\")\n",
    "    return ok\n",
    "\n",
    "def _try_load_transformer(model_dir: Path = MODEL_DIR):\n",
    "    try:\n",
    "        if not model_dir.exists() or not any(model_dir.iterdir()):\n",
    "            if not _ensure_checkpoint():\n",
    "                print(\"[TR] Transformer checkpoint not found; skipping transformer detector.\")\n",
    "                return\n",
    "        _TR[\"tok\"] = AutoTokenizer.from_pretrained(model_dir)\n",
    "        _TR[\"mdl\"] = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        _TR[\"mdl\"].eval()\n",
    "        _TR[\"id2label\"] = _TR[\"mdl\"].config.id2label\n",
    "        print(f\"[TR] Loaded transformer detector from {model_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[TR] Failed to load transformer detector: {e}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def tr_predict_proba(sentences: list[str]) -> np.ndarray | None:\n",
    "    if _TR[\"mdl\"] is None or _TR[\"tok\"] is None:\n",
    "        return None\n",
    "    tok = _TR[\"tok\"]; mdl = _TR[\"mdl\"]\n",
    "    out = []\n",
    "    bs = 32\n",
    "    for i in range(0, len(sentences), bs):\n",
    "        batch = sentences[i:i+bs]\n",
    "        enc = tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        logits = mdl(**enc).logits\n",
    "        if logits.shape[-1] == 2:\n",
    "            proba = torch.softmax(logits, dim=-1)[:, 1]  # assume index 1 = AI\n",
    "        else:\n",
    "            # try to find label \"AI\" in id2label; else use last column\n",
    "            if _TR[\"id2label\"]:\n",
    "                ai_index = None\n",
    "                for k, v in _TR[\"id2label\"].items():\n",
    "                    if str(v).lower().startswith(\"ai\"):\n",
    "                        ai_index = int(k)\n",
    "                        break\n",
    "                if ai_index is None:\n",
    "                    ai_index = logits.shape[-1] - 1\n",
    "            else:\n",
    "                ai_index = logits.shape[-1] - 1\n",
    "            proba = torch.softmax(logits, dim=-1)[:, ai_index]\n",
    "        out.append(proba.cpu().numpy())\n",
    "    return np.concatenate(out, axis=0) if out else np.zeros((0,), dtype=float)\n",
    "\n",
    "# Load (and auto-download if missing)\n",
    "_try_load_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b073cb",
   "metadata": {
    "id": "48b073cb"
   },
   "outputs": [],
   "source": [
    "# Cell 5.1 â€” Feature-Engineered Detector (sklearn)\n",
    "\n",
    "def ensemble_probs(*arrays: np.ndarray | None) -> np.ndarray | None:\n",
    "    parts = [a for a in arrays if a is not None]\n",
    "    if not parts:\n",
    "        return None\n",
    "    # simple mean; weights can be added if desired\n",
    "    return np.mean(np.vstack(parts), axis=0)\n",
    "\n",
    "def apply_detectors_to_rows(final_sents: list[str], final_text: str) -> dict[str, np.ndarray | None]:\n",
    "    p_fe  = fe_predict_proba(final_sents, final_text)\n",
    "    p_tr  = tr_predict_proba(final_sents)\n",
    "    p_ens = ensemble_probs(p_fe, p_tr)\n",
    "    return {\"p_ai_feature\": p_fe, \"p_ai_transformer\": p_tr, \"p_ai_ensemble\": p_ens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502cb6fd",
   "metadata": {
    "id": "502cb6fd"
   },
   "outputs": [],
   "source": [
    "# Cell 5.2 â€” Transformer Detector (RoBERTa/BERT)\n",
    "\n",
    "def detectgpt_score(text: str) -> float | None:\n",
    "    # Placeholder: wire to your logits-capable LM + perturbation routine when available.\n",
    "    # Return a float where higher â†’ more likely machine-generated (by that LM).\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b9405",
   "metadata": {
    "id": "f53b9405"
   },
   "outputs": [],
   "source": [
    "# Cell 5.3 â€” Ensemble Wrapper\n",
    "\n",
    "def watermark_p_value(text: str) -> float | None:\n",
    "    # Placeholder: implement z-test over green-list token proportion when key is available.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dcc022",
   "metadata": {
    "id": "e8dcc022"
   },
   "outputs": [],
   "source": [
    "# Cell 6.1 â€” analyze_article()\n",
    "\n",
    "def analyze_article(article_path: Path):\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    versions = load_versions(article_path)\n",
    "    final_text = md_to_text(versions.get(\"final\") or \"\")\n",
    "    final_sents, candidates, cand_meta = build_sentence_banks(versions)\n",
    "\n",
    "    if not final_sents:\n",
    "        print(f\"[SKIP] No final sentences: {article_path}\")\n",
    "        return None\n",
    "\n",
    "    combined_fast, sim_sem, sim_tfidf = fast_similarities(final_sents, candidates)\n",
    "    rows, heavy_calls = score_and_attribute(final_sents, candidates, cand_meta,\n",
    "                                            combined_fast, sim_sem, sim_tfidf)\n",
    "    label_rows(rows)\n",
    "\n",
    "    # OPTIONAL detectors (safe if artifacts/checkpoints absent)\n",
    "    doc_prob_levels = integrate_probs(rows, final_text)\n",
    "\n",
    "    origin_dist, mod_dist = distributions_from_rows(rows)\n",
    "    metrics = doc_metrics(final_text, final_sents)\n",
    "\n",
    "    # Optional advanced hooks\n",
    "    dg = detectgpt_score(final_text)\n",
    "    if dg is not None:\n",
    "        metrics[\"detectgpt_score\"] = dg\n",
    "    wp = watermark_p_value(final_text)\n",
    "    if wp is not None:\n",
    "        metrics[\"watermark_pval\"] = wp\n",
    "\n",
    "    if cfg.COMPUTE_SENT_PERPLEXITY:\n",
    "        sent_ppl = calc_sentence_ppl(final_sents)\n",
    "        for r, ppl in zip(rows, sent_ppl, strict=False):\n",
    "            r[\"perplexity\"] = ppl\n",
    "\n",
    "    # fold in doc-level AI probabilities if present\n",
    "    if doc_prob_levels:\n",
    "        metrics.update(doc_prob_levels)\n",
    "\n",
    "    summary = {\n",
    "        \"origin_distribution\": origin_dist,\n",
    "        \"modification_distribution\": mod_dist,\n",
    "        \"counts\": {\n",
    "            \"sentences_final\": int(len(final_sents)),\n",
    "            \"heavy_comparisons\": int(heavy_calls),\n",
    "            \"avg_heavy_per_final\": float(heavy_calls / max(1, len(final_sents))),\n",
    "            \"topk\": int(cfg.topk_candidates)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"version\": \"2.0-analysis\",\n",
    "        \"article_name\": article_path.name,\n",
    "        \"config\": {\n",
    "            \"model_name\": cfg.model_name,\n",
    "            \"topk_candidates\": cfg.topk_candidates,\n",
    "            \"weights\": {\"semantic\": cfg.weight_semantic, \"tfidf\": cfg.weight_tfidf},\n",
    "            \"percentiles\": {\"minor\": cfg.percentile_minor, \"unchanged\": cfg.percentile_unchanged},\n",
    "            \"new_floor\": {\"combined\": cfg.new_floor, \"tfidf\": cfg.new_floor_tfidf},\n",
    "        },\n",
    "        \"summary\": summary,\n",
    "        \"doc_metrics\": metrics,\n",
    "        \"rows\": rows,\n",
    "        \"attribution_analysis\": {  # legacy shim\n",
    "            \"statistics\": {\n",
    "                \"origin_distribution\": origin_dist,\n",
    "                \"modification_distribution\": mod_dist\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"\\nAnalysis: {article_path.name}\")\n",
    "    print(f\"- Final sentences analyzed: {len(final_sents)}\")\n",
    "    print(f\"- Heavy comparisons executed: {heavy_calls} (expected â‰ˆ topK={cfg.topk_candidates} per final)\")\n",
    "    print(f\"- Avg heavy comps per final: {summary['counts']['avg_heavy_per_final']:.2f}\")\n",
    "    print(f\"- Unchanged/minor/major (%): \"\n",
    "          f\"{mod_dist.get('unchanged',0):.2%} / {mod_dist.get('minor',0):.2%} / {mod_dist.get('major',0):.2%}\")\n",
    "    if doc_prob_levels:\n",
    "        print(\"- Doc AI probability:\", {k: f\"{v:.2%}\" for k, v in doc_prob_levels.items()})\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    footer = {\"elapsed_sec\": float(t1 - t0)}\n",
    "    return payload, footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c324f",
   "metadata": {
    "id": "368c324f"
   },
   "outputs": [],
   "source": [
    "# Cell 7.1 â€” Batch Runner\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_outputs(article_name: str, payload: dict, footer: dict, out_dir: Path):\n",
    "    ensure_dir(out_dir)\n",
    "    base = out_dir / article_name\n",
    "    json_path = out_dir / f\"{article_name}_complete_summary.json\"\n",
    "    footer_path = out_dir / f\"{article_name}_footer_metrics.json\"\n",
    "    csv_path = out_dir / f\"{article_name}_final_sentence_attribution.csv\"\n",
    "\n",
    "    # JSON\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Footer\n",
    "    with open(footer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(footer, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # CSV from rows\n",
    "    df_rows = pd.DataFrame(payload[\"rows\"])\n",
    "    df_rows.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"json\": str(json_path), \"csv\": str(csv_path), \"footer\": str(footer_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb5842",
   "metadata": {
    "id": "d3fb5842"
   },
   "outputs": [],
   "source": [
    "# Cell 7.2 â€” Outputs Summary\n",
    "\n",
    "results = []\n",
    "out_dir = Path(cfg.out_dir)\n",
    "ensure_dir(out_dir)\n",
    "\n",
    "for art in tqdm(articles, desc=\"Articles\", unit=\"article\"):\n",
    "    res = analyze_article(art)\n",
    "    if not res:\n",
    "        continue\n",
    "    payload, footer = res\n",
    "    paths = write_outputs(payload[\"article_name\"], payload, footer, out_dir)\n",
    "    results.append({\"article_name\": payload[\"article_name\"], **paths})\n",
    "\n",
    "if not results:\n",
    "    print(\"\\nNo outputs produced. Ensure your data folders/files match expected patterns.\")\n",
    "else:\n",
    "    print(\"\\nSummary of outputs:\")\n",
    "    for r in results:\n",
    "        print(f\"- {r['article_name']} -> JSON: {r['json']} | CSV: {r['csv']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022d9b2",
   "metadata": {
    "id": "0022d9b2"
   },
   "outputs": [],
   "source": [
    "# Cell 7.3 â€” Collection Summary CSV\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if results:\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        with open(r[\"json\"], encoding=\"utf-8\") as f:\n",
    "            p = json.load(f)\n",
    "        s = p[\"summary\"]; m = p[\"doc_metrics\"]\n",
    "\n",
    "        row = {\n",
    "            \"article_name\":     p[\"article_name\"],\n",
    "            \"sentences_final\":  s[\"counts\"][\"sentences_final\"],\n",
    "\n",
    "            # Origin distributions\n",
    "            \"origin_draft\":     s[\"origin_distribution\"].get(\"draft\", 0.0),\n",
    "            \"origin_refined\":   s[\"origin_distribution\"].get(\"refined\", 0.0),\n",
    "            \"origin_edited\":    s[\"origin_distribution\"].get(\"edited\", 0.0),\n",
    "            \"origin_none\":      s[\"origin_distribution\"].get(\"none\", 0.0),\n",
    "\n",
    "            # Modification distributions\n",
    "            \"mod_unchanged\":    s[\"modification_distribution\"].get(\"unchanged\", 0.0),\n",
    "            \"mod_minor\":        s[\"modification_distribution\"].get(\"minor\", 0.0),\n",
    "            \"mod_major\":        s[\"modification_distribution\"].get(\"major\", 0.0),\n",
    "\n",
    "            # Core statistical metrics\n",
    "            \"ppl\":              m.get(\"perplexity\", None),\n",
    "            \"burstiness_std\":   m.get(\"burstiness_std\", None),\n",
    "            \"fano_factor\":      m.get(\"fano_factor\", None),\n",
    "\n",
    "            # Model-based AI probabilities (optional, only if detectors ran)\n",
    "            \"p_ai_feature\":     m.get(\"p_ai_feature\", None),      # sklearn/FE detector\n",
    "            \"p_ai_transformer\": m.get(\"p_ai_transformer\", None),  # RoBERTa/BERT detector\n",
    "            \"p_ai_ensemble\":    m.get(\"p_ai\", None),              # blended ensemble\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"article_name\")\n",
    "    path = Path(cfg.out_dir) / \"_collection_summary.csv\"\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\n[Collection] Wrote {len(df)} rows â†’ {path}\")\n",
    "else:\n",
    "    print(\"[Collection] No results to summarize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zIN43n18j2Xb",
   "metadata": {
    "id": "zIN43n18j2Xb"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(cfg.out_dir)\n",
    "zip_filename = f\"{out_dir.name}_archive.zip\"\n",
    "zip_path = out_dir.parent / zip_filename # Place the zip file one level up from the output directory\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Walk through the output directory and add files to the zip\n",
    "    for root, _, files in os.walk(out_dir):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            # Add file to zip, preserving directory structure relative to out_dir\n",
    "            zipf.write(file_path, arcname=file_path.relative_to(out_dir))\n",
    "\n",
    "print(f\"Successfully created zip archive: {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}