{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rbv4cU346vZg",
   "metadata": {
    "id": "rbv4cU346vZg"
   },
   "outputs": [],
   "source": [
    "#CELL 0.1 - Installs\n",
    "%pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17N0Ai3DjX6R",
   "metadata": {
    "id": "17N0Ai3DjX6R"
   },
   "outputs": [],
   "source": [
    "# Cell 0.2a â€” Plotly â†’ PNG support (force kaleido engine, self-test)\n",
    "\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "_HAS_PLOTLY_PNG = False\n",
    "\n",
    "try:\n",
    "    import plotly.io as pio\n",
    "    import plotly.graph_objects as go\n",
    "    try:\n",
    "        import kaleido  # ensure module is importable\n",
    "        # Force the engine and do a self-test write\n",
    "        test_fig = go.Figure(go.Scatter(x=[0,1], y=[0,1]))\n",
    "        _ = test_fig.to_image(format=\"png\", engine=\"kaleido\")  # in-memory test\n",
    "        _HAS_PLOTLY_PNG = True\n",
    "        print(\"Kaleido detected:\", getattr(kaleido, \"__version__\", \"unknown\"))\n",
    "    except Exception as e:\n",
    "        print(\"Kaleido not active. Attempting installâ€¦\", e)\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaleido\"])\n",
    "            import kaleido  # retry\n",
    "            test_fig = go.Figure(go.Scatter(x=[0,1], y=[0,1]))\n",
    "            _ = test_fig.to_image(format=\"png\", engine=\"kaleido\")\n",
    "            _HAS_PLOTLY_PNG = True\n",
    "            print(\"Kaleido installed:\", getattr(kaleido, \"__version__\", \"unknown\"))\n",
    "        except Exception as e2:\n",
    "            print(\"[WARN] Plotly PNG export unavailable:\", e2)\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Plotly not available:\", e)\n",
    "\n",
    "def save_plotly_png_and_html(fig, png_path: Path | None, html_path: Path | None):\n",
    "    \"\"\"Save Plotly figure to PNG (kaleido) and HTML.\"\"\"\n",
    "    if html_path is not None:\n",
    "        try:\n",
    "            fig.write_html(str(html_path), include_plotlyjs=\"cdn\", full_html=True)\n",
    "            print(\"HTML saved:\", html_path)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] write_html failed:\", e)\n",
    "    if png_path is not None:\n",
    "        if _HAS_PLOTLY_PNG:\n",
    "            try:\n",
    "                fig.write_image(str(png_path), engine=\"kaleido\")  # << force engine\n",
    "                print(\"PNG saved:\", png_path)\n",
    "            except Exception as e:\n",
    "                print(\"[WARN] write_image failed (kaleido):\", e)\n",
    "        else:\n",
    "            print(\"[INFO] PNG skipped: kaleido engine not available. Try: pip install kaleido\")\n",
    "\n",
    "print(\"Plotly PNG export available:\", _HAS_PLOTLY_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rbeJmUvRj_Bc",
   "metadata": {
    "id": "rbeJmUvRj_Bc"
   },
   "outputs": [],
   "source": [
    "# Cell 0.2b - Quick self-check\n",
    "if _HAS_PLOTLY_PNG:\n",
    "    import plotly.graph_objects as go\n",
    "    tfig = go.Figure(go.Bar(x=[\"ok\"], y=[1]))\n",
    "    test_path = OUTPUT_FIG_DIR / f\"_plotly_png_sanity_{_ts()}.png\"\n",
    "    tfig.write_image(str(test_path), engine=\"kaleido\")\n",
    "    print(\"Plotly PNG sanity check saved:\", test_path)\n",
    "else:\n",
    "    print(\"Plotly PNG sanity check skipped (kaleido not available).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O-AVGjFP3YYJ",
   "metadata": {
    "id": "O-AVGjFP3YYJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 0.3b â€” Imports & Global Theme\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import textwrap\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn is used for a couple of charts (heatmap, bars)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    _HAS_SNS = True\n",
    "except Exception:\n",
    "    _HAS_SNS = False\n",
    "\n",
    "# Plotly for interactive charts (pie/donut, radar)\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.io as pio\n",
    "    pio.templates.default = \"plotly_white\"\n",
    "    _HAS_PLOTLY = True\n",
    "except Exception:\n",
    "    _HAS_PLOTLY = False\n",
    "\n",
    "# ---- Matplotlib defaults (export-safe) ----\n",
    "mpl.rcParams[\"figure.dpi\"] = 140\n",
    "mpl.rcParams[\"savefig.dpi\"] = 300\n",
    "mpl.rcParams[\"font.size\"] = 11\n",
    "mpl.rcParams[\"axes.spines.top\"] = False\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"axes.titleweight\"] = \"semibold\"\n",
    "mpl.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "if _HAS_SNS:\n",
    "    sns.set_context(\"talk\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(f\"Imports ok â€” seaborn={_HAS_SNS}, plotly={_HAS_PLOTLY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9wtlyz2G3YYJ",
   "metadata": {
    "id": "9wtlyz2G3YYJ",
    "tags": [
     "data"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 0.3 â€” Paths & Runtime Config\n",
    "# --- USER INPUT: Specify the path to your input JSON file here ---\n",
    "INPUT_JSON = Path(\"/content/content_complete_summary.json\") # <--- CHANGE THIS PATH\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_FIG_DIR = Path(\"figures\")\n",
    "OUTPUT_REP_DIR = Path(\"reports\")\n",
    "\n",
    "# Exports\n",
    "EXPORT_PNG = True\n",
    "EXPORT_SVG = False\n",
    "EXPORT_HTML = True\n",
    "\n",
    "# Ensure directories exist\n",
    "OUTPUT_FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_REP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _ts() -> str:\n",
    "    return dt.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "print(\"Input JSON:\", INPUT_JSON.resolve())\n",
    "print(\"Figures  ->\", OUTPUT_FIG_DIR.resolve())\n",
    "print(\"Reports  ->\", OUTPUT_REP_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BdNYMkb12A_f",
   "metadata": {
    "id": "BdNYMkb12A_f"
   },
   "outputs": [],
   "source": [
    "# Cell 0.2a â€” Plotly â†’ PNG support (kaleido helper)\n",
    "# Place this cell AFTER your Paths & Runtime Config (Cell 0.3) so OUTPUT_* dirs exist.\n",
    "\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "_HAS_PLOTLY_PNG = False\n",
    "\n",
    "try:\n",
    "    import plotly.io as pio\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Try a dry-run export; if it fails, attempt to install kaleido.\n",
    "    try:\n",
    "        _ = pio.to_image(go.Figure(go.Scatter(x=[0], y=[0])), format=\"png\")\n",
    "        _HAS_PLOTLY_PNG = True\n",
    "    except Exception:\n",
    "        try:\n",
    "            print(\"Attempting to install 'kaleido' for Plotly image exportâ€¦\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaleido\"])\n",
    "            _ = pio.to_image(go.Figure(go.Scatter(x=[0], y=[0])), format=\"png\")\n",
    "            _HAS_PLOTLY_PNG = True\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Plotly PNG export unavailable (kaleido install failed).\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Plotly not available; PNG export skipped.\", e)\n",
    "\n",
    "def save_plotly_png_and_html(fig, png_path: Path | None, html_path: Path | None):\n",
    "    \"\"\"Save Plotly figure to PNG (if kaleido is available) and HTML (always).\"\"\"\n",
    "    if png_path is not None:\n",
    "        if _HAS_PLOTLY_PNG:\n",
    "            try:\n",
    "                fig.write_image(str(png_path))\n",
    "                print(\"PNG saved:\", png_path)\n",
    "            except Exception as e:\n",
    "                print(\"[WARN] write_image failed:\", e)\n",
    "        else:\n",
    "            print(\"[INFO] To enable PNG export, install: pip install kaleido\")\n",
    "\n",
    "    if html_path is not None:\n",
    "        try:\n",
    "            fig.write_html(str(html_path), include_plotlyjs=\"cdn\", full_html=True)\n",
    "            print(\"HTML saved:\", html_path)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] write_html failed:\", e)\n",
    "\n",
    "print(\"Plotly PNG export available:\", _HAS_PLOTLY_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UOyiy_EE9BSX",
   "metadata": {
    "id": "UOyiy_EE9BSX"
   },
   "outputs": [],
   "source": [
    "# Cell 1.1 â€” Load JSON (Interactive Input)\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Define the load_json function here\n",
    "def load_json(path: Path) -> Dict[str, Any]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"JSON not found: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Temporarily capture stdout/stderr to avoid mixing input prompt\n",
    "# with other print statements from previous runs if cell output is not cleared.\n",
    "# This isn't strictly necessary for input() to work, but makes the prompt cleaner.\n",
    "# old_stdout = sys.stdout\n",
    "# sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "try:\n",
    "    # --- USER INPUT: Paste path to your JSON file here ---\n",
    "    in_path = input(\"ðŸ“ Paste path to your JSON file: \").strip()\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    # sys.stdout = old_stdout # Restore stdout\n",
    "\n",
    "    base = Path(in_path).expanduser().resolve()\n",
    "\n",
    "    if not base.exists():\n",
    "        raise FileNotFoundError(f\"JSON file not found: {base}\")\n",
    "\n",
    "    global INPUT_JSON # Update the global variable defined in Cell 0.3\n",
    "    INPUT_JSON = base\n",
    "\n",
    "    global raw\n",
    "    raw = load_json(INPUT_JSON)\n",
    "\n",
    "    print(f\"\\nJSON loaded successfully from: {INPUT_JSON}\")\n",
    "    print(\"Top-level keys:\", list(raw.keys()))\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    # sys.stdout = old_stdout # Restore stdout in case of error\n",
    "    print(f\"\\nError: {e}\", file=sys.stderr)\n",
    "except Exception as e:\n",
    "    # sys.stdout = old_stdout # Restore stdout in case of error\n",
    "    print(f\"\\nAn error occurred: {e}\", file=sys.stderr)\n",
    "\n",
    "# Note: The variable 'raw' will be updated globally after successful loading.\n",
    "# Subsequent cells that use 'raw' should be run after the \"JSON loaded successfully\" message appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XcbuQiwO3YYK",
   "metadata": {
    "id": "XcbuQiwO3YYK",
    "tags": [
     "data"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 1.2 â€” Schema Check & Accessors\n",
    "REQUIRED_KEYS = [\"summary\", \"doc_metrics\", \"rows\"]\n",
    "\n",
    "missing = [k for k in REQUIRED_KEYS if k not in raw]\n",
    "if missing:\n",
    "    print(\"[WARN] Missing keys in JSON:\", missing)\n",
    "else:\n",
    "    print(\"Schema checks passed.\")\n",
    "\n",
    "def getd(d: Dict[str, Any], key: str, default=None):\n",
    "    v = d.get(key, default)\n",
    "    return default if v is None else v\n",
    "\n",
    "summary = getd(raw, \"summary\", {})\n",
    "doc_metrics = getd(raw, \"doc_metrics\", {})\n",
    "rows = getd(raw, \"rows\", [])\n",
    "\n",
    "print(f\"rows: {len(rows)} sentence-level entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbzIXzQO3YYK",
   "metadata": {
    "id": "cbzIXzQO3YYK",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 1.3 â€” Normalize to DataFrames (+ helpers) â€” FIXED FOR YOUR SCHEMA\n",
    "def coerce_frame(obj):\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj.copy()\n",
    "    if isinstance(obj, list):\n",
    "        return pd.DataFrame(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        if all(not isinstance(v, (list, dict)) for v in obj.values()):\n",
    "            return pd.DataFrame([obj])\n",
    "        return pd.DataFrame([obj])\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def safe_percentages_from_counts(df: pd.DataFrame, count_col: str = \"count\") -> pd.DataFrame:\n",
    "    if df.empty or count_col not in df:\n",
    "        return df.assign(percent=0.0)\n",
    "    vals = pd.to_numeric(df[count_col], errors=\"coerce\").fillna(0)\n",
    "    total = vals.sum()\n",
    "    if total <= 0:\n",
    "        return df.assign(percent=0.0)\n",
    "    return df.assign(percent=(vals / total) * 100)\n",
    "\n",
    "# -------- unpack using your schema --------\n",
    "origin_dist = summary.get(\"origin_distribution\", {})            # may be fractions (sumâ‰ˆ1) or counts\n",
    "mod_dist = summary.get(\"modification_distribution\", {})         # may be fractions (sumâ‰ˆ1) or counts\n",
    "\n",
    "# Build origin / mod frames\n",
    "df_origin = pd.DataFrame([{\"category\": k, \"count\": v} for k, v in origin_dist.items()])\n",
    "df_mod    = pd.DataFrame([{\"category\": k, \"count\": v} for k, v in mod_dist.items()])\n",
    "\n",
    "# Coerce, then compute % (works whether they are counts or fractions)\n",
    "for _df in (df_origin, df_mod):\n",
    "    if not _df.empty:\n",
    "        _df[\"count\"] = pd.to_numeric(_df[\"count\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "df_origin = safe_percentages_from_counts(df_origin)\n",
    "df_mod    = safe_percentages_from_counts(df_mod)\n",
    "\n",
    "# Document metrics: flatten stylometrics\n",
    "doc_metrics_flat = dict(doc_metrics)\n",
    "sty = doc_metrics_flat.pop(\"stylometrics\", {}) or {}\n",
    "for k, v in sty.items():\n",
    "    doc_metrics_flat[f\"sty.{k}\"] = v\n",
    "df_doc_metrics = coerce_frame(doc_metrics_flat)\n",
    "\n",
    "# Rows (sentence-level)\n",
    "df_rows = coerce_frame(rows)\n",
    "\n",
    "# Shares â€” prefer doc-level p_ai; else infer from rows\n",
    "df_shares = pd.DataFrame(columns=[\"author\", \"share\"])\n",
    "p_ai_doc = pd.to_numeric(pd.Series([doc_metrics.get(\"p_ai\")]), errors=\"coerce\").dropna()\n",
    "if not p_ai_doc.empty:\n",
    "    ai_share = float(np.clip(p_ai_doc.iloc[0], 0, 1))\n",
    "    df_shares = pd.DataFrame([{\"author\": \"ai\", \"share\": ai_share},\n",
    "                              {\"author\": \"human\", \"share\": 1 - ai_share}])\n",
    "\n",
    "print(\"Frames built:\")\n",
    "print(\"- df_origin:\", df_origin.shape)\n",
    "print(\"- df_mod:\", df_mod.shape)\n",
    "print(\"- df_shares:\", df_shares.shape, \"(doc p_ai used)\" if not df_shares.empty else \"(to infer)\")\n",
    "print(\"- df_doc_metrics (flattened):\", df_doc_metrics.shape)\n",
    "print(\"- df_rows:\", df_rows.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "almiBWdb3YYL",
   "metadata": {
    "id": "almiBWdb3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 2.1 â€” KPI Table (Document-Level) â€” FIXED TO PULL FROM doc_metrics / stylometrics\n",
    "def kpi_val_from_doc(key: str, default=None):\n",
    "    return pd.to_numeric(pd.Series([doc_metrics.get(key)]), errors=\"coerce\").iloc[0]\n",
    "\n",
    "# Readability/stats live under stylometrics\n",
    "sty = doc_metrics.get(\"stylometrics\", {}) or {}\n",
    "\n",
    "kpis = {\n",
    "    \"perplexity\":          kpi_val_from_doc(\"perplexity\"),\n",
    "    \"burstiness_std\":      kpi_val_from_doc(\"burstiness_std\"),\n",
    "    \"fano_factor\":         kpi_val_from_doc(\"fano_factor\"),\n",
    "    \"ai_share (doc p_ai)\": pd.to_numeric(pd.Series([doc_metrics.get(\"p_ai\")]), errors=\"coerce\").iloc[0],\n",
    "    # stylometrics\n",
    "    \"ttr\":                  sty.get(\"ttr\"),\n",
    "    \"avg_word_len\":         sty.get(\"avg_word_len\"),\n",
    "    \"avg_sent_len_tokens\":  sty.get(\"avg_sent_len_tokens\"),\n",
    "    \"var_sent_len_tokens\":  sty.get(\"var_sent_len_tokens\"),\n",
    "    \"flesch_reading_ease\":  sty.get(\"flesch_reading_ease\"),\n",
    "    \"flesch_kincaid_grade\": sty.get(\"flesch_kincaid_grade\"),\n",
    "    \"gunning_fog\":          sty.get(\"gunning_fog\"),\n",
    "}\n",
    "df_kpis = pd.DataFrame([kpis]).T.reset_index()\n",
    "df_kpis.columns = [\"metric\", \"value\"]\n",
    "display(df_kpis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bM4BkYt23YYL",
   "metadata": {
    "id": "bM4BkYt23YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 2.2 â€” Readability Badge Bars (Matplotlib) â€” FIXED to include FK Grade\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.8, 2.8))\n",
    "\n",
    "# Preferred canonical keys\n",
    "canon_keys = [\n",
    "    (\"Flesch Reading Ease\", \"flesch_reading_ease\"),\n",
    "    (\"Fleschâ€“Kincaid Grade\", \"flesch_kincaid_grade\"),  # << correct key\n",
    "    (\"Gunning Fog\", \"gunning_fog\"),\n",
    "]\n",
    "\n",
    "# Light alias map (handled if your KPI table used other names)\n",
    "aliases = {\n",
    "    \"fk_grade\": \"flesch_kincaid_grade\",\n",
    "    \"fkgrade\": \"flesch_kincaid_grade\",\n",
    "}\n",
    "\n",
    "# Helper to pull a value from df_kpis or doc_metrics.stylometrics\n",
    "def get_readability_value(key: str):\n",
    "    # 1) df_kpis (if present)\n",
    "    if \"df_kpis\" in globals() and isinstance(df_kpis, pd.DataFrame) and not df_kpis.empty:\n",
    "        target = key\n",
    "        # accept alias rows in df_kpis\n",
    "        if target not in df_kpis[\"metric\"].values:\n",
    "            for a, canon in aliases.items():\n",
    "                if canon == key and a in df_kpis[\"metric\"].values:\n",
    "                    target = a\n",
    "                    break\n",
    "        if target in df_kpis[\"metric\"].values:\n",
    "            v = pd.to_numeric(df_kpis.set_index(\"metric\").loc[target][\"value\"], errors=\"coerce\")\n",
    "            if pd.notna(v):\n",
    "                return float(v)\n",
    "\n",
    "    # 2) fallback: doc_metrics.stylometrics\n",
    "    sty = (doc_metrics.get(\"stylometrics\") or {})\n",
    "    if key not in sty:\n",
    "        # try alias in stylometrics too\n",
    "        for a, canon in aliases.items():\n",
    "            if canon == key and a in sty:\n",
    "                key = a\n",
    "                break\n",
    "    v = sty.get(key, None)\n",
    "    try:\n",
    "        return None if v is None else float(v)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Collect values in the desired display order\n",
    "labels, values = [], []\n",
    "for display, k in canon_keys:\n",
    "    v = get_readability_value(k)\n",
    "    if v is not None:\n",
    "        labels.append(display)\n",
    "        values.append(v)\n",
    "\n",
    "if values:\n",
    "    y = np.arange(len(values))\n",
    "    ax.barh(y, values)\n",
    "    ax.set_yticks(y, labels)\n",
    "    ax.set_title(\"Readability Overview\", fontweight=\"semibold\")\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    for idx, v in enumerate(values):\n",
    "        ax.text(v, idx, f\"  {v:.2f}\", va=\"center\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No readability metrics found.\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "\n",
    "figpath = OUTPUT_FIG_DIR / f\"readability_bars_{_ts()}.png\"\n",
    "if EXPORT_PNG:\n",
    "    fig.savefig(figpath, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved:\", figpath if EXPORT_PNG else \"(not saved)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eAqExC9z4KH9",
   "metadata": {
    "id": "eAqExC9z4KH9"
   },
   "outputs": [],
   "source": [
    "# D3.1 â€” Origin distribution root-cause diagnostics\n",
    "\n",
    "print(\"=== JSON Summary Keys ===\")\n",
    "print(list(summary.keys()))\n",
    "\n",
    "print(\"\\n=== Stages (from summary.stages) ===\")\n",
    "stages_list = (summary.get(\"stages\") or [\"draft\",\"refined\",\"edited\",\"final\"])\n",
    "print(stages_list)\n",
    "\n",
    "print(\"\\n=== summary.origin_distribution (raw) ===\")\n",
    "print(summary.get(\"origin_distribution\"))\n",
    "\n",
    "print(\"\\n=== df_origin (as built earlier) ===\")\n",
    "try:\n",
    "    display(df_origin)\n",
    "except Exception as e:\n",
    "    print(\"df_origin not defined:\", e)\n",
    "\n",
    "# Try to see if we have any per-row stage labels we can count\n",
    "rows_df = df_rows.copy() if 'df_rows' in globals() else pd.DataFrame()\n",
    "stage_cols = [c for c in rows_df.columns if c.lower() in\n",
    "              [\"stage\", \"origin_stage\", \"author_stage\", \"stage_label\", \"source_stage\"]]\n",
    "print(\"\\n=== Candidate stage columns in rows ===\")\n",
    "print(stage_cols)\n",
    "\n",
    "if not rows_df.empty and stage_cols:\n",
    "    for c in stage_cols:\n",
    "        counts = rows_df[c].astype(str).str.lower().value_counts(dropna=False)\n",
    "        print(f\"\\nValue counts in rows['{c}']:\")\n",
    "        print(counts)\n",
    "\n",
    "# Inspect stage_metrics presence/shape\n",
    "print(\"\\n=== summary.stage_metrics present? ===\")\n",
    "has_stage_metrics = isinstance(summary.get(\"stage_metrics\"), dict)\n",
    "print(has_stage_metrics)\n",
    "if has_stage_metrics:\n",
    "    print(\"stage_metrics keys:\", list(summary[\"stage_metrics\"].keys())[:10])\n",
    "    # optional: show any explicit counts if they exist\n",
    "    for st, met in summary[\"stage_metrics\"].items():\n",
    "        if isinstance(met, dict):\n",
    "            # sniff for any count info\n",
    "            for k in [\"sentences_total\", \"tokens_total\", \"count\", \"counts\"]:\n",
    "                if k in met:\n",
    "                    print(f\"  {st} has {k} =\", met[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344dLAwY3YYL",
   "metadata": {
    "id": "344dLAwY3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 3.1 â€” Origin Distribution (Donut) â€” FIXED for stage alignment & PNG export\n",
    "\n",
    "if _HAS_PLOTLY and not df_origin.empty:\n",
    "    df_plot = df_origin.copy()\n",
    "\n",
    "    # === Patch: Align categories to canonical stages ===\n",
    "    stages_list = summary.get(\"stages\") or [\"draft\", \"refined\", \"edited\", \"final\"]\n",
    "\n",
    "    # If only percent exists, reconstruct count proxy\n",
    "    if \"percent\" in df_plot.columns and \"count\" not in df_plot.columns:\n",
    "        df_plot[\"count\"] = df_plot[\"percent\"] / 100.0\n",
    "\n",
    "    # Map \"other\" -> \"final\" if needed\n",
    "    if \"final\" in stages_list and \"final\" not in df_plot[\"category\"].values:\n",
    "        if \"other\" in df_plot[\"category\"].values:\n",
    "            df_plot.loc[df_plot[\"category\"].str.lower().eq(\"other\"), \"category\"] = \"final\"\n",
    "\n",
    "    # Collapse duplicates and align to stage order\n",
    "    df_plot = (\n",
    "        df_plot.groupby(\"category\", as_index=False)[\"count\"].sum()\n",
    "        if \"count\" in df_plot.columns else\n",
    "        df_plot.groupby(\"category\", as_index=False)[\"percent\"].sum()\n",
    "    )\n",
    "    complete = pd.DataFrame({\"category\": stages_list})\n",
    "    df_plot = complete.merge(df_plot, on=\"category\", how=\"left\")\n",
    "\n",
    "    # Fill missing numeric columns\n",
    "    for c in [\"count\", \"percent\"]:\n",
    "        if c in df_plot.columns:\n",
    "            df_plot[c] = pd.to_numeric(df_plot[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # Recompute percent from count if available\n",
    "    if \"count\" in df_plot.columns:\n",
    "        total = df_plot[\"count\"].sum()\n",
    "        df_plot[\"percent\"] = (df_plot[\"count\"] / total) * 100.0 if total > 0 else 0.0\n",
    "    else:\n",
    "        s = df_plot[\"percent\"].sum()\n",
    "        df_plot[\"percent\"] = (df_plot[\"percent\"] / s * 100.0) if s > 0 else 0.0\n",
    "\n",
    "    # === Plot ===\n",
    "    fig = px.pie(\n",
    "        df_plot,\n",
    "        values=\"percent\",\n",
    "        names=\"category\",\n",
    "        hole=0.45,\n",
    "        title=\"Origin Distribution (by percent)\",\n",
    "    )\n",
    "\n",
    "    png_path  = OUTPUT_FIG_DIR / f\"origin_donut_{_ts()}.png\" if EXPORT_PNG else None\n",
    "    html_path = OUTPUT_REP_DIR / f\"origin_donut_{_ts()}.html\" if EXPORT_HTML else None\n",
    "    save_plotly_png_and_html(fig, png_path, html_path)\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Origin distribution not available or plotly missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iRYGAkEV3YYL",
   "metadata": {
    "id": "iRYGAkEV3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 3.2 â€” Modification Distribution (Ordered Bar) â€” SAFE FOR FRACTIONS\n",
    "if not df_mod.empty and {\"category\", \"percent\"}.issubset(df_mod.columns):\n",
    "    order = [\"unchanged\", \"minor_edit\", \"major_edit\", \"other\"]\n",
    "    categories = [c for c in order if c in df_mod[\"category\"].tolist()] + \\\n",
    "                 [c for c in df_mod[\"category\"].tolist() if c not in order]\n",
    "    df_plot = df_mod.copy()\n",
    "    df_plot[\"category\"] = pd.Categorical(df_plot[\"category\"], categories=categories, ordered=True)\n",
    "    df_plot = df_plot.sort_values(\"category\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 3.0))\n",
    "    if _HAS_SNS:\n",
    "        sns.barplot(data=df_plot, x=\"category\", y=\"percent\", ax=ax)\n",
    "    else:\n",
    "        ax.bar(df_plot[\"category\"].astype(str), df_plot[\"percent\"])\n",
    "    ax.set_title(\"Modification Distribution (percent)\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Percent\")\n",
    "    for i, row in df_plot.reset_index(drop=True).iterrows():\n",
    "        ax.text(i, row[\"percent\"], f\"{row['percent']:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "    figpath = OUTPUT_FIG_DIR / f\"mod_distribution_{_ts()}.png\"\n",
    "    if EXPORT_PNG:\n",
    "        fig.savefig(figpath, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved:\", figpath if EXPORT_PNG else \"(not saved)\")\n",
    "else:\n",
    "    print(\"Modification distribution not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vehGrmaa3YYL",
   "metadata": {
    "id": "vehGrmaa3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 3.3 â€” Stacked Bar: AI vs Human Share â€” FIXED FOR p_ai_ensemble / p_ai_transformer\n",
    "def infer_shares_from_rows(df_rows: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    if df_rows.empty:\n",
    "        return None\n",
    "    # prefer ensemble, then transformer, then generic\n",
    "    for key in [\"p_ai_ensemble\", \"p_ai_transformer\", \"p_ai\", \"ai_probability\", \"ai_prob\", \"prob_ai\"]:\n",
    "        if key in df_rows.columns:\n",
    "            s = pd.to_numeric(df_rows[key], errors=\"coerce\")\n",
    "            if s.notna().any():\n",
    "                s = s.clip(0, None)\n",
    "                if s.max() > 1.0:\n",
    "                    s = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "                return pd.DataFrame([{\"author\": \"ai\", \"share\": float(s.mean())},\n",
    "                                     {\"author\": \"human\", \"share\": 1 - float(s.mean())}])\n",
    "    for key in [\"label\", \"pred_label\", \"author_label\"]:\n",
    "        if key in df_rows.columns:\n",
    "            s = (pd.to_numeric(df_rows[key], errors=\"coerce\") == 1).astype(float)\n",
    "            return pd.DataFrame([{\"author\": \"ai\", \"share\": float(s.mean())},\n",
    "                                 {\"author\": \"human\", \"share\": 1 - float(s.mean())}])\n",
    "    return None\n",
    "\n",
    "df_shares_use = df_shares.copy()\n",
    "if df_shares_use.empty:\n",
    "    inferred = infer_shares_from_rows(df_rows)\n",
    "    if inferred is not None:\n",
    "        df_shares_use = inferred\n",
    "\n",
    "if not df_shares_use.empty and {\"author\", \"share\"}.issubset(df_shares_use.columns):\n",
    "    shares = df_shares_use.set_index(\"author\")[\"share\"].reindex([\"human\", \"ai\"]).fillna(0)\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 1.0))\n",
    "    ax.barh([\"Contribution\"], [shares[\"human\"]], label=\"Human\")\n",
    "    ax.barh([\"Contribution\"], [shares[\"ai\"]], left=[shares[\"human\"]], label=\"AI\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xticks([0, 0.25, 0.5, 0.75, 1.0], labels=[f\"{int(v*100)}%\" for v in [0, .25, .5, .75, 1]])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"Overall Contribution â€” Human vs AI\")\n",
    "    ax.legend(loc=\"upper center\", ncols=2, frameon=False)\n",
    "    figpath = OUTPUT_FIG_DIR / f\"stacked_share_{_ts()}.png\"\n",
    "    if EXPORT_PNG:\n",
    "        fig.savefig(figpath, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved:\", figpath if EXPORT_PNG else \"(not saved)\")\n",
    "else:\n",
    "    print(\"No share data available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EtEmWP6t_QR7",
   "metadata": {
    "id": "EtEmWP6t_QR7"
   },
   "outputs": [],
   "source": [
    "# Cell 3.4 â€” Aggregate Contribution Pie (Human vs AI) â€” PNG+HTML\n",
    "\n",
    "def _infer_ai_share(df_rows: pd.DataFrame) -> float | None:\n",
    "    for k in [\"p_ai_ensemble\",\"p_ai_transformer\",\"p_ai\",\"ai_probability\",\"ai_prob\",\"prob_ai\"]:\n",
    "        if k in df_rows.columns:\n",
    "            s = pd.to_numeric(df_rows[k], errors=\"coerce\")\n",
    "            if s.notna().any():\n",
    "                s = s.clip(0, None)\n",
    "                if s.max() > 1.0:\n",
    "                    s = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "                return float(s.mean())\n",
    "    for k in [\"label\",\"pred_label\",\"author_label\"]:\n",
    "        if k in df_rows.columns:\n",
    "            s = (pd.to_numeric(df_rows[k], errors=\"coerce\") == 1).astype(float)\n",
    "            return float(s.mean())\n",
    "    return None\n",
    "\n",
    "ai_share = None\n",
    "if pd.notna(doc_metrics.get(\"p_ai\", None)):\n",
    "    try: ai_share = float(doc_metrics[\"p_ai\"])\n",
    "    except: ai_share = None\n",
    "if ai_share is None: ai_share = _infer_ai_share(df_rows)\n",
    "\n",
    "if _HAS_PLOTLY and ai_share is not None:\n",
    "    human = max(0.0, 1.0 - ai_share); ai = max(0.0, ai_share)\n",
    "    df_agg = pd.DataFrame({\"Author\":[\"Human\",\"AI\"], \"Share\":[human, ai]})\n",
    "    fig = px.pie(df_agg, values=\"Share\", names=\"Author\", title=\"Aggregate Contribution â€” Human vs AI\", hole=0.25)\n",
    "\n",
    "    png_path  = OUTPUT_FIG_DIR / f\"agg_contrib_pie_{_ts()}.png\" if EXPORT_PNG else None\n",
    "    html_path = OUTPUT_REP_DIR / f\"agg_contrib_pie_{_ts()}.html\" if EXPORT_HTML else None\n",
    "    save_plotly_png_and_html(fig, png_path, html_path)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Aggregate contribution pie skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dhbPuyWa3YYL",
   "metadata": {
    "id": "dhbPuyWa3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 4.1 â€” Stylometric Table â€” FIXED (pull from stylometrics)\n",
    "sty = doc_metrics.get(\"stylometrics\", {}) or {}\n",
    "stylometric_keys = [\n",
    "    \"ttr\", \"avg_word_len\", \"avg_sent_len_tokens\", \"var_sent_len_tokens\",\n",
    "    \"comma_rate\", \"semicolon_rate\", \"colon_rate\",\n",
    "    \"flesch_reading_ease\", \"flesch_kincaid_grade\", \"gunning_fog\"\n",
    "]\n",
    "stylometrics = {k: sty.get(k) for k in stylometric_keys if k in sty}\n",
    "df_stylo = pd.DataFrame([stylometrics]).T.reset_index()\n",
    "df_stylo.columns = [\"feature\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1yuXPlSp3YYL",
   "metadata": {
    "id": "1yuXPlSp3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 4.2 â€” Radar (Spider) Chart â€” MULTI-STAGE, FIXED BOUNDS, PNG+HTML\n",
    "\n",
    "# 1) Which features we show on the radar:\n",
    "radar_features = [\"ttr\", \"avg_word_len\", \"avg_sent_len_tokens\", \"comma_rate\", \"lexical_density\"]\n",
    "\n",
    "# 2) Get global fixed bounds (prefer JSON feature_bounds; else sensible defaults)\n",
    "default_bounds = {\n",
    "    \"ttr\": (0.0, 0.80),\n",
    "    \"avg_word_len\": (3.0, 7.0),\n",
    "    \"avg_sent_len_tokens\": (5.0, 40.0),\n",
    "    \"comma_rate\": (0.0, 0.05),\n",
    "    \"lexical_density\": (0.0, 0.70),\n",
    "}\n",
    "feature_bounds = (raw.get(\"feature_bounds\") or {})  # optional in JSON\n",
    "bounds = {k: tuple(feature_bounds.get(k, default_bounds[k])) for k in radar_features}\n",
    "\n",
    "# 3) Pull multi-stage metrics OR fall back to single doc_metrics\n",
    "stages = (summary.get(\"stages\") or [])\n",
    "stage_metrics = summary.get(\"stage_metrics\") or {}\n",
    "\n",
    "# Helper: derive lexical_density from POS if missing\n",
    "def ensure_lexical_density(stylo: dict) -> dict:\n",
    "    if \"lexical_density\" not in stylo or stylo.get(\"lexical_density\") is None:\n",
    "        pos_keys = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\n",
    "        if all(k in stylo for k in pos_keys):\n",
    "            stylo = dict(stylo)  # copy\n",
    "            stylo[\"lexical_density\"] = sum(float(stylo.get(k, 0) or 0) for k in pos_keys)\n",
    "    return stylo\n",
    "\n",
    "def extract_feature_vector(stylo: dict) -> list[float | None]:\n",
    "    stylo = ensure_lexical_density(stylo or {})\n",
    "    vals = []\n",
    "    for k in radar_features:\n",
    "        v = stylo.get(k, None)\n",
    "        vals.append(None if v is None else float(v))\n",
    "    return vals\n",
    "\n",
    "def normalize_to_bounds(values: list[float | None], bounds_map: dict) -> list[float]:\n",
    "    out = []\n",
    "    for val, name in zip(values, radar_features):\n",
    "        if val is None:\n",
    "            out.append(0.0)\n",
    "            continue\n",
    "        lo, hi = bounds_map[name]\n",
    "        if hi <= lo:\n",
    "            out.append(0.0)\n",
    "        else:\n",
    "            out.append(max(0.0, min(1.0, (val - lo) / (hi - lo))))\n",
    "    return out\n",
    "\n",
    "profiles = []   # list of (label, r_values_norm, raw_values)\n",
    "labels = []\n",
    "\n",
    "if stages and stage_metrics:\n",
    "    # MULTI-STAGE path\n",
    "    for st in stages:\n",
    "        met = stage_metrics.get(st) or {}\n",
    "        sty = (met.get(\"stylometrics\") or {})\n",
    "        raw_vals = extract_feature_vector(sty)\n",
    "        norm_vals = normalize_to_bounds(raw_vals, bounds)\n",
    "        profiles.append((st, norm_vals, raw_vals))\n",
    "        labels.append(st)\n",
    "else:\n",
    "    # SINGLE-DOC fallback (previous behavior)\n",
    "    sty = (doc_metrics.get(\"stylometrics\") or {})\n",
    "    raw_vals = extract_feature_vector(sty)\n",
    "    norm_vals = normalize_to_bounds(raw_vals, bounds)\n",
    "    profiles.append((\"document\", norm_vals, raw_vals))\n",
    "    labels.append(\"document\")\n",
    "\n",
    "# 4) Plot with Plotly and export PNG/HTML\n",
    "if _HAS_PLOTLY:\n",
    "    theta = [k.replace(\"_\", \" \").title() for k in radar_features]\n",
    "    fig = go.Figure()\n",
    "    for lbl, r_norm, raw_vals in profiles:\n",
    "        # Close the loop\n",
    "        r_closed = list(r_norm) + [r_norm[0]]\n",
    "        th_closed = theta + [theta[0]]\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=r_closed,\n",
    "            theta=th_closed,\n",
    "            fill='toself',\n",
    "            name=lbl\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Stylometric Profile by Stage (Fixed Scaling)\",\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.15, xanchor=\"center\", x=0.5)\n",
    "    )\n",
    "\n",
    "    # Annotate bounds in the subtitle (optional)\n",
    "    subtitle = \" | \".join([f\"{k.replace('_',' ').title()}: {bounds[k][0]}â€“{bounds[k][1]}\" for k in radar_features])\n",
    "    fig.update_layout(margin=dict(t=90))\n",
    "    fig.add_annotation(\n",
    "        text=subtitle,\n",
    "        x=0.5, xref=\"paper\", y=1.06, yref=\"paper\",\n",
    "        showarrow=False, align=\"center\", font=dict(size=10)\n",
    "    )\n",
    "\n",
    "    png_path  = OUTPUT_FIG_DIR / f\"radar_{_ts()}.png\" if EXPORT_PNG else None\n",
    "    html_path = OUTPUT_REP_DIR / f\"radar_{_ts()}.html\" if EXPORT_HTML else None\n",
    "    try:\n",
    "        save_plotly_png_and_html(fig, png_path, html_path)  # uses the helper from Cell 0.2a\n",
    "    except NameError:\n",
    "        # Fallback if helper cell didn't run\n",
    "        if EXPORT_HTML:\n",
    "            fig.write_html(str(html_path), include_plotlyjs=\"cdn\", full_html=True)\n",
    "            print(\"HTML saved:\", html_path)\n",
    "        if EXPORT_PNG:\n",
    "            try:\n",
    "                fig.write_image(str(png_path))\n",
    "                print(\"PNG saved:\", png_path)\n",
    "            except Exception as e:\n",
    "                print(\"[WARN] Plotly PNG export unavailable:\", e)\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Plotly not available for radar chart.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EC6Ze4kY3YYL",
   "metadata": {
    "id": "EC6Ze4kY3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 4.3 â€” POS Distribution (Horizontal Bar) â€” FIXED (from stylometrics POS shares)\n",
    "pos_keys = [\"DET\",\"PROPN\",\"PRON\",\"AUX\",\"PUNCT\",\"SCONJ\",\"ADP\",\"ADJ\",\"NOUN\",\"NUM\",\"VERB\",\"PART\",\"CCONJ\",\"ADV\"]\n",
    "pos_ratios = {k: sty[k] for k in pos_keys if k in sty}\n",
    "if pos_ratios:\n",
    "    df_pos = pd.DataFrame([{\"pos\": k, \"ratio\": v} for k, v in pos_ratios.items()])\n",
    "    df_pos[\"ratio\"] = pd.to_numeric(df_pos[\"ratio\"], errors=\"coerce\").fillna(0.0)\n",
    "    if df_pos[\"ratio\"].max() > 1.0:\n",
    "        df_pos[\"ratio\"] = df_pos[\"ratio\"] / 100.0\n",
    "    df_pos = df_pos.sort_values(\"ratio\", ascending=True)\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 3.5))\n",
    "    if _HAS_SNS:\n",
    "        sns.barplot(data=df_pos, x=\"ratio\", y=\"pos\", ax=ax, orient=\"h\")\n",
    "    else:\n",
    "        ax.barh(df_pos[\"pos\"], df_pos[\"ratio\"])\n",
    "    ax.set_title(\"POS Tag Ratios\")\n",
    "    ax.set_xlabel(\"Ratio\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    for i, row in df_pos.reset_index(drop=True).iterrows():\n",
    "        ax.text(row[\"ratio\"], i, f\"  {row['ratio']:.2f}\", va=\"center\")\n",
    "    path = OUTPUT_FIG_DIR / f\"pos_ratios_{_ts()}.png\"\n",
    "    if EXPORT_PNG: fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path if EXPORT_PNG else \"(not saved)\")\n",
    "else:\n",
    "    print(\"No POS ratios available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kxPPwOOI3YYL",
   "metadata": {
    "id": "kxPPwOOI3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 5.1 â€” Stage Similarity Heatmap (ensure labels)\n",
    "sim_mat = None\n",
    "stages = None\n",
    "\n",
    "if isinstance(summary.get(\"stage_similarity\"), list):\n",
    "    sim_mat = np.array(summary[\"stage_similarity\"], dtype=float)\n",
    "if isinstance(summary.get(\"stages\"), list):\n",
    "    stages = [str(s) for s in summary[\"stages\"]]\n",
    "\n",
    "if sim_mat is not None and sim_mat.ndim == 2 and _HAS_SNS:\n",
    "    fig, ax = plt.subplots(figsize=(5 + 0.35 * sim_mat.shape[0], 4.8))\n",
    "    sns.heatmap(\n",
    "        sim_mat, annot=True, fmt=\".2f\", cmap=\"vlag\", square=True,\n",
    "        xticklabels=stages if stages else \"auto\",\n",
    "        yticklabels=stages if stages else \"auto\",\n",
    "        cbar_kws={\"label\": \"Similarity (0â€“1)\"}\n",
    "    )\n",
    "    ax.set_title(\"Stage-to-Stage Similarity\")\n",
    "    path = OUTPUT_FIG_DIR / f\"similarity_heatmap_{_ts()}.png\"\n",
    "    if EXPORT_PNG:\n",
    "        fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path if EXPORT_PNG else \"(not saved)\")\n",
    "else:\n",
    "    print(\"No similarity matrix available (or seaborn missing).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vGH1wtN_3YYL",
   "metadata": {
    "id": "vGH1wtN_3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 5.2 â€” Top-K Edits Table â€” FIXED (uses your similarity/distance fields and final_sentence)\n",
    "def compute_change_score(row: pd.Series) -> Optional[float]:\n",
    "    # Prefer explicit edit/distance\n",
    "    for key in [\"levenshtein\", \"edit_distance\", \"delta_score\", \"change_score\"]:\n",
    "        if key in row.index and pd.notna(row[key]):\n",
    "            return float(row[key])\n",
    "    # Otherwise, invert similarity\n",
    "    for key in [\"combined_sim\", \"semantic_sim\", \"tfidf_sim\", \"jaccard\", \"similarity_to_previous\", \"similarity_prev\"]:\n",
    "        if key in row.index and pd.notna(row[key]):\n",
    "            try: return 1.0 - float(row[key])\n",
    "            except Exception: pass\n",
    "    return None\n",
    "\n",
    "df_edits = df_rows.copy()\n",
    "text_col = None\n",
    "for cand in [\"final_sentence\", \"sentence\", \"text\", \"content\", \"span\", \"origin_sentence\"]:\n",
    "    if cand in df_edits.columns:\n",
    "        text_col = cand; break\n",
    "if text_col is None:\n",
    "    print(\"No text column found for edits table.\")\n",
    "else:\n",
    "    df_edits[\"change_score\"] = df_edits.apply(compute_change_score, axis=1)\n",
    "    df_edits = df_edits.dropna(subset=[\"change_score\"])\n",
    "    if not df_edits.empty:\n",
    "        df_top = df_edits.sort_values(\"change_score\", ascending=False).head(15)\n",
    "    else:\n",
    "        print(\"No change scores available to rank edits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g9CakQ0q3YYL",
   "metadata": {
    "id": "g9CakQ0q3YYL",
    "tags": [
     "viz"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 6.1 â€” Sentence-Level AI Probability Heatmap â€” FIXED (p_ai_ensemble / p_ai_transformer + final_sentence)\n",
    "from typing import Optional\n",
    "\n",
    "def get_ai_probs(df_rows: pd.DataFrame) -> Optional[np.ndarray]:\n",
    "    if df_rows.empty:\n",
    "        return None\n",
    "    for key in [\"p_ai_ensemble\", \"p_ai_transformer\", \"p_ai\", \"ai_probability\", \"ai_prob\", \"prob_ai\"]:\n",
    "        if key in df_rows.columns:\n",
    "            s = pd.to_numeric(df_rows[key], errors=\"coerce\")\n",
    "            if s.notna().any():\n",
    "                s = s.clip(0, None)\n",
    "                if s.max() > 1.0:\n",
    "                    s = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
    "                return s.values.reshape(-1, 1)\n",
    "    for key in [\"label\", \"pred_label\", \"author_label\"]:\n",
    "        if key in df_rows.columns:\n",
    "            s = (pd.to_numeric(df_rows[key], errors=\"coerce\") == 1).astype(float)\n",
    "            return s.values.reshape(-1, 1)\n",
    "    return None\n",
    "\n",
    "def get_texts(df_rows: pd.DataFrame) -> list[str]:\n",
    "    for c in [\"final_sentence\", \"sentence\", \"text\", \"content\", \"span\", \"origin_sentence\"]:\n",
    "        if c in df_rows.columns:\n",
    "            return df_rows[c].fillna(\"\").astype(str).tolist()\n",
    "    return [f\"Sentence {i+1}\" for i in range(len(df_rows))]\n",
    "\n",
    "# ---- Plot heatmap ----\n",
    "probs = get_ai_probs(df_rows)\n",
    "if probs is not None and _HAS_SNS:\n",
    "    N = probs.shape[0]\n",
    "    fig, ax = plt.subplots(figsize=(4.0, max(3.5, N * 0.18)))\n",
    "    sns.heatmap(probs, cmap=\"vlag\", annot=False, cbar_kws={\"label\": \"AI Probability\"}, vmin=0, vmax=1)\n",
    "    ax.set_title(\"Sentence-Level AI Probability Heatmap\")\n",
    "    ax.set_xlabel(\"Probability\")\n",
    "    ax.set_ylabel(\"Sentence #\")\n",
    "    step = max(1, N // 30)\n",
    "    ax.set_yticks(np.arange(0, N, step) + 0.5, labels=[str(i+1) for i in range(0, N, step)])\n",
    "    path = OUTPUT_FIG_DIR / f\"ai_prob_heatmap_{_ts()}.png\"\n",
    "    if EXPORT_PNG: fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path if EXPORT_PNG else \"(not saved)\")\n",
    "elif probs is None:\n",
    "    print(\"No per-sentence probabilities available in rows.\")\n",
    "else:\n",
    "    print(\"Seaborn not available; cannot render heatmap.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3ozvQ3YYM",
   "metadata": {
    "id": "69c3ozvQ3YYM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6.2 â€” Probability Stripe (1D Rug/Line) â€” FIXED\n",
    "probs = get_ai_probs(df_rows)\n",
    "if probs is not None:\n",
    "    p = probs.flatten()\n",
    "    x = np.arange(len(p))\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 1.8))\n",
    "    ax.scatter(x, [0.5] * len(p), s=np.clip(p * 80, 8, 80), alpha=0.85)  # size ~ prob\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(\"AI Probability Stripe (by Sentence Order)\")\n",
    "    path = OUTPUT_FIG_DIR / f\"ai_prob_stripe_{_ts()}.png\"\n",
    "    if EXPORT_PNG: fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved:\", path if EXPORT_PNG else \"(not saved)\")\n",
    "else:\n",
    "    print(\"No probabilities available for stripe plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DadSJqwP3YYM",
   "metadata": {
    "id": "DadSJqwP3YYM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6.3 â€” Text Highlighting (HTML Export) â€” FIXED (uses final_sentence + probs)\n",
    "def build_highlight_html(df_rows: pd.DataFrame, title: str = \"Sentence-Level Highlighting\") -> str:\n",
    "    probs = get_ai_probs(df_rows)\n",
    "    if probs is None:\n",
    "        return \"<p>No AI probabilities available to highlight.</p>\"\n",
    "    texts = get_texts(df_rows)\n",
    "\n",
    "    def prob_to_hex(p: float) -> str:\n",
    "        p = float(np.clip(p, 0, 1))\n",
    "        if p <= 0.5:\n",
    "            t = p / 0.5\n",
    "            r = int((1 - t) * 66 + t * 255)\n",
    "            g = int((1 - t) * 133 + t * 255)\n",
    "            b = int((1 - t) * 244 + t * 255)\n",
    "        else:\n",
    "            t = (p - 0.5) / 0.5\n",
    "            r = int((1 - t) * 255 + t * 219)\n",
    "            g = int((1 - t) * 255 + t * 68)\n",
    "            b = int((1 - t) * 255 + t * 55)\n",
    "        return f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "\n",
    "    items = []\n",
    "    for i, (txt, p) in enumerate(zip(texts, probs.flatten().tolist())):\n",
    "        color = prob_to_hex(p)\n",
    "        txt_color = \"#000000\" if p < 0.65 else \"#111111\"\n",
    "        items.append(\n",
    "            f'<div style=\"background:{color};padding:6px;border-radius:6px;margin:6px 0;color:{txt_color};\">'\n",
    "            f'<strong>{i+1:02d}</strong> â€” {txt} '\n",
    "            f'<span style=\"float:right;opacity:.8;\">AI p={p:.2f}</span>'\n",
    "            f'</div>'\n",
    "        )\n",
    "\n",
    "    html = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/>\n",
    "<title>{title}</title>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/>\n",
    "<style>\n",
    "body {{ font-family: -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; margin: 18px; line-height: 1.45; }}\n",
    "h1 {{ font-size: 20px; margin: 0 0 12px 0; }}\n",
    ".sub {{ color: #555; margin-bottom: 16px; }}\n",
    ".legend {{ margin: 16px 0; display:flex; gap:8px; align-items:center; }}\n",
    ".legend .swatch {{ width: 18px; height: 18px; border-radius: 4px; display:inline-block; border:1px solid rgba(0,0,0,.08); }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Document Analysis: Sentence-Level Highlighting</h1>\n",
    "<div class=\"sub\">Background color encodes AI probability (blue=lower, white=mid, red=higher).</div>\n",
    "<div class=\"legend\">\n",
    "  <span class=\"swatch\" style=\"background:#4285f4;\"></span> 0.0\n",
    "  <span class=\"swatch\" style=\"background:#ffffff;\"></span> 0.5\n",
    "  <span class=\"swatch\" style=\"background:#db4437;\"></span> 1.0\n",
    "</div>\n",
    "{''.join(items)}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    return html\n",
    "\n",
    "html = build_highlight_html(df_rows)\n",
    "outpath = OUTPUT_REP_DIR / f\"highlighted_{_ts()}.html\"\n",
    "if EXPORT_HTML:\n",
    "    with outpath.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "print(\"Highlight HTML saved:\", outpath if EXPORT_HTML else \"(not saved)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VK9JGnwt3YYM",
   "metadata": {
    "id": "VK9JGnwt3YYM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7.1 â€” Figure Export Helper (Matplotlib bulk save)\n",
    "def save_all_open_matplotlib(prefix: str = \"figure\"):\n",
    "    fig_nums = plt.get_fignums()\n",
    "    saved_paths = []\n",
    "    for i in fig_nums:\n",
    "        fig = plt.figure(i)\n",
    "        if EXPORT_PNG:\n",
    "            path_png = OUTPUT_FIG_DIR / f\"{prefix}_{i}_{_ts()}.png\"\n",
    "            fig.savefig(path_png, bbox_inches=\"tight\")\n",
    "            saved_paths.append(str(path_png))\n",
    "        if EXPORT_SVG:\n",
    "            path_svg = OUTPUT_FIG_DIR / f\"{prefix}_{i}_{_ts()}.svg\"\n",
    "            fig.savefig(path_svg, bbox_inches=\"tight\")\n",
    "            saved_paths.append(str(path_svg))\n",
    "    return saved_paths\n",
    "\n",
    "print(\"Bulk save helper ready. Call save_all_open_matplotlib() if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MCI7w-qp3YYM",
   "metadata": {
    "id": "MCI7w-qp3YYM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7.2 â€” Lightweight HTML Report Stitching\n",
    "def build_summary_report(\n",
    "    df_kpis: pd.DataFrame,\n",
    "    fig_dir: Path = OUTPUT_FIG_DIR,\n",
    "    extra_sections: Optional[List[str]] = None\n",
    ") -> str:\n",
    "    # Collect the most recent relevant figures (best-effort)\n",
    "    pngs = sorted(fig_dir.glob(\"*.png\"), key=os.path.getmtime)[-12:]\n",
    "    imgs_html = \"\\n\".join(\n",
    "        f'<div style=\"margin:10px 0;\"><img src=\"../{p}\" style=\"max-width:100%;border:1px solid #eee;border-radius:8px\"/></div>'\n",
    "        for p in map(lambda x: x.as_posix(), pngs)\n",
    "    )\n",
    "    # KPIs table (basic HTML)\n",
    "    if not df_kpis.empty:\n",
    "        tbl = df_kpis.copy()\n",
    "        tbl[\"metric\"] = tbl[\"metric\"].str.replace(\"_\", \" \").str.title()\n",
    "        tbl_html = (\n",
    "            \"<table style='border-collapse:collapse;width:100%'>\"\n",
    "            \"<thead><tr><th style='text-align:left;padding:6px;border-bottom:1px solid #ddd'>Metric</th>\"\n",
    "            \"<th style='text-align:left;padding:6px;border-bottom:1px solid #ddd'>Value</th></tr></thead><tbody>\"\n",
    "            + \"\\n\".join(\n",
    "                f\"<tr><td style='padding:6px;border-bottom:1px solid #f2f2f2'>{m}</td>\"\n",
    "                f\"<td style='padding:6px;border-bottom:1px solid #f2f2f2'>{'' if pd.isna(v) else v}</td></tr>\"\n",
    "                for m, v in tbl[[\"metric\",\"value\"]].itertuples(index=False)\n",
    "            )\n",
    "            + \"</tbody></table>\"\n",
    "        )\n",
    "    else:\n",
    "        tbl_html = \"<p>No KPI metrics available.</p>\"\n",
    "\n",
    "    extra_html = \"\\n\".join(extra_sections or [])\n",
    "\n",
    "    return f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta charset=\"utf-8\"/>\n",
    "<title>Visualization Report</title>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/>\n",
    "<style>\n",
    "body {{ font-family: -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; margin: 24px; line-height: 1.5; color: #111; }}\n",
    "h1, h2 {{ font-weight: 600; }}\n",
    "h1 {{ font-size: 22px; margin: 0 0 14px 0; }}\n",
    "h2 {{ font-size: 18px; margin: 22px 0 8px 0; }}\n",
    ".section {{ margin-bottom: 26px; }}\n",
    ".kicker {{ color: #444; margin-bottom: 12px; }}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Visualization Report</h1>\n",
    "<div class=\"kicker\">Generated: {dt.datetime.now().isoformat(timespec='seconds')}</div>\n",
    "<div class=\"section\">\n",
    "  <h2>Key Metrics</h2>\n",
    "  {tbl_html}\n",
    "</div>\n",
    "<div class=\"section\">\n",
    "  <h2>Figures</h2>\n",
    "  {imgs_html if imgs_html else \"<p>No figure PNGs found.</p>\"}\n",
    "</div>\n",
    "{extra_html}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "report_html = build_summary_report(df_kpis, OUTPUT_FIG_DIR)\n",
    "report_path = OUTPUT_REP_DIR / f\"visualization_report_{_ts()}.html\"\n",
    "if EXPORT_HTML:\n",
    "    with report_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_html)\n",
    "print(\"Report saved:\", report_path if EXPORT_HTML else \"(not saved)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aMR_Xb74if_",
   "metadata": {
    "id": "1aMR_Xb74if_"
   },
   "outputs": [],
   "source": [
    "# Cell 7.3 â€” Manifest (Artifacts Index)\n",
    "def make_manifest(fig_dir: Path = OUTPUT_FIG_DIR, rep_dir: Path = OUTPUT_REP_DIR) -> Dict[str, Any]:\n",
    "    figs = sorted(fig_dir.glob(\"*.*\"), key=os.path.getmtime)\n",
    "    reps = sorted(rep_dir.glob(\"*.*\"), key=os.path.getmtime)\n",
    "    return {\n",
    "        \"generated_at\": dt.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"figures\": [p.as_posix() for p in figs],\n",
    "        \"reports\": [p.as_posix() for p in reps],\n",
    "        \"input_json\": INPUT_JSON.as_posix(),\n",
    "    }\n",
    "\n",
    "manifest = make_manifest()\n",
    "manifest_path = OUTPUT_REP_DIR / f\"manifest_{_ts()}.json\"\n",
    "with manifest_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(\"Manifest saved:\", manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wJHdZj0_4kXv",
   "metadata": {
    "id": "wJHdZj0_4kXv"
   },
   "outputs": [],
   "source": [
    "# Cell 8.0 â€” Sanity Echo (what data we actually found)\n",
    "print(\"Summary:\")\n",
    "print(\" - df_origin:\", df_origin.shape, \"sample:\", df_origin.head(3).to_dict('records'))\n",
    "print(\" - df_mod:\", df_mod.shape, \"sample:\", df_mod.head(3).to_dict('records'))\n",
    "print(\" - df_shares:\", df_shares.shape, df_shares.to_dict('records') if not df_shares.empty else \"(empty)\")\n",
    "print(\" - doc-level metrics keys:\", list(doc_metrics.keys())[:10])\n",
    "print(\" - stylometrics keys:\", list(doc_metrics.get('stylometrics', {}).keys())[:10])\n",
    "print(\" - df_rows:\", df_rows.shape, \"cols:\", list(df_rows.columns))\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc282d4d",
   "metadata": {
    "id": "fc282d4d"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "def create_output_zip(manifest: Dict[str, Any], json_path: Path, fig_dir: Path, rep_dir: Path):\n",
    "    \"\"\"Creates a zip archive of generated figures and reports in the directory containing the input JSON.\"\"\"\n",
    "\n",
    "    if not json_path.is_file():\n",
    "        print(f\"Warning: Input JSON path is not a file or does not exist: {json_path}. Skipping creating zip archive.\")\n",
    "        return\n",
    "\n",
    "    json_directory = json_path.parent\n",
    "    zip_filename = f\"visualization_outputs_{_ts()}.zip\"\n",
    "    zip_path = json_directory / zip_filename\n",
    "\n",
    "    print(f\"Creating zip archive: {zip_path}\")\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            # Collect all generated files from the manifest\n",
    "            all_generated_files = manifest.get(\"figures\", []) + manifest.get(\"reports\", [])\n",
    "\n",
    "            for file_path_str in all_generated_files:\n",
    "                try:\n",
    "                    source_path = Path(file_path_str)\n",
    "                     # Ensure source path is absolute and exists\n",
    "                    if not source_path.is_absolute():\n",
    "                        # Assuming figures and reports were saved relative to the current working directory (likely /content)\n",
    "                        source_path = Path(os.getcwd()) / source_path\n",
    "\n",
    "\n",
    "                    if source_path.exists():\n",
    "                        # Add file to zip, preserving directory structure relative to fig_dir/rep_dir\n",
    "                        if source_path.is_relative_to(fig_dir):\n",
    "                            arcname = source_path.relative_to(fig_dir)\n",
    "                        elif source_path.is_relative_to(rep_dir):\n",
    "                             arcname = source_path.relative_to(rep_dir)\n",
    "                        else:\n",
    "                             arcname = source_path.name # Fallback\n",
    "\n",
    "                        zipf.write(source_path, arcname=arcname)\n",
    "                        # print(f\"Added to zip: {source_path} as {arcname}\") # Uncomment for verbose output\n",
    "                    else:\n",
    "                        print(f\"Warning: Generated file not found: {source_path}. Skipping.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error adding file {file_path_str} to zip: {e}\")\n",
    "\n",
    "        print(f\"Zip archive created successfully: {zip_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating zip archive: {e}\")\n",
    "\n",
    "\n",
    "# Call the function using the manifest and INPUT_JSON from previous cells\n",
    "if 'manifest' in globals() and 'INPUT_JSON' in globals():\n",
    "    create_output_zip(manifest, INPUT_JSON, OUTPUT_FIG_DIR, OUTPUT_REP_DIR)\n",
    "else:\n",
    "    print(\"Manifest or INPUT_JSON not found. Please run preceding cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-2ZyglUn6Vzb",
   "metadata": {
    "id": "-2ZyglUn6Vzb"
   },
   "outputs": [],
   "source": [
    "# Diagnostics: why a given visualization didn't render/save\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def yesno(x): return \"âœ…\" if x else \"âŒ\"\n",
    "\n",
    "checks = []\n",
    "\n",
    "# --- Environment flags & paths ---\n",
    "checks.append((\"seaborn available (_HAS_SNS)\", _HAS_SNS))\n",
    "checks.append((\"plotly available (_HAS_PLOTLY)\", _HAS_PLOTLY))\n",
    "\n",
    "checks.append((\"EXPORT_PNG is True\", bool(EXPORT_PNG)))\n",
    "checks.append((\"EXPORT_HTML is True\", bool(EXPORT_HTML)))\n",
    "\n",
    "checks.append((\"Figures dir exists\", OUTPUT_FIG_DIR.exists()))\n",
    "checks.append((\"Reports dir exists\", OUTPUT_REP_DIR.exists()))\n",
    "\n",
    "# --- JSON / DataFrames present ---\n",
    "checks.append((\"raw loaded (raw)\", 'raw' in globals()))\n",
    "checks.append((\"summary present\", isinstance(summary, dict)))\n",
    "checks.append((\"doc_metrics present\", isinstance(doc_metrics, dict)))\n",
    "checks.append((\"rows present\", isinstance(rows, list)))\n",
    "\n",
    "# --- Normalized frames ---\n",
    "checks.append((\"df_origin non-empty & has [category,percent]\", (not df_origin.empty) and {\"category\",\"percent\"}.issubset(df_origin.columns)))\n",
    "checks.append((\"df_mod non-empty & has [category,percent]\", (not df_mod.empty) and {\"category\",\"percent\"}.issubset(df_mod.columns)))\n",
    "checks.append((\"df_doc_metrics non-empty\", not df_doc_metrics.empty))\n",
    "checks.append((\"df_rows non-empty\", not df_rows.empty))\n",
    "\n",
    "# --- Readability metrics (3 bars expected) ---\n",
    "sty = doc_metrics.get(\"stylometrics\", {}) or {}\n",
    "checks.append((\"Readability: flesch_reading_ease exists\", \"flesch_reading_ease\" in sty))\n",
    "checks.append((\"Readability: flesch_kincaid_grade exists\", \"flesch_kincaid_grade\" in sty))\n",
    "checks.append((\"Readability: gunning_fog exists\", \"gunning_fog\" in sty))\n",
    "\n",
    "# --- Stacked share gates ---\n",
    "# starting df_shares_use logic again here to see if we can compute it\n",
    "def _infer_shares_from_rows(df_rows: pd.DataFrame):\n",
    "    for k in [\"p_ai_ensemble\",\"p_ai_transformer\",\"p_ai\",\"ai_probability\",\"ai_prob\",\"prob_ai\"]:\n",
    "        if k in df_rows.columns:\n",
    "            s = pd.to_numeric(df_rows[k], errors=\"coerce\")\n",
    "            if s.notna().any():\n",
    "                s = s.clip(0, None)\n",
    "                if s.max() > 1.0:\n",
    "                    s = (s - s.min())/(s.max() - s.min() + 1e-9)\n",
    "                return float(s.mean())\n",
    "    for k in [\"label\",\"pred_label\",\"author_label\"]:\n",
    "        if k in df_rows.columns:\n",
    "            s = (pd.to_numeric(df_rows[k], errors=\"coerce\") == 1).astype(float)\n",
    "            return float(s.mean())\n",
    "    return None\n",
    "\n",
    "p_ai_doc = pd.to_numeric(pd.Series([doc_metrics.get(\"p_ai\")]), errors=\"coerce\").dropna()\n",
    "ai_share_from_doc = float(p_ai_doc.iloc[0]) if not p_ai_doc.empty else None\n",
    "ai_share_from_rows = _infer_shares_from_rows(df_rows)\n",
    "\n",
    "checks.append((\"AI share (doc p_ai present)\", ai_share_from_doc is not None))\n",
    "checks.append((\"AI share (rows inferable)\", ai_share_from_rows is not None))\n",
    "\n",
    "# --- POS ratios presence ---\n",
    "pos_keys = [\"DET\",\"PROPN\",\"PRON\",\"AUX\",\"PUNCT\",\"SCONJ\",\"ADP\",\"ADJ\",\"NOUN\",\"NUM\",\"VERB\",\"PART\",\"CCONJ\",\"ADV\"]\n",
    "checks.append((\"POS ratios present (any of the standard tags)\", any(k in sty for k in pos_keys)))\n",
    "\n",
    "# --- Stage metrics & similarity (for multi-profile radar & heatmap) ---\n",
    "summ = summary or {}\n",
    "has_stage_metrics = isinstance(summ.get(\"stage_metrics\"), dict) and bool(summ.get(\"stage_metrics\"))\n",
    "has_stages = isinstance(summ.get(\"stages\"), list) and bool(summ.get(\"stages\"))\n",
    "has_stage_similarity = isinstance(summ.get(\"stage_similarity\"), list)\n",
    "checks.append((\"Stage metrics present\", has_stage_metrics))\n",
    "checks.append((\"Stages list present\", has_stages))\n",
    "checks.append((\"Stage similarity matrix present\", has_stage_similarity))\n",
    "\n",
    "# --- Probability columns for per-sentence plots ---\n",
    "prob_cols_present = any(c in df_rows.columns for c in [\"p_ai_ensemble\",\"p_ai_transformer\",\"p_ai\",\"ai_probability\",\"ai_prob\",\"prob_ai\",\"label\",\"pred_label\",\"author_label\"])\n",
    "checks.append((\"Per-sentence probabilities/labels present\", prob_cols_present))\n",
    "\n",
    "# ---- Print summary table ----\n",
    "print(\"=== Visualization Gates ===\")\n",
    "for name, ok in checks:\n",
    "    print(f\"{yesno(ok)} {name}\")\n",
    "\n",
    "# ---- Quick hints based on failures ----\n",
    "print(\"\\n=== Hints ===\")\n",
    "if not _HAS_PLOTLY:\n",
    "    print(\"- Plotly is missing; donut/radar won't render. Install plotly.\")\n",
    "try:\n",
    "    # See if PNG export via kaleido is available from the helper\n",
    "    print(f\"- Plotly PNG export available: {_HAS_PLOTLY_PNG}\")\n",
    "except NameError:\n",
    "    print(\"- Plotly PNG helper cell (0.2a) not found; PNG export for Plotly may be skipped.\")\n",
    "\n",
    "if df_origin.empty or not {\"category\",\"percent\"}.issubset(df_origin.columns):\n",
    "    print(\"- Origin donut skipped: df_origin empty or missing ['category','percent']. Ensure origin_distribution present (fractions or counts).\")\n",
    "\n",
    "if df_mod.empty or not {\"category\",\"percent\"}.issubset(df_mod.columns):\n",
    "    print(\"- Modification bar skipped: df_mod empty or missing ['category','percent']. Ensure modification_distribution present.\")\n",
    "\n",
    "if not prob_cols_present:\n",
    "    print(\"- Sentence-level heatmap/stripe skipped: no probability or label columns found in rows.\")\n",
    "\n",
    "if not ((\"flesch_reading_ease\" in sty) and (\"flesch_kincaid_grade\" in sty) and (\"gunning_fog\" in sty)):\n",
    "    print(\"- Readability bars may show fewer than 3 items: double-check stylometrics keys spelled exactly.\")\n",
    "\n",
    "if not has_stage_metrics:\n",
    "    print(\"- Multi-stage radar skipped: add summary.stage_metrics with per-stage stylometrics.\")\n",
    "if not has_stage_similarity:\n",
    "    print(\"- Stage similarity heatmap skipped: add summary.stage_similarity (square matrix) aligned to summary.stages.\")\n",
    "\n",
    "# ---- List saved artifacts ----\n",
    "def list_latest(p: Path, glob=\"*.*\", n=20):\n",
    "    files = sorted(p.glob(glob), key=lambda x: x.stat().st_mtime if x.exists() else 0, reverse=True)\n",
    "    return [f\"{i+1:02d}. {f.name}\" for i,f in enumerate(files[:n])]\n",
    "\n",
    "print(\"\\n=== Files in figures/ (newest first) ===\")\n",
    "for line in list_latest(OUTPUT_FIG_DIR, \"*.png\", n=30):\n",
    "    print(line)\n",
    "\n",
    "print(\"\\n=== Files in reports/ (newest first) ===\")\n",
    "for line in list_latest(OUTPUT_REP_DIR, \"*.*\", n=30):\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1cd16",
   "metadata": {},
   "source": [
    "# âœ¨ Enhanced Visuals & Report (Schema â‰¥ 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c396e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced setup (imports, paths, load JSON)\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import plotly.graph_objs as go\n",
    "    _HAS_PLOTLY = True\n",
    "except Exception:\n",
    "    _HAS_PLOTLY = False\n",
    "\n",
    "try:\n",
    "    import kaleido  # noqa\n",
    "    _HAS_KALEIDO = True\n",
    "except Exception:\n",
    "    _HAS_KALEIDO = False\n",
    "\n",
    "JSON_PATH = Path(\"/mnt/data/content_complete_summary.json\")\n",
    "OUTPUT_DIR = Path.cwd() / \"enhanced_outputs\"\n",
    "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
    "REP_DIR = OUTPUT_DIR / \"reports\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def _safe_get(d, *ks, default=None):\n",
    "    cur = d\n",
    "    for k in ks:\n",
    "        if isinstance(cur, dict) and k in cur:\n",
    "            cur = cur[k]\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "schema_version = data.get(\"schema_version\") or _safe_get(data, \"version\")\n",
    "stages = data.get(\"stages\") or _safe_get(data, \"summary\", \"stages\") or [\"draft\",\"refined\",\"edited\",\"final\"]\n",
    "print(\"Loaded:\", JSON_PATH.name, \"| schema:\", schema_version, \"| stages:\", stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Validator for new structures\n",
    "has_stage_sentences = isinstance(data.get(\"stage_sentences\"), dict)\n",
    "has_alignments = isinstance(data.get(\"alignments\"), list) or isinstance(data.get(\"alignments_pairs\"), list)\n",
    "rows_list = data.get(\"rows\", [])\n",
    "need = {\"final_sentence_id\",\"final_token_count\",\"final_paragraph_id\"}\n",
    "if rows_list and not need.issubset(set(rows_list[0].keys())):\n",
    "    print(\"Note: rows missing enriched fields; using fallbacks.\")\n",
    "print(\"Has stage_sentences:\", has_stage_sentences, \"| Has alignments:\", has_alignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse data into DataFrames\n",
    "rows = pd.DataFrame(data.get(\"rows\", []))\n",
    "\n",
    "if not rows.empty:\n",
    "    for col in [\"p_ai_ensemble\",\"final_token_count\",\"origin_token_count\"]:\n",
    "        if col in rows: rows[col] = pd.to_numeric(rows[col], errors=\"coerce\")\n",
    "\n",
    "stage_sentences = data.get(\"stage_sentences\", {}) if has_stage_sentences else {}\n",
    "stage_frames = {st: pd.DataFrame(stage_sentences.get(st, [])) for st in stages}\n",
    "for st, df in stage_frames.items():\n",
    "    if not df.empty:\n",
    "        if \"token_count\" in df: df[\"token_count\"] = pd.to_numeric(df[\"token_count\"], errors=\"coerce\")\n",
    "        if \"p_ai\" in df.columns: df[\"p_ai_ensemble\"] = df[\"p_ai\"].map(lambda x: (x or {}).get(\"ensemble\") if isinstance(x, dict) else np.nan)\n",
    "\n",
    "def pairs_to_df(block):\n",
    "    out = []\n",
    "    for b in data.get(\"alignments_pairs\", []) or []:\n",
    "        fs, ts = b.get(\"from_stage\"), b.get(\"to_stage\")\n",
    "        for p in b.get(\"pairs\", []):\n",
    "            r = {\"from_stage\": fs, \"to_stage\": ts}; r.update(p); out.append(r)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def rich_to_df(block):\n",
    "    out = []\n",
    "    for b in data.get(\"alignments\", []) or []:\n",
    "        fs, ts = b.get(\"from\"), b.get(\"to\")\n",
    "        for l in b.get(\"links\", []):\n",
    "            r = {\"from_stage\": fs, \"to_stage\": ts}; r.update(l); out.append(r)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "df_pairs = pairs_to_df(data.get(\"alignments_pairs\"))\n",
    "df_links = rich_to_df(data.get(\"alignments\"))\n",
    "print(\"rows:\", len(rows), \"| pairs:\", len(df_pairs), \"| links:\", len(df_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weighted AI vs Human share\n",
    "def compute_weighted_ai_share(rows_df: pd.DataFrame) -> dict:\n",
    "    if rows_df.empty or \"p_ai_ensemble\" not in rows_df:\n",
    "        return {\"weighted_ai\": np.nan, \"unweighted_ai\": np.nan, \"n\": 0}\n",
    "    p = rows_df[\"p_ai_ensemble\"].clip(0,1)\n",
    "    if \"final_token_count\" in rows_df and rows_df[\"final_token_count\"].notna().any():\n",
    "        w = rows_df[\"final_token_count\"].fillna(1.0)\n",
    "    else:\n",
    "        w = rows_df.get(\"text\", pd.Series([\"\"]*len(rows_df))).map(lambda s: len(str(s).split()))\n",
    "    return {\"weighted_ai\": float((p*w).sum()/max(w.sum(),1)), \"unweighted_ai\": float(p.mean()), \"n\": int(len(rows_df))}\n",
    "\n",
    "ws = compute_weighted_ai_share(rows)\n",
    "print(\"Weighted:\", round(ws[\"weighted_ai\"],4), \"Unweighted:\", round(ws[\"unweighted_ai\"],4), \"N:\", ws[\"n\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ai = ws[\"weighted_ai\"]; human = 1 - ai if ai==ai else np.nan\n",
    "ax.bar([\"AI\",\"Human\"], [ai, human])\n",
    "ax.set_ylim(0,1); ax.set_title(\"Final: Length-Weighted AI vs Human Share\")\n",
    "for i,val in enumerate([ai, human]):\n",
    "    if val==val: ax.text(i, min(val+0.02, 0.98), f\"{val:.2%}\", ha=\"center\")\n",
    "fig.tight_layout(); fig.savefig((FIG_DIR / \"ai_vs_human_weighted.png\"), dpi=160); plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed920810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Per-stage AI-likeness trend (mean + IQR)\n",
    "def compute_stage_ai_trend(stage_frames) -> pd.DataFrame:\n",
    "    recs = []\n",
    "    for st, df in stage_frames.items():\n",
    "        if df.empty or \"p_ai_ensemble\" not in df: continue\n",
    "        vals = df[\"p_ai_ensemble\"].astype(float).clip(0,1)\n",
    "        recs.append({\"stage\": st, \"n\": int(vals.notna().sum()), \"mean\": float(vals.mean()),\n",
    "                     \"p25\": float(vals.quantile(0.25)), \"p75\": float(vals.quantile(0.75))})\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "trend = compute_stage_ai_trend(stage_frames)\n",
    "if not trend.empty:\n",
    "    order = [s for s in [\"draft\",\"refined\",\"edited\",\"final\"] if s in trend[\"stage\"].tolist()]\n",
    "    ts = trend.set_index(\"stage\").loc[order].reset_index()\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.plot(ts[\"stage\"], ts[\"mean\"], marker=\"o\")\n",
    "    ax.fill_between(ts[\"stage\"], ts[\"p25\"], ts[\"p75\"], alpha=0.2, step=\"mid\")\n",
    "    ax.set_ylim(0,1); ax.set_title(\"Per-Stage AI-likeness (mean with IQR)\"); ax.set_ylabel(\"p_ai (ensemble)\")\n",
    "    fig.tight_layout(); fig.savefig((FIG_DIR / \"ai_trend_per_stage.png\"), dpi=160); plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retention metrics and Sankey\n",
    "def retention_from_links(df_links: pd.DataFrame, stage_frames) -> pd.DataFrame:\n",
    "    if df_links.empty: return pd.DataFrame()\n",
    "    recs = []\n",
    "    for (fs, ts), part in df_links.groupby([\"from_stage\",\"to_stage\"]):\n",
    "        kept = float(part.get(\"source_tokens_kept\", pd.Series(dtype=float)).fillna(0).sum())\n",
    "        newt = float(part.get(\"target_tokens_new\", pd.Series(dtype=float)).fillna(0).sum())\n",
    "        t_from = stage_frames.get(fs, pd.DataFrame()).get(\"token_count\", pd.Series(dtype=float)).sum()\n",
    "        t_to   = stage_frames.get(ts, pd.DataFrame()).get(\"token_count\", pd.Series(dtype=float)).sum()\n",
    "        accounted = kept + newt\n",
    "        add_residual = max(t_to - accounted, 0.0)\n",
    "        del_residual = max(t_from - kept, 0.0)\n",
    "        recs.append({\"from_stage\": fs, \"to_stage\": ts, \"tokens_kept\": kept, \"tokens_new\": newt,\n",
    "                     \"tokens_added_residual\": add_residual, \"tokens_deleted_residual\": del_residual})\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "ret = retention_from_links(df_links, stage_frames)\n",
    "if not ret.empty:\n",
    "    print(ret)\n",
    "\n",
    "# Editorial operations breakdown\n",
    "if not df_links.empty and \"type\" in df_links.columns:\n",
    "    ops = df_links.copy(); ops[\"count\"] = 1\n",
    "    br = ops.groupby([\"from_stage\",\"to_stage\",\"type\"])[\"count\"].sum().reset_index()\n",
    "    pv = br.pivot(index=\"type\", columns=[\"from_stage\",\"to_stage\"], values=\"count\").fillna(0)\n",
    "    fig, ax = plt.subplots(figsize=(7,4)); pv.plot(kind=\"bar\", ax=ax, rot=45)\n",
    "    ax.set_title(\"Editorial Operations Breakdown\"); fig.tight_layout()\n",
    "    fig.savefig((FIG_DIR / \"editorial_operations_breakdown.png\"), dpi=160); plt.close(fig)\n",
    "\n",
    "# Sankey\n",
    "def build_sankey(df_links: pd.DataFrame):\n",
    "    if df_links.empty: return None\n",
    "    if not _HAS_PLOTLY: return None\n",
    "    nodes, node_index = [], {}\n",
    "    for st in [\"draft\",\"refined\",\"edited\",\"final\"]:\n",
    "        if st in stage_frames and not stage_frames[st].empty:\n",
    "            node_index[st] = len(nodes); nodes.append(st)\n",
    "    agg = df_links.groupby([\"from_stage\",\"to_stage\"]).size().reset_index(name=\"n\")\n",
    "    if agg.empty: return None\n",
    "    src = [node_index[a] for a in agg[\"from_stage\"]]; tgt = [node_index[b] for b in agg[\"to_stage\"]]; val = agg[\"n\"].tolist()\n",
    "    fig = go.Figure(data=[go.Sankey(node=dict(label=nodes), link=dict(source=src, target=tgt, value=val))])\n",
    "    fig.update_layout(title_text=\"Sentence Flow Sankey\")\n",
    "    html_path = (FIG_DIR / \"sankey_sentence_flow.html\"); fig.write_html(str(html_path), include_plotlyjs=\"cdn\")\n",
    "    try:\n",
    "        fig.write_image(str(FIG_DIR / \"sankey_sentence_flow.png\"), scale=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return html_path\n",
    "\n",
    "_ = build_sankey(df_links if not df_links.empty else df_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3478168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paragraph-level charts (final stage)\n",
    "final_df = stage_frames.get(\"final\", pd.DataFrame())\n",
    "if not final_df.empty and \"paragraph_id\" in final_df.columns:\n",
    "    para = final_df.groupby(\"paragraph_id\").agg(tokens=(\"token_count\",\"sum\"), n=(\"id\",\"count\"), ai_mean=(\"p_ai_ensemble\",\"mean\")).reset_index().sort_values(\"paragraph_id\")\n",
    "    fig, ax = plt.subplots(figsize=(8,3.5)); ax.bar(para[\"paragraph_id\"].astype(str), para[\"tokens\"].astype(float)); ax.set_title(\"Final: Tokens per Paragraph\"); ax.set_xticklabels(para[\"paragraph_id\"].astype(str), rotation=45, ha=\"right\"); fig.tight_layout(); fig.savefig((FIG_DIR / \"final_tokens_by_paragraph.png\"), dpi=160); plt.close(fig)\n",
    "    fig, ax = plt.subplots(figsize=(8,3.5)); ax.bar(para[\"paragraph_id\"].astype(str), para[\"ai_mean\"].astype(float)); ax.set_ylim(0,1); ax.set_title(\"Final: Mean p_ai by Paragraph\"); ax.set_xticklabels(para[\"paragraph_id\"].astype(str), rotation=45, ha=\"right\"); fig.tight_layout(); fig.savefig((FIG_DIR / \"final_ai_by_paragraph.png\"), dpi=160); plt.close(fig)\n",
    "else:\n",
    "    print(\"No paragraph_id in final; skipping paragraph charts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdeaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Highlight HTML (final)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "final_df = stage_frames.get(\"final\", pd.DataFrame()) if \"stage_frames\" in globals() else pd.DataFrame()\n",
    "rows_df = rows if \"rows\" in globals() else pd.DataFrame()\n",
    "\n",
    "def build_highlight_html(final_df: pd.DataFrame, rows_df: pd.DataFrame, out_html):\n",
    "    if final_df.empty or \"text\" not in final_df: return None\n",
    "    cols = [\"final_sentence_id\",\"origin_version\",\"modification_label\"]\n",
    "    extra = rows_df[cols].copy() if all(c in rows_df.columns for c in cols) else pd.DataFrame(columns=cols)\n",
    "    df = final_df.merge(extra, left_on=\"id\", right_on=\"final_sentence_id\", how=\"left\")\n",
    "    def esc(s): return (str(s).replace(\"&\",\"&amp;\").replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\"))\n",
    "    items = []\n",
    "    for _, r in df.iterrows():\n",
    "        text = esc(r.get(\"text\",\"\")); pai=r.get(\"p_ai_ensemble\", float(\"nan\")); ov=r.get(\"origin_version\",\"\"); ml=r.get(\"modification_label\",\"\")\n",
    "        span = '<span class=\"sent\" data-pai=\"{pai}\" data-ov=\"{ov}\" data-ml=\"{ml}\">{text}</span>'.format(\n",
    "            pai=(\"%.4f\" % pai) if isinstance(pai,(int,float)) and pai==pai else \"\", ov=esc(ov), ml=esc(ml), text=text)\n",
    "        items.append(span)\n",
    "    html = \"\"\"\n",
    "<!doctype html>\n",
    "<html><head><meta charset=\"utf-8\" />\n",
    "<style>\n",
    "body{{font-family:sans-serif; line-height:1.6; margin:20px;}}\n",
    ".controls button{{margin-right:8px;}}\n",
    ".sent{{padding:2px 3px; border-radius:4px; display:inline;}}\n",
    ".legend{{margin:10px 0; font-size:12px;}}\n",
    "</style>\n",
    "</head><body>\n",
    "<h2>Final Sentences â€” Highlight View</h2>\n",
    "<div class=\"controls\">\n",
    "  <button onclick=\"mode='pai'; recolor()\">AI probability</button>\n",
    "  <button onclick=\"mode='ov';  recolor()\">Origin version</button>\n",
    "  <button onclick=\"mode='ml';  recolor()\">Modification label</button>\n",
    "</div>\n",
    "<div class=\"legend\" id=\"legend\"></div>\n",
    "<div id=\"content\">\n",
    "  {items}\n",
    "</div>\n",
    "<script>\n",
    "let mode = 'pai';\n",
    "function recolor(){{\n",
    "  const spans = document.querySelectorAll('.sent');\n",
    "  spans.forEach(s=>{{\n",
    "    if(mode==='pai'){{ \n",
    "      const v = parseFloat(s.dataset.pai);\n",
    "      if(!isFinite(v)){{ s.style.background=''; return; }}\n",
    "      const g = Math.round(255 - 255*v);\n",
    "      s.style.background = 'rgb(255,'+g+','+g+')';\n",
    "      s.title = 'p_ai=' + v.toFixed(2);\n",
    "    }} else if(mode==='ov'){{\n",
    "      const m = s.dataset.ov || '';\n",
    "      s.style.background = hashColor(m);\n",
    "      s.title = 'origin=' + m;\n",
    "    }} else if(mode==='ml'){{\n",
    "      const m = s.dataset.ml || '';\n",
    "      s.style.background = hashColor(m);\n",
    "      s.title = 'mod=' + m;\n",
    "    }}\n",
    "  }});\n",
    "  document.getElementById('legend').innerText = (mode==='pai' ? 'Redder = higher AI probability' : 'Categorical color by ' + mode);\n",
    "}}\n",
    "function hashColor(str){{\n",
    "  let h=0; for(let i=0;i<str.length;i++) h = (h*31 + str.charCodeAt(i))>>>0;\n",
    "  const r = 100 + (h & 127), g = 100 + ((h>>7) & 127), b = 100 + ((h>>14) & 127);\n",
    "  return 'rgb('+r+','+g+','+b+')';\n",
    "}}\n",
    "recolor();\n",
    "</script>\n",
    "</body></html>\n",
    "\"\"\".format(items=\" \".join(items))\n",
    "    Path(out_html).write_text(html, encoding=\"utf-8\"); return Path(out_html)\n",
    "\n",
    "if not final_df.empty:\n",
    "    REP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out = build_highlight_html(final_df, rows_df, REP_DIR / \"highlight_final.html\")\n",
    "    print(\"Highlight:\", out)\n",
    "else:\n",
    "    print(\"No final stage sentences found; skipping highlight.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d420ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build enhanced summary report (HTML)\n",
    "def embed_img(relpath):\n",
    "    return '<div><img src=\"../figures/{rel}\" style=\"max-width:100%;\"/></div>'.format(rel=relpath)\n",
    "\n",
    "sections = []\n",
    "for fn in [\"ai_vs_human_weighted.png\",\"ai_trend_per_stage.png\",\"editorial_operations_breakdown.png\",\"final_tokens_by_paragraph.png\",\"final_ai_by_paragraph.png\",\"sankey_sentence_flow.png\"]:\n",
    "    p = FIG_DIR / fn\n",
    "    if p.exists():\n",
    "        sections.append(embed_img(fn))\n",
    "\n",
    "if (FIG_DIR / \"sankey_sentence_flow.html\").exists():\n",
    "    sections.append('<iframe src=\"../figures/sankey_sentence_flow.html\" width=\"100%\" height=\"520\" loading=\"lazy\"></iframe>')\n",
    "if (REP_DIR / \"highlight_final.html\").exists():\n",
    "    sections.append('<p><a href=\"highlight_final.html\">Open Highlighted Final</a></p>')\n",
    "\n",
    "report_html = \"<!doctype html><html><head><meta charset='utf-8' /><title>Enhanced Visualization Report</title><meta name='viewport' content='width=device-width, initial-scale=1'/><style>body{font-family:sans-serif; margin:20px;}</style></head><body><h1>Enhanced Visualization Report</h1>{}</body></html>\".format(\"\".join(sections) if sections else \"<p>No figures available.</p>\")\n",
    "(REP_DIR / \"enhanced_visualization_report.html\").write_text(report_html, encoding=\"utf-8\")\n",
    "print(\"Report built:\", (REP_DIR / \"enhanced_visualization_report.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9180df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Diagnostics\n",
    "def overview(d):\n",
    "    return {\"top_keys\": list(d.keys()), \"has_stage_sentences\": isinstance(d.get(\"stage_sentences\"), dict),\n",
    "            \"has_alignments\": isinstance(d.get(\"alignments\"), list) or isinstance(d.get(\"alignments_pairs\"), list),\n",
    "            \"rows\": len(d.get(\"rows\", [])), \"schema_version\": d.get(\"schema_version\") or d.get(\"version\")}\n",
    "print(overview(data))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
