{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Budbjd0PUG6A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d88e08c0-6343-4c02-ccf2-a14dbc75055f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing pinned stack … (no pip cache, wheels only)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "pip install failed with exit code 1. Run this cell again after checking internet space/availability.",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m pip install failed with exit code 1. Run this cell again after checking internet space/availability.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "# cell 0.0 — ENVIRONMENT LOCK (install once, then RESTART RUNTIME)\n",
        "import os, sys, json, subprocess, platform, time, textwrap\n",
        "\n",
        "# Pinned, Py3.12-safe wheels (avoid source builds and hotfix drift)\n",
        "REQ = {\n",
        "    \"numpy\": \"1.26.4\",\n",
        "    \"pandas\": \"2.2.2\",\n",
        "    \"scipy\": \"1.13.1\",\n",
        "    \"scikit-learn\": \"1.4.2\",\n",
        "    \"pyarrow\": \"15.0.2\",\n",
        "    \"numba\": \"0.60.0\",\n",
        "    \"llvmlite\": \"0.43.0\",\n",
        "    \"umap-learn\": \"0.5.6\",\n",
        "    \"hdbscan\": \"0.8.36\",\n",
        "    \"bertopic\": \"0.16.3\",\n",
        "    \"sentence-transformers\": \"3.0.1\",\n",
        "    \"rapidfuzz\": \"3.9.6\",\n",
        "    \"nltk\": \"3.8.1\",\n",
        "    \"matplotlib\": \"3.8.4\",\n",
        "    \"tqdm\": \"4.66.4\",\n",
        "}\n",
        "\n",
        "# (Optional) keep HF cache predictable; installs won’t use pip cache to save disk\n",
        "os.environ.setdefault(\"TRANSFORMERS_CACHE\", \"/content/.cache/hf\")\n",
        "os.makedirs(\"outputs/_env\", exist_ok=True)\n",
        "\n",
        "pkgs = [f\"{k}=={v}\" for k, v in REQ.items()]\n",
        "print(\"Installing pinned stack … (no pip cache, wheels only)\")\n",
        "cmd = [sys.executable, \"-m\", \"pip\", \"install\",\n",
        "       \"--upgrade\", \"--quiet\", \"--no-input\",\n",
        "       \"--no-cache-dir\", \"--only-binary=:all:\"] + pkgs\n",
        "rc = subprocess.call(cmd)\n",
        "if rc != 0:\n",
        "    raise SystemExit(\n",
        "        f\"pip install failed with exit code {rc}. \"\n",
        "        \"Run this cell again after checking internet space/availability.\"\n",
        "    )\n",
        "\n",
        "lock = {\n",
        "    \"python\": sys.version.split()[0],\n",
        "    \"platform\": platform.platform(),\n",
        "    \"packages\": REQ,\n",
        "    \"ts\": int(time.time()),\n",
        "}\n",
        "with open(\"outputs/_env/lock.json\", \"w\") as f:\n",
        "    json.dump(lock, f, indent=2)\n",
        "\n",
        "print(\"✓ Wrote outputs/_env/lock.json\")\n",
        "print(textwrap.dedent(\"\"\"\n",
        "    IMPORTANT — ACTION REQUIRED\n",
        "    1) Restart the runtime now (e.g., Runtime ▸ Restart runtime).\n",
        "    2) Then run 0.1 and 0.2 before any module cells.\n",
        "\"\"\").strip())\n",
        "\n",
        "# Hard stop to enforce restart\n",
        "raise SystemExit(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sOjkNR_t-67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "9e4b5aa3-c171-428c-a166-2f95db6d04fc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Missing outputs/_env/lock.json — run 0.0 first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-300015157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs/_env/lock.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Missing outputs/_env/lock.json — run 0.0 first.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Thread caps & quiet tokenizers (stability/determinism)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Missing outputs/_env/lock.json — run 0.0 first."
          ]
        }
      ],
      "source": [
        "# cell 0.1 — RUNTIME GUARD + THREAD/SEED SETUP (run after restart)\n",
        "import os, json, random, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "assert Path(\"outputs/_env/lock.json\").exists(), \"Missing outputs/_env/lock.json — run 0.0 first.\"\n",
        "\n",
        "# Thread caps & quiet tokenizers (stability/determinism)\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"NUMEXPR_MAX_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "# Determinism\n",
        "SEED = int(os.environ.get(\"LSA_SEED\", \"42\"))\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "# If PyTorch is installed later modules will use it — seed it too (no-op if not available)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Non-interactive plotting & Parquet behavior\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "os.environ.setdefault(\"PYARROW_IGNORE_TIMEZONE\", \"1\")\n",
        "\n",
        "# Keep HF cache in a predictable place\n",
        "os.environ.setdefault(\"TRANSFORMERS_CACHE\", \"/content/.cache/hf\")\n",
        "\n",
        "print(\"Guard OK. SEED =\", SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX0SzGtnuMgu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "a2954f8a-75db-4c23-fd1e-205bead481e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/plots.py:448: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  axis.set_ylabel('$\\lambda$ value')\n",
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolved versions:\n",
            "{\n",
            "  \"hdbscan\": \"n/a\",\n",
            "  \"llvmlite\": \"0.43.0\",\n",
            "  \"matplotlib\": \"3.10.0\",\n",
            "  \"nltk\": \"3.9.1\",\n",
            "  \"numba\": \"0.60.0\",\n",
            "  \"numpy\": \"2.0.2\",\n",
            "  \"pandas\": \"2.2.2\",\n",
            "  \"pyarrow\": \"18.1.0\",\n",
            "  \"sklearn\": \"1.6.1\",\n",
            "  \"umap\": \"0.5.9.post2\"\n",
            "}\n",
            "\n",
            "Missing or failed imports:\n",
            " - bertopic (import name 'bertopic') — ModuleNotFoundError: No module named 'bertopic'\n",
            " - rapidfuzz (import name 'rapidfuzz') — ModuleNotFoundError: No module named 'rapidfuzz'\n",
            "\n",
            "[warn] numba (0.60.0) vs llvmlite (0.43.0) minor versions differ; if UMAP/HDBSCAN complain, reinstall as a matched pair.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Some required libraries are missing. Run the module-specific install cells first (e.g., 6.1 for BERTopic stack, 7.1 for rapidfuzz).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-327086411.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m               f\"if UMAP/HDBSCAN complain, reinstall as a matched pair.\")\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m assert not missing, (\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;34m\"Some required libraries are missing. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;34m\"Run the module-specific install cells first (e.g., 6.1 for BERTopic stack, 7.1 for rapidfuzz).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Some required libraries are missing. Run the module-specific install cells first (e.g., 6.1 for BERTopic stack, 7.1 for rapidfuzz)."
          ]
        }
      ],
      "source": [
        "# cell 0.2 — HEAVY LIBS IMPORT CHECK (must pass before Module 6+)\n",
        "import importlib, json\n",
        "\n",
        "to_check = [\n",
        "    (\"numpy\", \"numpy\"),\n",
        "    (\"pandas\", \"pandas\"),\n",
        "    (\"pyarrow\", \"pyarrow\"),\n",
        "    (\"sklearn\", \"scikit-learn\"),\n",
        "    (\"numba\", \"numba\"),\n",
        "    (\"llvmlite\", \"llvmlite\"),\n",
        "    (\"umap\", \"umap-learn\"),\n",
        "    (\"hdbscan\", \"hdbscan\"),\n",
        "    (\"bertopic\", \"bertopic\"),\n",
        "    (\"rapidfuzz\", \"rapidfuzz\"),\n",
        "    (\"nltk\", \"nltk\"),\n",
        "    (\"matplotlib\", \"matplotlib\"),\n",
        "]\n",
        "\n",
        "present, missing = {}, {}\n",
        "\n",
        "for mod_name, pkg_label in to_check:\n",
        "    try:\n",
        "        mod = importlib.import_module(mod_name)\n",
        "        present[mod_name] = getattr(mod, \"__version__\", \"n/a\")\n",
        "    except Exception as e:\n",
        "        missing[mod_name] = f\"{pkg_label} (import name '{mod_name}') — {type(e).__name__}: {e}\"\n",
        "\n",
        "# ensure sklearn version is reported cleanly\n",
        "if \"sklearn\" in present:\n",
        "    import sklearn  # noqa\n",
        "    present[\"sklearn\"] = sklearn.__version__\n",
        "\n",
        "print(\"Resolved versions:\")\n",
        "print(json.dumps(present, indent=2, sort_keys=True))\n",
        "\n",
        "if missing:\n",
        "    print(\"\\nMissing or failed imports:\")\n",
        "    for k, v in missing.items():\n",
        "        print(f\" - {v}\")\n",
        "\n",
        "# light compatibility hint only (no hard fail)\n",
        "if \"numba\" in present and \"llvmlite\" in present:\n",
        "    nb_minor = \".\".join(present[\"numba\"].split(\".\")[:2])\n",
        "    ll_minor = \".\".join(present[\"llvmlite\"].split(\".\")[:2])\n",
        "    if nb_minor != ll_minor:\n",
        "        print(f\"\\n[warn] numba ({present['numba']}) vs llvmlite ({present['llvmlite']}) minor versions differ; \"\n",
        "              f\"if UMAP/HDBSCAN complain, reinstall as a matched pair.\")\n",
        "\n",
        "assert not missing, (\n",
        "    \"Some required libraries are missing. \"\n",
        "    \"Run the module-specific install cells first (e.g., 6.1 for BERTopic stack, 7.1 for rapidfuzz).\"\n",
        ")\n",
        "print(\"Heavy libs check: OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8dqQhvhuVP5"
      },
      "outputs": [],
      "source": [
        "# cell 0.3 — OPTIONAL: PURGE BUILD/WHEEL CACHES (saves disk)\n",
        "import os, sys, shutil, subprocess, json\n",
        "from pathlib import Path\n",
        "\n",
        "def _dir_size(p: Path) -> int:\n",
        "    try:\n",
        "        return sum(f.stat().st_size for f in p.rglob(\"*\") if f.is_file())\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "targets = []\n",
        "\n",
        "# 1) pip cache (newer pip has 'pip cache purge'; else delete the dir)\n",
        "try:\n",
        "    out = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"cache\", \"dir\"], text=True).strip()\n",
        "    pip_cache_dir = Path(out)\n",
        "except Exception:\n",
        "    pip_cache_dir = Path.home() / \".cache\" / \"pip\"\n",
        "\n",
        "if pip_cache_dir.exists():\n",
        "    targets.append(pip_cache_dir)\n",
        "\n",
        "# 2) transient pip build/metadata dirs\n",
        "for p in [\"/tmp/pip-req-build\", \"/tmp/pip-modern-metadata\", \"/tmp/pip-unpack\"]:\n",
        "    if os.path.isdir(p):\n",
        "        targets.append(Path(p))\n",
        "\n",
        "# Measure before\n",
        "before = {str(p): _dir_size(p) for p in targets}\n",
        "total_before = sum(before.values())\n",
        "\n",
        "# Try the official purge first (no failure if unsupported)\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"], check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Hard delete any remaining target dirs (idempotent)\n",
        "for p in targets:\n",
        "    if p.exists():\n",
        "        shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "# Report\n",
        "freed_mb = total_before / (1024**2)\n",
        "disk_total, disk_used, disk_free = shutil.disk_usage(\"/\")\n",
        "print(json.dumps({\n",
        "    \"purged_paths\": list(before.keys()),\n",
        "    \"freed_mb_estimate\": round(freed_mb, 2),\n",
        "    \"disk_free_mb\": round(disk_free/(1024**2), 2),\n",
        "    \"note\": \"Installed packages remain intact. HF model cache left untouched.\"\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z6DBSDZvIBK"
      },
      "outputs": [],
      "source": [
        "# cell 0.4 — foundations: status logging & self-report\n",
        "from __future__ import annotations\n",
        "import datetime, json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "# Resolve outputs/ base even if \"paths\" isn't defined yet\n",
        "_out_base = getattr(globals().get(\"paths\", None), \"out_dir\", Path(\"outputs\"))\n",
        "_out_base.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODULE_STATUS: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "def _jsonify(obj: Any) -> Any:\n",
        "    \"\"\"Ensure values are JSON-serializable; fallback to string.\"\"\"\n",
        "    try:\n",
        "        json.dumps(obj)\n",
        "        return obj\n",
        "    except Exception:\n",
        "        return str(obj)\n",
        "\n",
        "def report_status(module: str, ok: bool, note: str = \"\", extra: Optional[Dict[str, Any]] = None):\n",
        "    rec = {\n",
        "        \"ok\": bool(ok),\n",
        "        \"note\": str(note),\n",
        "        \"extra\": {k: _jsonify(v) for k, v in (extra or {}).items()},\n",
        "        \"ts\": datetime.datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "    }\n",
        "    MODULE_STATUS[module] = rec\n",
        "    print(json.dumps({\"module\": module, **rec}))\n",
        "\n",
        "def dump_status_json(out_path: Path = None):\n",
        "    out_path = out_path or (_out_base / \"module_status.json\")\n",
        "    out_path.write_text(json.dumps(MODULE_STATUS, indent=2), encoding=\"utf-8\")\n",
        "    print(json.dumps({\"wrote\": str(out_path), \"entries\": len(MODULE_STATUS)}))\n",
        "\n",
        "# cell-level self-report (per rule #9)\n",
        "print(json.dumps({\"cell_id\": \"0.4 foundations: status utilities\", \"status\": \"pass\"}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_LN4zeRvJ4l"
      },
      "outputs": [],
      "source": [
        "# cell 0.5 — foundations: offset-preserving normalization (CRLF fix + maps)\n",
        "from typing import NamedTuple, List\n",
        "import unicodedata\n",
        "\n",
        "class NormResult(NamedTuple):\n",
        "    text: str\n",
        "    norm_to_orig: List[int]\n",
        "    orig_to_norm: List[int]\n",
        "\n",
        "def normalize_with_offsets(s: str) -> NormResult:\n",
        "    \"\"\"\n",
        "    Normalize text while preserving a bidirectional char index map:\n",
        "      - CRLF -> LF, lone CR -> LF\n",
        "      - Unicode NFKC\n",
        "      - Unicode line/para separators -> LF\n",
        "      - Collapse runs of whitespace to a single space (but keep LF)\n",
        "    Returns:\n",
        "      text: normalized string\n",
        "      norm_to_orig[i] = original index for normalized char i\n",
        "      orig_to_norm[j] = nearest normalized index corresponding to original j (left-fill)\n",
        "    \"\"\"\n",
        "    norm_to_orig: List[int] = []\n",
        "    out_chars: List[str] = []\n",
        "    prev_space = False\n",
        "    j = 0\n",
        "    L = len(s)\n",
        "\n",
        "    while j < L:\n",
        "        ch = s[j]\n",
        "        # Handle CR/LF combinations\n",
        "        if ch == \"\\r\":\n",
        "            if j + 1 < L and s[j + 1] == \"\\n\":\n",
        "                out_chars.append(\"\\n\")\n",
        "                norm_to_orig.append(j)  # map LF to CR position\n",
        "                prev_space = False\n",
        "                j += 2\n",
        "                continue\n",
        "            else:\n",
        "                ch_n = \"\\n\"\n",
        "        else:\n",
        "            ch_n = unicodedata.normalize(\"NFKC\", ch)\n",
        "            if ch_n in (\"\\u2028\", \"\\u2029\"):\n",
        "                ch_n = \"\\n\"\n",
        "\n",
        "        # Collapse whitespace (except keep newlines)\n",
        "        if ch_n.isspace() and ch_n != \"\\n\":\n",
        "            if prev_space:\n",
        "                j += 1\n",
        "                continue\n",
        "            ch_n = \" \"\n",
        "            prev_space = True\n",
        "        else:\n",
        "            prev_space = (ch_n == \" \")\n",
        "\n",
        "        out_chars.append(ch_n)\n",
        "        norm_to_orig.append(j)\n",
        "        j += 1\n",
        "\n",
        "    text = \"\".join(out_chars)\n",
        "\n",
        "    # Build orig->norm with left fill for unmapped positions\n",
        "    orig_to_norm: List[int] = [-1] * L\n",
        "    for norm_i, orig_j in enumerate(norm_to_orig):\n",
        "        if 0 <= orig_j < L:\n",
        "            orig_to_norm[orig_j] = norm_i\n",
        "    last = 0\n",
        "    for k in range(L):\n",
        "        if orig_to_norm[k] == -1:\n",
        "            orig_to_norm[k] = last\n",
        "        else:\n",
        "            last = orig_to_norm[k]\n",
        "\n",
        "    return NormResult(text=text, norm_to_orig=norm_to_norm, orig_to_norm=orig_to_norm)\n",
        "\n",
        "# sanity self-report\n",
        "try:\n",
        "    demo = \"A  test\\u00A0string\\r\\nwith\\rmixed  spaces.\\nNext line.\"\n",
        "    res = normalize_with_offsets(demo)\n",
        "    assert \"\\r\" not in res.text and \"\\r\\n\" not in res.text, \"CR/CRLF not fully normalized\"\n",
        "    if \"report_status\" in globals():\n",
        "        report_status(\"0.foundation.normalize\", True, \"Normalization OK\", {\"len\": len(res.text)})\n",
        "    else:\n",
        "        print({\"module\": \"0.foundation.normalize\", \"ok\": True, \"note\": \"Normalization OK\", \"len\": len(res.text)})\n",
        "except Exception as e:\n",
        "    if \"report_status\" in globals():\n",
        "        report_status(\"0.foundation.normalize\", False, f\"Init error: {e}\")\n",
        "    else:\n",
        "        print({\"module\": \"0.foundation.normalize\", \"ok\": False, \"note\": f\"Init error: {e}\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u49mIDpuvlUQ"
      },
      "outputs": [],
      "source": [
        "# cell 0.6 — foundations: sentence segmentation & windowing (regex heuristic)\n",
        "\n",
        "import re\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "# Regex: split on . ! ? when followed by whitespace + capital/digit,\n",
        "# but DO NOT split after single initials (A.), multi-initials (U.S.),\n",
        "# or common abbreviations (Dr., Mr., e.g., i.e., etc., vs., Prof., Sr., Jr., St., Inc., Ltd.).\n",
        "SENT_SPLIT_RE = re.compile(\n",
        "    r\"\"\"\n",
        "    (?<!\\b[A-Z]\\.)                                  # not single-letter initial before period\n",
        "    (?<!\\b(?:[A-Z]\\.){2,})                          # not multi-initials like U.S.\n",
        "    (?<!\\b(?:Mr|Mrs|Ms|Dr|Prof|Sr|Jr|St|vs|etc|     # not common abbrevs (period included below)\n",
        "         e\\.g|i\\.e|Inc|Ltd))\\.\n",
        "    (?<=[.!?])\\s+(?=[A-Z0-9])                       # end mark + space(s) + capital/digit\n",
        "    \"\"\",\n",
        "    re.VERBOSE,\n",
        ")\n",
        "\n",
        "def split_sentences(text: str) -> List[Tuple[str, Tuple[int, int]]]:\n",
        "    \"\"\"\n",
        "    Return [(sentence_text, (char_start, char_end)), ...]\n",
        "    Uses a lightweight regex heuristic; newline handling is left to upstream normalization.\n",
        "    \"\"\"\n",
        "    spans: List[Tuple[int, int]] = []\n",
        "    start = 0\n",
        "    for m in SENT_SPLIT_RE.finditer(text):\n",
        "        # m is at the gap between sentences; previous end is the last punctuation char\n",
        "        end = m.start() + 1\n",
        "        if end > start:\n",
        "            spans.append((start, end))\n",
        "        start = m.end()\n",
        "    if start < len(text):\n",
        "        spans.append((start, len(text)))\n",
        "    return [(text[a:b], (a, b)) for a, b in spans if b > a]\n",
        "\n",
        "def window_sentences(\n",
        "    sents: List[Tuple[str, Tuple[int, int]]],\n",
        "    win: int,\n",
        "    stride: int\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Fixed-size windows over sentence list.\n",
        "    Returns a list of dicts with:\n",
        "      - sent_start_idx, sent_end_idx (end-exclusive)\n",
        "      - char_span = [char_start, char_end] from original text\n",
        "      - text = joined sentence text (space-joined, stripped)\n",
        "    No trailing partial window is emitted.\n",
        "    \"\"\"\n",
        "    if win <= 0 or stride <= 0:\n",
        "        raise ValueError(\"win and stride must be positive integers\")\n",
        "\n",
        "    windows: List[Dict[str, Any]] = []\n",
        "    n = len(sents)\n",
        "    max_start = max(n - win + 1, 0)\n",
        "    for i in range(0, max_start, stride):\n",
        "        chunk = sents[i:i + win]\n",
        "        if len(chunk) < win:\n",
        "            break\n",
        "        text = \" \".join(t.strip() for t, _ in chunk if t)\n",
        "        char_start = chunk[0][1][0]\n",
        "        char_end = chunk[-1][1][1]\n",
        "        windows.append({\n",
        "            \"sent_start_idx\": i,\n",
        "            \"sent_end_idx\": i + win,\n",
        "            \"char_span\": [char_start, char_end],\n",
        "            \"text\": text,\n",
        "        })\n",
        "    return windows\n",
        "\n",
        "# sanity self-report\n",
        "try:\n",
        "    sample = \"Dr. A. Smith wrote this. Second sentence! Third sentence? U.S. officials agreed. Fourth one. Fifth here.\"\n",
        "    sents = split_sentences(sample)\n",
        "    wins = window_sentences(sents, 4, 2)\n",
        "    payload = {\"sents\": len(sents), \"windows\": len(wins)}\n",
        "    if \"report_status\" in globals():\n",
        "        report_status(\"0.foundation.segmentation\", True, \"Splitter/windowing OK\", payload)\n",
        "    else:\n",
        "        print({\"module\": \"0.foundation.segmentation\", \"ok\": True, \"note\": \"Splitter/windowing OK\", **payload})\n",
        "except Exception as e:\n",
        "    if \"report_status\" in globals():\n",
        "        report_status(\"0.foundation.segmentation\", False, f\"Error: {e}\")\n",
        "    else:\n",
        "        print({\"module\": \"0.foundation.segmentation\", \"ok\": False, \"note\": f\"Error: {e}\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qNtVT1dv5z4"
      },
      "outputs": [],
      "source": [
        "# cell 0.7 — foundations: visualization smoke test (matplotlib only)\n",
        "import json, datetime\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt  # local import by design\n",
        "\n",
        "out_dir = Path(\"outputs/diagnostics\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "ts = datetime.datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
        "fp = out_dir / f\"viz_smoke_{ts}.png\"\n",
        "\n",
        "try:\n",
        "    xs = np.arange(0, 10)\n",
        "    ys = np.sqrt(xs)\n",
        "\n",
        "    plt.figure(figsize=(4, 3), dpi=120)\n",
        "    plt.plot(xs, ys, marker=\"o\")\n",
        "    plt.title(\"Matplotlib render check (0.7)\")\n",
        "    plt.xlabel(\"x\"); plt.ylabel(\"sqrt(x)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fp)\n",
        "    plt.close(\"all\")\n",
        "\n",
        "    assert fp.exists() and fp.stat().st_size > 0, \"Smoke image was not written\"\n",
        "    # status hook from 0.4\n",
        "    report_status(\"0.foundation.viz\", True, \"Matplotlib rendering OK\", {\"file\": str(fp), \"bytes\": fp.stat().st_size})\n",
        "    print(json.dumps({\"cell_id\": \"0.7 foundations: viz smoke\", \"file\": str(fp)}))\n",
        "except Exception as e:\n",
        "    plt.close(\"all\")\n",
        "    report_status(\"0.foundation.viz\", False, f\"Matplotlib failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.8 — FS scaffold + disk sanity (one-time)\n",
        "import os, shutil, json, datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Create a predictable outputs layout for all modules up-front\n",
        "BASE = Path(\"outputs\")\n",
        "LAYOUT = {f\"module-{i}\": [\"data\", \"plots\"] for i in range(1, 13)}\n",
        "LAYOUT.update({\"bundles\": [], \"diagnostics\": []})\n",
        "\n",
        "for mod, subs in LAYOUT.items():\n",
        "    if subs:\n",
        "        for s in subs:\n",
        "            (BASE / mod / s).mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "        (BASE / mod).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Light disk report at the current working dir mount\n",
        "root = Path(\".\").resolve()\n",
        "total, used, free = shutil.disk_usage(str(root))\n",
        "def _fmt(b):\n",
        "    for u in [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]:\n",
        "        if b < 1024: return f\"{b:,.1f} {u}\"\n",
        "        b /= 1024\n",
        "    return f\"{b:,.1f} PB\"\n",
        "\n",
        "stamp = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
        "summary = {\n",
        "    \"cwd\": str(root),\n",
        "    \"created_dirs\": [str((BASE / k)) for k in LAYOUT.keys()],\n",
        "    \"disk\": {\"total\": _fmt(total), \"used\": _fmt(used), \"free\": _fmt(free)},\n",
        "    \"ts\": stamp,\n",
        "}\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "# status hook (from 0.4)\n",
        "report_status(\"0.foundation.fs\", True, \"FS scaffold ready\", summary[\"disk\"])\n"
      ],
      "metadata": {
        "id": "MCyLlA2pde5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.9 — Compact Parquet I/O helpers (space + schema guards)\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: use pyarrow if present for zstd compression\n",
        "try:\n",
        "    import pyarrow as pa\n",
        "    import pyarrow.parquet as pq\n",
        "    _HAVE_PA = True\n",
        "except Exception:\n",
        "    _HAVE_PA = False\n",
        "\n",
        "def _downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Downcast numerics to save disk without changing semantics.\"\"\"\n",
        "    out = df.copy()\n",
        "    for c in out.columns:\n",
        "        s = out[c]\n",
        "        if pd.api.types.is_float_dtype(s):\n",
        "            out[c] = s.astype(\"float32\")\n",
        "        elif pd.api.types.is_integer_dtype(s):\n",
        "            # Nullable Int32 to preserve NaNs if present\n",
        "            out[c] = s.astype(\"Int32\" if s.isna().any() else \"int32\")\n",
        "        elif pd.api.types.is_bool_dtype(s):\n",
        "            out[c] = s.astype(\"bool\")\n",
        "        # strings/categoricals left as-is; caller may choose to cast\n",
        "    return out\n",
        "\n",
        "def assert_has_cols(df: pd.DataFrame, required: set, where: str = \"\"):\n",
        "    missing = set(required) - set(df.columns)\n",
        "    assert not missing, f\"Missing columns{f' in {where}' if where else ''}: {sorted(missing)}\"\n",
        "\n",
        "def write_parquet(df: pd.DataFrame, path: str | Path, compression: str = \"zstd\") -> dict:\n",
        "    \"\"\"Write compact parquet; falls back to pandas engine if pyarrow unavailable.\"\"\"\n",
        "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df2 = _downcast_numeric(df)\n",
        "    if _HAVE_PA:\n",
        "        table = pa.Table.from_pandas(df2, preserve_index=False)\n",
        "        pq.write_table(table, path, compression=compression)\n",
        "    else:\n",
        "        df2.to_parquet(path, index=False)  # engine=\"auto\" fallback\n",
        "    meta = {\"rows\": int(len(df2)), \"cols\": list(map(str, df2.columns)), \"bytes\": int(path.stat().st_size)}\n",
        "    return meta\n",
        "\n",
        "def read_parquet(path: str | Path) -> pd.DataFrame:\n",
        "    \"\"\"Read parquet via pandas (auto-engine).\"\"\"\n",
        "    return pd.read_parquet(path)\n",
        "\n",
        "print(json.dumps({\"cell_id\": \"0.9 foundations: parquet_io\", \"pyarrow\": _HAVE_PA}))\n",
        "report_status(\"0.foundation.io\", True, \"Parquet helpers ready\", {\"pyarrow\": _HAVE_PA})\n"
      ],
      "metadata": {
        "id": "snA4QWBOeE02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.10 — Resilience toggles + resource guard\n",
        "import os, json, shutil, datetime, re\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- Defaults (overridable via env before running the notebook) ----------\n",
        "def _setdef(k, v):\n",
        "    if os.environ.get(k) is None:\n",
        "        os.environ[k] = str(v)\n",
        "\n",
        "_setdef(\"LSA_FULL_STACK\",              0)   # heavy deps only when needed\n",
        "_setdef(\"LSA_MIN_FREE_GB\",             3)   # degrade if disk below this\n",
        "_setdef(\"LSA_MIN_RAM_GB\",              4)   # degrade if free RAM below this\n",
        "_setdef(\"LSA_SAVE_EMB\",                0)   # M6: don't write *.npy by default\n",
        "_setdef(\"LSA_INLINE_PLOTS\",            0)   # don't display inline by default\n",
        "_setdef(\"LSA_SAVE_PLOTS\",              1)   # write PNGs unless degraded\n",
        "_setdef(\"SAFE_MODE\",                   1)   # M7: skip heavy sentence tables\n",
        "_setdef(\"LSA_RF_MAX_CANDS_PER_WIN\",  300)  # M7: cap candidate count per window\n",
        "_setdef(\"LSA_ALLOW_SOURCE\",            0)   # 0 = wheel-only installs in 0.11\n",
        "\n",
        "# If inline plots are off, force a non-interactive backend early\n",
        "if os.environ.get(\"LSA_INLINE_PLOTS\", \"0\") == \"0\":\n",
        "    os.environ[\"MPLBACKEND\"] = \"Agg\"\n",
        "\n",
        "# ---------- Resource probes ----------\n",
        "def _disk_free_gb(path=\".\"):\n",
        "    total, used, free = shutil.disk_usage(str(Path(path).resolve()))\n",
        "    return round(free / (1024**3), 2)\n",
        "\n",
        "def _free_ram_gb():\n",
        "    try:\n",
        "        import psutil\n",
        "        return round(psutil.virtual_memory().available / (1024**3), 2)\n",
        "    except Exception:\n",
        "        try:\n",
        "            txt = Path(\"/proc/meminfo\").read_text()\n",
        "            m = re.search(r\"MemAvailable:\\s+(\\d+)\\s+kB\", txt)\n",
        "            if m: return round(int(m.group(1)) / (1024**2), 2)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return 0.0\n",
        "\n",
        "free_gb  = _disk_free_gb(\".\")\n",
        "ram_gb   = _free_ram_gb()\n",
        "need_gb  = float(os.environ[\"LSA_MIN_FREE_GB\"])\n",
        "need_ram = float(os.environ[\"LSA_MIN_RAM_GB\"])\n",
        "\n",
        "degraded = int(free_gb < need_gb or ram_gb < need_ram)\n",
        "os.environ[\"LSA_DEGRADED\"] = str(degraded)\n",
        "\n",
        "# If disk is tight, auto-disable plot saving to conserve space\n",
        "if degraded and free_gb < need_gb:\n",
        "    os.environ[\"LSA_SAVE_PLOTS\"] = \"0\"\n",
        "\n",
        "status = {\n",
        "    \"cell_id\": \"0.10_resilience_guard\",\n",
        "    \"thresholds\": {\"min_free_gb\": need_gb, \"min_ram_gb\": need_ram},\n",
        "    \"observed\": {\"free_gb\": free_gb, \"free_ram_gb\": ram_gb},\n",
        "    \"flags\": {\n",
        "        \"LSA_DEGRADED\": degraded,\n",
        "        \"LSA_FULL_STACK\": int(os.environ[\"LSA_FULL_STACK\"]),\n",
        "        \"LSA_SAVE_EMB\": int(os.environ[\"LSA_SAVE_EMB\"]),\n",
        "        \"LSA_INLINE_PLOTS\": int(os.environ[\"LSA_INLINE_PLOTS\"]),\n",
        "        \"LSA_SAVE_PLOTS\": int(os.environ[\"LSA_SAVE_PLOTS\"]),\n",
        "        \"SAFE_MODE\": int(os.environ[\"SAFE_MODE\"]),\n",
        "        \"LSA_RF_MAX_CANDS_PER_WIN\": int(os.environ[\"LSA_RF_MAX_CANDS_PER_WIN\"]),\n",
        "        \"LSA_ALLOW_SOURCE\": int(os.environ[\"LSA_ALLOW_SOURCE\"]),\n",
        "    },\n",
        "    \"ts\": datetime.datetime.utcnow().isoformat()+\"Z\",\n",
        "}\n",
        "print(json.dumps(status, indent=2))\n",
        "\n",
        "# Persist a run-env snapshot for post-hoc debugging\n",
        "env_dir = Path(\"outputs/_env\"); env_dir.mkdir(parents=True, exist_ok=True)\n",
        "(Path(env_dir) / \"_run_env.json\").write_text(json.dumps(status, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "# Soft-report to the notebook ledger if present\n",
        "try:\n",
        "    report_status(\"0.foundation.resilience\", True, \"Resilience toggles applied\", status)\n",
        "except NameError:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "tp5mOCffDFRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.11 — Lazy install helper (wheel-first, retry once, proper import mapping)\n",
        "import os, re, importlib, subprocess, sys, json\n",
        "\n",
        "# Map pip names → import module names\n",
        "NAME_MAP = {\n",
        "    \"scikit-learn\": \"sklearn\",\n",
        "    \"sentence-transformers\": \"sentence_transformers\",\n",
        "    \"umap-learn\": \"umap\",\n",
        "    # common cases that already match kept implicit: numpy, pandas, pyarrow, hdbscan, rapidfuzz, nltk, matplotlib, scipy\n",
        "}\n",
        "\n",
        "def _import_name_from_spec(spec: str) -> str:\n",
        "    base = spec.split(\"[\")[0]            # strip extras\n",
        "    base = re.split(r\"[<>=]\", base)[0]   # strip version ops\n",
        "    base = base.strip()\n",
        "    return NAME_MAP.get(base, base.replace(\"-\", \"_\"))\n",
        "\n",
        "def ensure_installed(reqs, quiet=True, no_cache=True, wheel_only=None):\n",
        "    \"\"\"\n",
        "    Install only if missing. Returns {\"installed\":[...], \"skipped\":[...], \"errors\":[...]}.\n",
        "    wheel_only:\n",
        "      - None → infer from env LSA_ALLOW_SOURCE (0 = wheel-only, 1 = allow source)\n",
        "      - True → force '--only-binary=:all:'\n",
        "      - False → allow source builds\n",
        "    \"\"\"\n",
        "    if wheel_only is None:\n",
        "        wheel_only = (os.environ.get(\"LSA_ALLOW_SOURCE\", \"0\") != \"1\")\n",
        "\n",
        "    installed, skipped, errors = [], [], []\n",
        "\n",
        "    for spec in reqs:\n",
        "        modname = _import_name_from_spec(spec)\n",
        "        try:\n",
        "            importlib.import_module(modname)\n",
        "            skipped.append(spec)\n",
        "            continue\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", spec]\n",
        "        if quiet: cmd.append(\"-q\")\n",
        "        if no_cache: cmd.append(\"--no-cache-dir\")\n",
        "        cmd.append(\"--prefer-binary\")\n",
        "        if wheel_only: cmd.append(\"--only-binary=:all:\")\n",
        "\n",
        "        env = os.environ.copy()\n",
        "        print(json.dumps({\"pip\": \"install\", \"spec\": spec, \"cmd\": \" \".join(cmd)}))\n",
        "\n",
        "        # try once, then one retry on failure\n",
        "        try:\n",
        "            subprocess.check_call(cmd, env=env)\n",
        "            installed.append(spec)\n",
        "        except subprocess.CalledProcessError as e1:\n",
        "            print(json.dumps({\"pip\": \"retry\", \"spec\": spec}))\n",
        "            try:\n",
        "                subprocess.check_call(cmd, env=env)\n",
        "                installed.append(spec)\n",
        "            except subprocess.CalledProcessError as e2:\n",
        "                errors.append({\"spec\": spec, \"returncode\": e2.returncode})\n",
        "\n",
        "    out = {\"installed\": installed, \"skipped\": skipped, \"errors\": errors}\n",
        "    print(json.dumps(out, indent=2))\n",
        "    try:\n",
        "        report_status(\"0.foundation.installer\", True if not errors else False, \"ensure_installed completed\", out)\n",
        "    except NameError:\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "print(json.dumps({\"cell_id\": \"0.11_lazy_install_helper\", \"usage\": \"ensure_installed([...]) in module install cells\"}))\n"
      ],
      "metadata": {
        "id": "0VL7KYQhDJs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm1rUVnpMVOb"
      },
      "outputs": [],
      "source": [
        "# cell 1.0A: set SOURCE_DIR to your article folder (expects 01-*.md .. 04-*.md)\n",
        "from pathlib import Path\n",
        "SOURCE_DIR = Path(\"/content\")  # adjust if needed\n",
        "print({\"cell_id\": \"cell 1.0A: set SOURCE_DIR\",\n",
        "       \"SOURCE_DIR\": str(SOURCE_DIR),\n",
        "       \"exists\": SOURCE_DIR.exists(),\n",
        "       \"is_dir\": SOURCE_DIR.is_dir()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnTsGamAMWYf"
      },
      "outputs": [],
      "source": [
        "# cell 1.0B: filename sanity (previews matches and version coverage by slug)\n",
        "import re, json\n",
        "from pathlib import Path\n",
        "\n",
        "FNAME_RE = re.compile(r\"^(?P<prefix>0[1-4])-(?P<slug>.+)\\.md$\")\n",
        "if not SOURCE_DIR.exists() or not SOURCE_DIR.is_dir():\n",
        "    raise ValueError(f\"SOURCE_DIR not found or not a directory: {SOURCE_DIR}\")\n",
        "\n",
        "files = [p for p in SOURCE_DIR.iterdir() if p.is_file() and FNAME_RE.match(p.name)]\n",
        "preview = [p.name for p in files[:10]]\n",
        "groups = {}\n",
        "for p in files:\n",
        "    m = FNAME_RE.match(p.name)\n",
        "    slug = m.group(\"slug\")\n",
        "    vid = int(m.group(\"prefix\"))\n",
        "    groups.setdefault(slug, []).append(vid)\n",
        "\n",
        "summary = {slug: sorted(vs) for slug, vs in groups.items()}\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"cell 1.0B: filename sanity\",\n",
        "    \"SOURCE_DIR\": str(SOURCE_DIR),\n",
        "    \"matches\": len(files),\n",
        "    \"preview\": preview,\n",
        "    \"articles\": len(summary),\n",
        "    \"versions_by_article\": summary\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IygMdpH-osOh"
      },
      "outputs": [],
      "source": [
        "# cell 1.1: textstat|wordfreq: install (module-only; no base upgrades; pinned, wheels-only)\n",
        "# Ensure pyphen is installed before textstat imports.\n",
        "import importlib, sys, subprocess\n",
        "\n",
        "def _ensure_pkg(import_name: str, pip_name: str = None, version: str = None):\n",
        "    try:\n",
        "        return importlib.import_module(import_name)\n",
        "    except ModuleNotFoundError:\n",
        "        pkg_spec = pip_name or import_name\n",
        "        if version:\n",
        "            pkg_spec = f\"{pkg_spec}{version}\"\n",
        "        print(f\"[1.1 hotfix] Installing missing dependency: {pkg_spec} …\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg_spec])\n",
        "        return importlib.import_module(import_name)\n",
        "\n",
        "_ensure_pkg(\"pyphen\", \"pyphen\", \">=0.14,<0.15\")\n",
        "\n",
        "# Install main packages and their dependencies\n",
        "%pip install -q --only-binary=:all: \\\n",
        "  \"textstat>=0.7,<0.8\" \\\n",
        "  \"wordfreq>=3,<4\" \\\n",
        "  \"regex>=2023.10,<2026.0\" \\\n",
        "  \"pyarrow>=14,<18\" \\\n",
        "  \"ftfy>=6,<7\" \\\n",
        "  \"langcodes>=3,<4\"\n",
        "\n",
        "from pathlib import Path, PurePosixPath\n",
        "import json\n",
        "\n",
        "BASE_OUT = Path(\"outputs/textstat_lex\")\n",
        "PLOTS_OUT = BASE_OUT / \"plots\"\n",
        "BASE_OUT.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS_OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"cell 1.1: textstat|wordfreq: install\",\n",
        "    \"status\": \"pass\",\n",
        "    \"dirs\": {\"base\": str(PurePosixPath(BASE_OUT)), \"plots\": str(PurePosixPath(PLOTS_OUT))}\n",
        "}, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmuYlLpqRzyX"
      },
      "outputs": [],
      "source": [
        "lazy_import_ml()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An1C44KUovRd"
      },
      "outputs": [],
      "source": [
        "# cell 1.2: textstat|wordfreq: imports & sanity checks + filename-driven versioning\n",
        "import json, datetime, math, random, re\n",
        "from pathlib import Path\n",
        "\n",
        "# Environment & Imports (hotfix: ensure pyphen for textstat)\n",
        "import importlib, sys, subprocess\n",
        "\n",
        "def _ensure_pkg(import_name: str, pip_name: str = None, version: str = None):\n",
        "    try:\n",
        "        return importlib.import_module(import_name)\n",
        "    except ModuleNotFoundError:\n",
        "        pkg_spec = pip_name or import_name\n",
        "        if version:\n",
        "            pkg_spec = f\"{pkg_spec}=={version}\"\n",
        "        print(f\"[1.2 hotfix] Installing missing dependency: {pkg_spec} …\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg_spec])\n",
        "        return importlib.import_module(import_name)\n",
        "\n",
        "# textstat requires 'pyphen' at import time for hyphenation/syllables.\n",
        "_ensure_pkg(\"pyphen\", \"pyphen\", \"0.14.0\")\n",
        "\n",
        "# Now import textstat (works if pyphen is present)\n",
        "import textstat\n",
        "from importlib.metadata import version as _pkg_version\n",
        "\n",
        "print(\"[1.2] textstat:\", _pkg_version(\"textstat\"), \"| pyphen:\", _pkg_version(\"pyphen\"))\n",
        "\n",
        "\n",
        "import textstat\n",
        "from importlib.metadata import version as _pkg_version\n",
        "\n",
        "# Assert foundations (avoid base re-imports here)\n",
        "_missing = [name for name in (\"np\",\"pd\",\"plt\") if name not in globals()]\n",
        "if _missing:\n",
        "    raise RuntimeError(f\"Foundations not loaded (missing {_missing}). Run cell 0.2 before Module 1.\")\n",
        "\n",
        "# ---- Versions & determinism ----\n",
        "VERS = {\"textstat\": _pkg_version(\"textstat\"), \"wordfreq\": _pkg_version(\"wordfreq\")}\n",
        "\n",
        "# ---- Inputs handshake ----\n",
        "g = globals()\n",
        "DOCS = g.get(\"DOCS\"); TEXT = g.get(\"TEXT\")\n",
        "\n",
        "# If neither DOCS nor TEXT is provided, load from SOURCE_DIR using strict filename rule.\n",
        "SOURCE_DIR = Path(g.get(\"SOURCE_DIR\", \"content\"))\n",
        "\n",
        "FNAME_RE = re.compile(r\"^(?P<prefix>0[1-4])-(?P<slug>.+)\\.md$\")\n",
        "NOTES = []\n",
        "\n",
        "def _clean_markdown(s: str) -> str:\n",
        "    \"\"\"Regex-only light cleaner for readability/lexical metrics.\"\"\"\n",
        "    if not s: return \"\"\n",
        "    s = re.sub(r\"```[\\s\\S]*?```\", \" \", s)                            # fenced code blocks\n",
        "    s = re.sub(r\"!\\[([^\\]]*)\\]\\([^)]+\\)\", r\"\\1\", s)                  # images -> alt\n",
        "    s = re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", r\"\\1\", s)                   # links -> text\n",
        "    s = re.sub(r\"`([^`]+)`\", r\"\\1\", s)                               # inline code\n",
        "    s = re.sub(r\"^\\s*#{1,6}\\s+\", \"\", s, flags=re.MULTILINE)          # headings\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)                                    # spaces\n",
        "    s = re.sub(r\"\\s*\\n\\s*\", \"\\n\", s).strip()                         # newlines\n",
        "    return s\n",
        "\n",
        "if DOCS is None and TEXT is not None:\n",
        "    DOCS = [{\"doc_id\": \"doc_0001\", \"text\": TEXT, \"article_id\": \"doc_0001\", \"version_id\": 1,\n",
        "             \"version_tag\": \"v1\", \"filename\": \"<in-memory>\", \"path\": \"<in-memory>\"}]\n",
        "elif DOCS is None and TEXT is None:\n",
        "    # Filename-driven discovery\n",
        "    if not SOURCE_DIR.exists() or not SOURCE_DIR.is_dir():\n",
        "        raise ValueError(f\"Module 1: SOURCE_DIR does not exist or is not a directory: {SOURCE_DIR}\")\n",
        "    files = [p for p in SOURCE_DIR.iterdir() if p.is_file() and FNAME_RE.match(p.name)]\n",
        "    if not files:\n",
        "        raise ValueError(\"Module 1: No files found matching ^(0[1-4])-(.+)\\\\.md$ in SOURCE_DIR.\")\n",
        "    groups = {}\n",
        "    for p in files:\n",
        "        m = FNAME_RE.match(p.name)\n",
        "        prefix, slug = m.group(\"prefix\"), m.group(\"slug\")\n",
        "        vid = int(prefix)\n",
        "        grp = groups.setdefault(slug, {})\n",
        "        if vid in grp:\n",
        "            raise ValueError(f\"Duplicate version prefix for article '{slug}': {prefix}\")\n",
        "        grp[vid] = p\n",
        "    DOCS = []\n",
        "    for slug, versions in sorted(groups.items()):\n",
        "        present = sorted(versions.keys())\n",
        "        if len(present) < 4:\n",
        "            NOTES.append(f\"Article '{slug}' missing versions: {sorted(set(range(1,5)) - set(present))}\")\n",
        "        for vid in present:\n",
        "            path = versions[vid]\n",
        "            text_raw = path.read_text(encoding=\"utf-8-sig\", errors=\"replace\")\n",
        "            text_clean = _clean_markdown(text_raw)\n",
        "            DOCS.append({\n",
        "                \"article_id\": slug,\n",
        "                \"version_id\": vid,\n",
        "                \"version_tag\": f\"v{vid}\",\n",
        "                \"filename\": path.name,\n",
        "                \"path\": str(path),\n",
        "                \"doc_id\": f\"{slug}::v{vid}\",\n",
        "                \"text_raw\": text_raw,\n",
        "                \"text\": text_clean,   # downstream cells use 'text'\n",
        "            })\n",
        "else:\n",
        "    # DOCS provided by user: validate shape\n",
        "    if not (isinstance(DOCS, list) and all(isinstance(d, dict) for d in DOCS)):\n",
        "        raise ValueError(\"DOCS must be a list of dicts.\")\n",
        "    if not all((\"doc_id\" in d and \"text\" in d) for d in DOCS):\n",
        "        raise ValueError(\"DOCS dicts must include 'doc_id' and 'text'.\")\n",
        "\n",
        "# ---- Metadata ----\n",
        "BASE_OUT = Path(\"outputs/textstat_lex\"); PLOTS_OUT = BASE_OUT / \"plots\"\n",
        "BASE_OUT.mkdir(parents=True, exist_ok=True); PLOTS_OUT.mkdir(parents=True, exist_ok=True)\n",
        "metadata_path = BASE_OUT / \"metadata.json\"\n",
        "\n",
        "articles = {}\n",
        "for d in DOCS:\n",
        "    slug = d.get(\"article_id\", d.get(\"doc_id\", \"<doc>\"))\n",
        "    vid = d.get(\"version_id\")\n",
        "    if slug not in articles: articles[slug] = []\n",
        "    if vid is not None and vid not in articles[slug]:\n",
        "        articles[slug].append(vid)\n",
        "for slug in articles:\n",
        "    articles[slug] = sorted(articles[slug])\n",
        "\n",
        "metadata = {\n",
        "    \"module\": \"module_1_textstat_lex_v1\",\n",
        "    \"timestamp_utc\": datetime.datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "    \"seed\": 7,\n",
        "    \"library_versions\": VERS,\n",
        "    \"input_count\": len(DOCS),\n",
        "    \"source_dir\": (\n",
        "        str(SOURCE_DIR.resolve())\n",
        "        if (TEXT is None and isinstance(DOCS, list) and len(DOCS) and isinstance(DOCS[0], dict) and \"path\" in DOCS[0])\n",
        "        else None\n",
        "    ),\n",
        "    \"articles\": len(articles) if articles else None,\n",
        "    \"versions_per_article_min\": min((len(v) for v in articles.values()), default=None),\n",
        "    \"versions_per_article_max\": max((len(v) for v in articles.values()), default=None),\n",
        "    \"expected_versions\": 4,\n",
        "    \"version_order_source\": \"filename_prefix\",\n",
        "    \"notes\": NOTES,\n",
        "}\n",
        "metadata_path.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"cell 1.2: textstat|wordfreq: imports & sanity checks\",\n",
        "    \"status\": \"pass\",\n",
        "    \"inputs\": len(DOCS),\n",
        "    \"articles\": len(articles) if articles else 1,\n",
        "    \"metadata\": str(metadata_path),\n",
        "    \"versions\": VERS\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNlwKT9HMZhR"
      },
      "outputs": [],
      "source": [
        "# cell 1.3: textstat: readability metrics (document-level on text_clean)\n",
        "problems = []\n",
        "rows = []\n",
        "\n",
        "# naive sentence split for eligibility check (not for final stats)\n",
        "SENT_SPLIT = re.compile(r'(?<=[.!?])\\s+')\n",
        "\n",
        "def _eligible(text: str) -> bool:\n",
        "    if text is None: return False\n",
        "    if len(text) < 200: return False\n",
        "    sents = [s for s in SENT_SPLIT.split(text.strip()) if s]\n",
        "    return len(sents) >= 3\n",
        "\n",
        "for d in DOCS:\n",
        "    doc_id = str(d.get(\"doc_id\", d.get(\"article_id\", \"doc\")) )\n",
        "    text = d.get(\"text\") or \"\"\n",
        "    row = {\n",
        "        \"doc_id\": doc_id,\n",
        "        \"article_id\": d.get(\"article_id\", None),\n",
        "        \"version_id\": d.get(\"version_id\", None),\n",
        "        \"version_tag\": d.get(\"version_tag\", None),\n",
        "        \"filename\": d.get(\"filename\", None),\n",
        "        \"path\": d.get(\"path\", None),\n",
        "        \"n_chars\": int(len(text)),\n",
        "    }\n",
        "    heavy_ok = _eligible(text)\n",
        "\n",
        "    metrics = {\n",
        "        \"flesch_reading_ease\": textstat.flesch_reading_ease,\n",
        "        \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade,\n",
        "        \"gunning_fog\": textstat.gunning_fog,\n",
        "        \"smog_index\": textstat.smog_index,\n",
        "        \"coleman_liau_index\": textstat.coleman_liau_index,\n",
        "        \"automated_readability_index\": textstat.automated_readability_index,\n",
        "        \"dale_chall_readability_score\": textstat.dale_chall_readability_score,\n",
        "        \"linsear_write_formula\": textstat.linsear_write_formula,\n",
        "    }\n",
        "\n",
        "    if heavy_ok:\n",
        "        for k, fn in metrics.items():\n",
        "            try:\n",
        "                row[k] = float(fn(text))\n",
        "            except Exception as e:\n",
        "                row[k] = np.nan\n",
        "                problems.append({\"doc_id\": doc_id, \"metric\": k, \"error\": str(e)})\n",
        "        try:\n",
        "            row[\"text_standard\"] = str(textstat.text_standard(text))\n",
        "        except Exception as e:\n",
        "            row[\"text_standard\"] = \"\"\n",
        "            problems.append({\"doc_id\": doc_id, \"metric\": \"text_standard\", \"error\": str(e)})\n",
        "    else:\n",
        "        for k in metrics.keys(): row[k] = np.nan\n",
        "        row[\"text_standard\"] = \"\"\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "df_lex = pd.DataFrame(rows)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"cell 1.3: textstat: readability metrics\",\n",
        "    \"status\": \"pass\", \"docs\": int(len(df_lex)), \"problems\": int(len(problems))\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvEppcwiMgMn"
      },
      "outputs": [],
      "source": [
        "# cell 1.4: wordfreq: Zipf frequency features (Unicode-aware, regex-only tokenization)\n",
        "import regex as rxx\n",
        "from wordfreq import zipf_frequency\n",
        "\n",
        "TOKEN_RE = rxx.compile(r\"\\b[\\p{L}\\p{M}\\p{N}’'-]+\\b\", flags=rxx.UNICODE)\n",
        "\n",
        "def tokenize(text: str):\n",
        "    return [m.group(0) for m in TOKEN_RE.finditer(text or \"\")]\n",
        "\n",
        "def token_zipf_stats(tokens):\n",
        "    lowers = [t.lower() for t in tokens]\n",
        "    freqs = [zipf_frequency(t, 'en', wordlist='best') for t in lowers]\n",
        "    freqs = [f for f in freqs if f is not None and f > 0]\n",
        "    if len(freqs) == 0:\n",
        "        return {\n",
        "            \"zipf_mean\": np.nan, \"zipf_std\": np.nan,\n",
        "            \"zipf_p25\": np.nan, \"zipf_p50\": np.nan, \"zipf_p75\": np.nan,\n",
        "            \"rare_rate\": np.nan, \"mid_rate\": np.nan, \"common_rate\": np.nan\n",
        "        }, []\n",
        "    arr = np.asarray(freqs, dtype=float)\n",
        "    stats = {\n",
        "        \"zipf_mean\": float(np.mean(arr)),\n",
        "        \"zipf_std\": float(np.std(arr, ddof=0)),\n",
        "        \"zipf_p25\": float(np.percentile(arr, 25)),\n",
        "        \"zipf_p50\": float(np.percentile(arr, 50)),\n",
        "        \"zipf_p75\": float(np.percentile(arr, 75)),\n",
        "        \"rare_rate\": float(np.mean(arr < 3.0)),\n",
        "        \"mid_rate\":  float(np.mean((arr >= 3.0) & (arr < 5.0))),\n",
        "        \"common_rate\": float(np.mean(arr >= 5.0)),\n",
        "    }\n",
        "    return stats, arr\n",
        "\n",
        "def latin_alpha_ratio(text: str) -> float:\n",
        "    if not text: return 0.0\n",
        "    all_alpha = [ch for ch in text if ch.isalpha()]\n",
        "    if not all_alpha: return 0.0\n",
        "    latin = [ch for ch in all_alpha if ('A' <= ch <= 'Z') or ('a' <= ch <= 'z')]\n",
        "    return len(latin) / len(all_alpha)\n",
        "\n",
        "ZIPF_ALL = []\n",
        "zipf_rows = []\n",
        "for idx, d in enumerate(DOCS):\n",
        "    doc_id, text = str(d.get(\"doc_id\", f\"doc_{idx:04d}\")), (d.get(\"text\") or \"\")\n",
        "    toks = tokenize(text)\n",
        "    stats, freqs = token_zipf_stats(toks)\n",
        "    zipf_rows.append({\"doc_id\": doc_id, **stats})\n",
        "    if len(freqs): ZIPF_ALL.append(np.asarray(freqs, dtype=float))\n",
        "\n",
        "df_zipf = pd.DataFrame(zipf_rows)\n",
        "df_lex = df_lex.merge(df_zipf, on=\"doc_id\", how=\"left\")\n",
        "\n",
        "# language heuristic update (non-blocking)\n",
        "meta_path = Path(\"outputs/textstat_lex/metadata.json\")\n",
        "try:\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "    ratios = [latin_alpha_ratio(d.get(\"text\") or \"\") for d in DOCS]\n",
        "    if len(ratios) and float(np.mean([r < 0.5 for r in ratios])) > 0.5:\n",
        "        meta[\"lang_guess\"] = \"non_en_possible\"\n",
        "    meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "ZIPF_ALL = np.concatenate(ZIPF_ALL, axis=0) if len(ZIPF_ALL) else np.array([], dtype=float)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"cell 1.4: wordfreq: Zipf frequency features\",\n",
        "    \"status\": \"pass\", \"docs\": int(len(df_zipf)), \"zipf_values\": int(ZIPF_ALL.size)\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAfcB189Mjd1"
      },
      "outputs": [],
      "source": [
        "# cell 1.5: lexicons: function-word profile, TTR, naive sentence stats + dtype enforcement\n",
        "from pathlib import Path\n",
        "\n",
        "# Reuse TOKEN_RE & tokenize from 1.4\n",
        "\n",
        "LEX_PATH = Path(\"lexicons/function_words_en.txt\")\n",
        "if LEX_PATH.exists():\n",
        "    FUNC_WORDS = {w.strip().lower() for w in LEX_PATH.read_text(encoding=\"utf-8\").splitlines() if w.strip()}\n",
        "else:\n",
        "    FUNC_WORDS = {\n",
        "        \"a\",\"an\",\"the\",\"and\",\"or\",\"but\",\"if\",\"because\",\"as\",\"until\",\"while\",\n",
        "        \"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\n",
        "        \"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\n",
        "        \"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\n",
        "        \"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\n",
        "        \"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\n",
        "        \"can\",\"will\",\"just\",\"don\",\"should\",\"now\",\"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\n",
        "        \"me\",\"him\",\"her\",\"us\",\"them\",\"my\",\"your\",\"his\",\"its\",\"our\",\"their\",\n",
        "        \"is\",\"am\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"do\",\"does\",\"did\"\n",
        "    }\n",
        "\n",
        "PUNCT_CHARS = set(list(\".,;:!?—–-()[]'\\\"“”‘’\"))\n",
        "SENT_SPLIT_FOR_STATS = rxx.compile(r\"(?<=[.!?])\\s+(?=[A-Z])\")\n",
        "\n",
        "SENT_LEN_ALL = []\n",
        "rows = []\n",
        "for idx, d in enumerate(DOCS):\n",
        "    doc_id, text = str(d.get(\"doc_id\", f\"doc_{idx:04d}\")), (d.get(\"text\") or \"\")\n",
        "    toks = tokenize(text)\n",
        "    n_tokens = int(len(toks))\n",
        "    lowers = [t.lower() for t in toks]\n",
        "    n_unique = len(set(lowers))\n",
        "    ttr = float(n_unique / n_tokens) if n_tokens > 0 else float(\"nan\")\n",
        "    fw_rate = float(sum(1 for t in lowers if t in FUNC_WORDS) / n_tokens) if n_tokens > 0 else float(\"nan\")\n",
        "    tok_lens = np.array([len(t) for t in toks], dtype=float) if n_tokens > 0 else np.array([], dtype=float)\n",
        "    token_len_mean = float(tok_lens.mean()) if tok_lens.size else float(\"nan\")\n",
        "    token_len_std  = float(tok_lens.std(ddof=0)) if tok_lens.size else float(\"nan\")\n",
        "    pden = (sum(1 for ch in (text or \"\") if ch in PUNCT_CHARS) / n_tokens * 100.0) if n_tokens > 0 else float(\"nan\")\n",
        "\n",
        "    sents = [s for s in SENT_SPLIT_FOR_STATS.split(text.strip()) if s]\n",
        "    sent_lens = [len(tokenize(s)) for s in sents] if sents else []\n",
        "    if sent_lens: SENT_LEN_ALL.extend(sent_lens)\n",
        "    naive_sentence_count = int(len(sents))\n",
        "    naive_sentence_len_mean = float(np.mean(sent_lens)) if sent_lens else float(\"nan\")\n",
        "    naive_sentence_len_std  = float(np.std(sent_lens, ddof=0)) if sent_lens else float(\"nan\")\n",
        "\n",
        "    rows.append({\n",
        "        \"doc_id\": doc_id,\n",
        "        \"n_tokens_regex\": n_tokens,\n",
        "        \"ttr\": ttr,\n",
        "        \"function_word_rate\": fw_rate,\n",
        "        \"punctuation_density_per_100toks\": pden,\n",
        "        \"token_len_mean\": token_len_mean,\n",
        "        \"token_len_std\": token_len_std,\n",
        "        \"naive_sentence_count\": naive_sentence_count,\n",
        "        \"naive_sentence_len_mean\": naive_sentence_len_mean,\n",
        "        \"naive_sentence_len_std\": naive_sentence_len_std,\n",
        "    })\n",
        "\n",
        "df_lex2 = pd.DataFrame(rows)\n",
        "df_lex = df_lex.merge(df_lex2, on=\"doc_id\", how=\"left\")\n",
        "\n",
        "# ---- dtype & required columns (includes filename-driven identifiers) ----\n",
        "required_cols = [\n",
        "    \"doc_id\",\"article_id\",\"version_id\",\"version_tag\",\"filename\",\"path\",\n",
        "    \"n_chars\",\"n_tokens_regex\",\"ttr\",\"function_word_rate\",\"punctuation_density_per_100toks\",\n",
        "    \"token_len_mean\",\"token_len_std\",\"zipf_mean\",\"zipf_std\",\"zipf_p25\",\"zipf_p50\",\"zipf_p75\",\n",
        "    \"rare_rate\",\"mid_rate\",\"common_rate\",\"flesch_reading_ease\",\"flesch_kincaid_grade\",\"gunning_fog\",\n",
        "    \"smog_index\",\"coleman_liau_index\",\"automated_readability_index\",\"dale_chall_readability_score\",\n",
        "    \"linsear_write_formula\",\"text_standard\",\"naive_sentence_count\",\"naive_sentence_len_mean\",\n",
        "    \"naive_sentence_len_std\"\n",
        "]\n",
        "for c in required_cols:\n",
        "    if c not in df_lex.columns:\n",
        "        df_lex[c] = np.nan\n",
        "\n",
        "# identifiers\n",
        "df_lex[\"doc_id\"] = df_lex[\"doc_id\"].astype(\"string\")\n",
        "df_lex[\"article_id\"] = df_lex[\"article_id\"].astype(\"string\")\n",
        "df_lex[\"version_tag\"] = df_lex[\"version_tag\"].astype(\"string\")\n",
        "df_lex[\"filename\"] = df_lex[\"filename\"].astype(\"string\")\n",
        "df_lex[\"path\"] = df_lex[\"path\"].astype(\"string\")\n",
        "df_lex[\"version_id\"] = pd.to_numeric(df_lex[\"version_id\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "\n",
        "# ints\n",
        "for c in [\"n_chars\",\"n_tokens_regex\",\"naive_sentence_count\"]:\n",
        "    df_lex[c] = pd.to_numeric(df_lex[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "\n",
        "# strings\n",
        "df_lex[\"text_standard\"] = df_lex[\"text_standard\"].astype(\"string\")\n",
        "\n",
        "# floats\n",
        "float_cols = [c for c in required_cols if c not in\n",
        "              [\"doc_id\",\"article_id\",\"version_id\",\"version_tag\",\"filename\",\"path\",\n",
        "               \"n_chars\",\"n_tokens_regex\",\"naive_sentence_count\",\"text_standard\"]]\n",
        "for c in float_cols:\n",
        "    df_lex[c] = pd.to_numeric(df_lex[c], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"cell 1.5: lexicons: function-word profile & TTR\",\n",
        "    \"status\": \"pass\", \"docs\": int(len(df_lex)), \"have_sent_len\": int(len(SENT_LEN_ALL))\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptVjTt7pMpCe"
      },
      "outputs": [],
      "source": [
        "# cell 1.6: textstat|wordfreq: visuals — baseline distributions + per-article trends + deltas\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "BASE_OUT = Path(\"outputs/textstat_lex\"); PLOTS_OUT = BASE_OUT / \"plots\"\n",
        "PLOTS_OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Assert foundations / prior cells present (no base re-imports here)\n",
        "_missing = [name for name in (\"np\",\"pd\",\"plt\",\"df_lex\") if name not in globals()]\n",
        "if _missing:\n",
        "    raise RuntimeError(f\"Prereqs missing in cell 1.6: {_missing}. Run 0.2 and cells 1.3–1.5 first.\")\n",
        "\n",
        "# Fallback tokenizer if 1.4 wasn't run\n",
        "try:\n",
        "    TOKEN_RE  # noqa: F821\n",
        "except NameError:\n",
        "    import regex as rxx\n",
        "    TOKEN_RE = rxx.compile(r\"\\b[\\p{L}\\p{M}\\p{N}’'-]+\\b\", flags=rxx.UNICODE)\n",
        "\n",
        "def _placeholder_plot(title, note, outfile):\n",
        "    plt.figure(dpi=120)\n",
        "    plt.text(0.5, 0.6, \"No data available\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    plt.text(0.5, 0.4, note, ha=\"center\", va=\"center\", fontsize=9)\n",
        "    plt.title(title); plt.axis(\"off\")\n",
        "    plt.tight_layout(); plt.savefig(outfile); plt.show(); plt.close()\n",
        "\n",
        "# ---- Global histograms ----\n",
        "# Sentence length histogram\n",
        "try:\n",
        "    sent_ok = False\n",
        "    if 'SENT_LEN_ALL' in globals() and len(SENT_LEN_ALL):\n",
        "        plt.figure(dpi=120)\n",
        "        bins = np.arange(0, max(SENT_LEN_ALL)+2)\n",
        "        plt.hist(SENT_LEN_ALL, bins=bins, edgecolor='black', linewidth=0.5)\n",
        "        plt.title(\"Sentence Lengths (regex tokens) — Module 1 baselines\")\n",
        "        plt.xlabel(\"Tokens per sentence\"); plt.ylabel(\"Count\")\n",
        "        plt.figtext(0.01, 0.01, \"Module 1 regex-based baselines; not syntactic tokenization\",\n",
        "                    ha=\"left\", fontsize=9)\n",
        "        out1 = PLOTS_OUT / \"sentence_length_hist.png\"\n",
        "        plt.tight_layout(); plt.savefig(out1); plt.show()\n",
        "        sent_ok = True\n",
        "    else:\n",
        "        out1 = PLOTS_OUT / \"sentence_length_hist.png\"\n",
        "        _placeholder_plot(\"Sentence Lengths — Module 1 baselines\",\n",
        "                          \"Regex tokens; produced as placeholder due to empty input.\",\n",
        "                          out1)\n",
        "except Exception as e:\n",
        "    print(json.dumps({\"plot\": \"sentence_length_hist\", \"error\": str(e)}))\n",
        "\n",
        "# Zipf frequency histogram\n",
        "try:\n",
        "    zipf_ok = False\n",
        "    if 'ZIPF_ALL' in globals() and getattr(ZIPF_ALL, \"size\", 0):\n",
        "        plt.figure(dpi=120)\n",
        "        bins = np.arange(1.0, 7.01, 0.25)\n",
        "        plt.hist(ZIPF_ALL, bins=bins, edgecolor='black', linewidth=0.5)\n",
        "        plt.title(\"Token Zipf Frequencies — Module 1 baselines\")\n",
        "        plt.xlabel(\"Zipf frequency (best list)\"); plt.ylabel(\"Count\")\n",
        "        plt.figtext(0.01, 0.01, \"Module 1 regex-based baselines; not syntactic tokenization\",\n",
        "                    ha=\"left\", fontsize=9)\n",
        "        out2 = PLOTS_OUT / \"zipf_hist.png\"\n",
        "        plt.tight_layout(); plt.savefig(out2); plt.show()\n",
        "        zipf_ok = True\n",
        "    else:\n",
        "        out2 = PLOTS_OUT / \"zipf_hist.png\"\n",
        "        _placeholder_plot(\"Token Zipf Frequencies — Module 1 baselines\",\n",
        "                          \"English word list; produced as placeholder due to empty input.\",\n",
        "                          out2)\n",
        "except Exception as e:\n",
        "    print(json.dumps({\"plot\": \"zipf_hist\", \"error\": str(e)}))\n",
        "\n",
        "# ---- Per-article visuals (up to first 10 slugs alphabetically) ----\n",
        "per_article_ok = True\n",
        "try:\n",
        "    if \"article_id\" in df_lex.columns and df_lex[\"article_id\"].notna().any():\n",
        "        for slug in sorted(df_lex[\"article_id\"].dropna().unique())[:10]:\n",
        "            sub = df_lex[df_lex[\"article_id\"] == slug].copy()\n",
        "            if \"version_id\" in sub.columns:\n",
        "                sub = sub.sort_values(\"version_id\")\n",
        "            # Trend: Flesch Reading Ease across versions\n",
        "            try:\n",
        "                plt.figure(dpi=120)\n",
        "                plt.plot(sub[\"version_id\"], sub[\"flesch_reading_ease\"], marker=\"o\")\n",
        "                plt.title(f\"Flesch Reading Ease — {slug} (v1..v4)\")\n",
        "                plt.xlabel(\"Version (from filename prefix)\"); plt.ylabel(\"Flesch Reading Ease\")\n",
        "                plt.xticks(sub[\"version_id\"])\n",
        "                outp = PLOTS_OUT / f\"trend_flesch_reading_ease_{slug}.png\"\n",
        "                plt.tight_layout(); plt.savefig(outp); plt.close()\n",
        "            except Exception as e:\n",
        "                per_article_ok = False\n",
        "                print(json.dumps({\"plot\": \"trend_flesch\", \"slug\": slug, \"error\": str(e)}))\n",
        "            # Stacked bars: Zipf bins per version\n",
        "            try:\n",
        "                plt.figure(dpi=120)\n",
        "                idx = sub[\"version_id\"].astype(int).to_numpy()\n",
        "                rare = sub[\"rare_rate\"].fillna(0).to_numpy()\n",
        "                mid  = sub[\"mid_rate\"].fillna(0).to_numpy()\n",
        "                com  = sub[\"common_rate\"].fillna(0).to_numpy()\n",
        "                plt.bar(idx, rare, label=\"rare <3.0\")\n",
        "                plt.bar(idx, mid, bottom=rare, label=\"mid 3–<5\")\n",
        "                plt.bar(idx, com, bottom=rare+mid, label=\"common ≥5\")\n",
        "                plt.title(f\"Zipf Bins — {slug} (v1..v4)\")\n",
        "                plt.xlabel(\"Version (from filename prefix)\"); plt.ylabel(\"Fraction\")\n",
        "                plt.xticks(idx); plt.ylim(0, 1); plt.legend(frameon=False)\n",
        "                outp = PLOTS_OUT / f\"zipf_bins_stacked_{slug}.png\"\n",
        "                plt.tight_layout(); plt.savefig(outp); plt.close()\n",
        "            except Exception as e:\n",
        "                per_article_ok = False\n",
        "                print(json.dumps({\"plot\": \"zipf_bins_stacked\", \"slug\": slug, \"error\": str(e)}))\n",
        "except Exception as e:\n",
        "    per_article_ok = False\n",
        "    print(json.dumps({\"plot\": \"per_article\", \"error\": str(e)}))\n",
        "\n",
        "# ---- Write Parquet artifact (document-level) ----\n",
        "parquet_path = BASE_OUT / \"lexical_features.parquet\"\n",
        "df_lex.to_parquet(parquet_path, index=False, engine=\"pyarrow\")\n",
        "\n",
        "# ---- Deltas between consecutive versions within each article ----\n",
        "def _token_set(text: str) -> set:\n",
        "    return set([m.group(0).lower() for m in TOKEN_RE.finditer(text or \"\")])\n",
        "\n",
        "def token_jaccard(a_text: str, b_text: str) -> float:\n",
        "    A, B = _token_set(a_text), _token_set(b_text)\n",
        "    if not A and not B: return float(\"nan\")\n",
        "    inter = len(A & B); uni = len(A | B)\n",
        "    return inter / uni if uni else float(\"nan\")\n",
        "\n",
        "def js_divergence(p, q, eps=1e-8):\n",
        "    p = np.asarray(p, dtype=float) + eps\n",
        "    q = np.asarray(q, dtype=float) + eps\n",
        "    p /= p.sum(); q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    kl = lambda x, y: np.sum(x * np.log(x / y))\n",
        "    return float(0.5 * kl(p, m) + 0.5 * kl(q, m))\n",
        "\n",
        "delta_rows = []\n",
        "if \"article_id\" in df_lex.columns and df_lex[\"article_id\"].notna().any():\n",
        "    # need access to original cleaned text for jaccard; rebuild map from DOCS\n",
        "    text_map = {d.get(\"doc_id\", f\"doc_{i:04d}\"): d.get(\"text\",\"\") for i,d in enumerate(DOCS)}\n",
        "    for slug in sorted(df_lex[\"article_id\"].dropna().unique()):\n",
        "        sub = df_lex[df_lex[\"article_id\"] == slug].copy().sort_values(\"version_id\")\n",
        "        vids = sub[\"version_id\"].astype(int).tolist()\n",
        "        docs_order = sub[\"doc_id\"].tolist()\n",
        "        for i in range(len(vids)-1):\n",
        "            v_from, v_to = vids[i], vids[i+1]\n",
        "            if v_to != v_from + 1:  # only adjacent pairs (1->2, 2->3, 3->4)\n",
        "                continue\n",
        "            docA, docB = docs_order[i], docs_order[i+1]\n",
        "            p = [float(sub.iloc[i][\"rare_rate\"] or 0), float(sub.iloc[i][\"mid_rate\"] or 0), float(sub.iloc[i][\"common_rate\"] or 0)]\n",
        "            q = [float(sub.iloc[i+1][\"rare_rate\"] or 0), float(sub.iloc[i+1][\"mid_rate\"] or 0), float(sub.iloc[i+1][\"common_rate\"] or 0)]\n",
        "            delta_rows.append({\n",
        "                \"article_id\": slug,\n",
        "                \"from_version\": int(v_from),\n",
        "                \"to_version\": int(v_to),\n",
        "                \"token_jaccard\": token_jaccard(text_map.get(docA,\"\"), text_map.get(docB,\"\")),\n",
        "                \"zipf_jsd\": js_divergence(p, q),\n",
        "            })\n",
        "\n",
        "df_deltas = pd.DataFrame(delta_rows) if delta_rows else pd.DataFrame(\n",
        "    columns=[\"article_id\",\"from_version\",\"to_version\",\"token_jaccard\",\"zipf_jsd\"])\n",
        "deltas_path = BASE_OUT / \"lexical_deltas.parquet\"\n",
        "df_deltas.to_parquet(deltas_path, index=False, engine=\"pyarrow\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"cell 1.6: textstat|wordfreq: visuals — baseline distributions (+per-article & deltas)\",\n",
        "    \"status\": \"pass\",\n",
        "    \"plots\": {\"sentence_length_hist\": bool('sent_ok' in locals() and sent_ok),\n",
        "              \"zipf_hist\": bool('zipf_ok' in locals() and zipf_ok),\n",
        "              \"per_article\": bool(per_article_ok)},\n",
        "    \"artifacts\": {\"features_parquet\": str(parquet_path), \"deltas_parquet\": str(deltas_path)}\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S4Zs9K_viEM"
      },
      "outputs": [],
      "source": [
        "# cell 1.Y: Module 1 validtion\n",
        "import pandas as pd, json, pathlib\n",
        "\n",
        "base = pathlib.Path(\"outputs/textstat_lex\")  # or the folder where you placed them\n",
        "meta = json.loads((base / \"metadata.json\").read_text())\n",
        "feat = pd.read_parquet(base / \"lexical_features.parquet\")\n",
        "delt = pd.read_parquet(base / \"lexical_deltas.parquet\")\n",
        "\n",
        "print(\"version_order_source:\", meta.get(\"version_order_source\"))\n",
        "\n",
        "# Features: required keys\n",
        "req_feat = {\"article_id\",\"version_id\",\"version_tag\",\"doc_id\"}\n",
        "print(\"features missing keys:\", sorted(req_feat - set(feat.columns)))\n",
        "\n",
        "# Deltas: adjacency & ranges\n",
        "req_delta = {\"article_id\",\"from_version\",\"to_version\",\"token_jaccard\",\"zipf_jsd\"}\n",
        "print(\"deltas missing keys:\", sorted(req_delta - set(delt.columns)))\n",
        "print(\"adjacent only:\", ((delt[\"to_version\"] - delt[\"from_version\"]) == 1).all())\n",
        "print(\"token_jaccard in [0,1]:\", delt[\"token_jaccard\"].dropna().between(0,1).all())\n",
        "print(\"zipf_jsd in [0,1]:\", delt[\"zipf_jsd\"].dropna().between(0,1).all())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cqS9BgLzQdG"
      },
      "outputs": [],
      "source": [
        "# cell 2.1: NLTK — install & corpora download (module-only; base-safe)\n",
        "# Wheels-only, quiet; no base upgrades. Install pyarrow for Parquet writes used in 2.3.\n",
        "%pip install -q --only-binary=:all: --no-deps \"nltk>=3.8,<3.10\" \"pyarrow>=14,<18\"\n",
        "\n",
        "import json, sys\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "needed = {\"punkt\": \"tokenizers/punkt\", \"stopwords\": \"corpora/stopwords\"}\n",
        "present = {}\n",
        "for pkg, loc in needed.items():\n",
        "    try:\n",
        "        find(loc)\n",
        "        present[pkg] = True\n",
        "    except LookupError:\n",
        "        nltk.download(pkg, quiet=True, raise_on_error=False)\n",
        "        try:\n",
        "            find(loc)\n",
        "            present[pkg] = True\n",
        "        except LookupError:\n",
        "            present[pkg] = False\n",
        "\n",
        "report = {\n",
        "    \"cell_id\": \"2.1\",\n",
        "    \"python\": sys.version.split()[0],\n",
        "    \"nltk\": nltk.__version__,\n",
        "    \"corpora\": present,\n",
        "}\n",
        "print(json.dumps(report, indent=2))\n",
        "\n",
        "# Fail-fast if required corpora are unavailable after attempted download\n",
        "if not all(present.values()):\n",
        "    missing = [k for k, ok in present.items() if not ok]\n",
        "    raise RuntimeError(f\"Module 2 requires NLTK corpora {missing}; download failed. Aborting Module 2.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ijw0BOepzlJ"
      },
      "outputs": [],
      "source": [
        "# 2.2a — NLTK data hotfix (punkt, punkt_tab, stopwords)\n",
        "from pathlib import Path\n",
        "import os, nltk\n",
        "\n",
        "# Put NLTK data under the working dir so it’s reproducible and non-root\n",
        "NLTK_DIR = Path(os.environ.get(\"NLTK_DATA\", (BASE_DIR if 'BASE_DIR' in globals() else Path.cwd()) / \"nltk_data\"))\n",
        "NLTK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "if str(NLTK_DIR) not in nltk.data.path:\n",
        "    nltk.data.path.insert(0, str(NLTK_DIR))\n",
        "\n",
        "def _ensure(resource: str, locator: str):\n",
        "    try:\n",
        "        nltk.data.find(locator)\n",
        "    except LookupError:\n",
        "        nltk.download(resource, download_dir=str(NLTK_DIR), quiet=True)\n",
        "        nltk.data.find(locator)  # fail fast if still missing\n",
        "\n",
        "# Required for sentence tokenization & stopwords used in 2.x\n",
        "_ensure(\"punkt\",      \"tokenizers/punkt\")\n",
        "# punkt_tab is needed in newer NLTK; ignore quietly if resource not recognized\n",
        "try:\n",
        "    _ensure(\"punkt_tab\",  \"tokenizers/punkt_tab\")\n",
        "except Exception:\n",
        "    pass\n",
        "_ensure(\"stopwords\",  \"corpora/stopwords\")\n",
        "\n",
        "print(f\"✔ NLTK data ready at {NLTK_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvuKna9pzSU7"
      },
      "outputs": [],
      "source": [
        "# cell 2.2: NLTK — imports & tokenizer init (filename-prefix discovery compatible)\n",
        "# Singletons for sentence/word tokenization. English stopwords used as function-word proxy.\n",
        "from __future__ import annotations\n",
        "\n",
        "import re, math, statistics\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.data import load as nltk_load\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "LANG = \"english\"  # If Module 1 later exposes lang detection, wire it here.\n",
        "STOPWORDS = set(stopwords.words(LANG))\n",
        "\n",
        "# Singletons\n",
        "PUNKT = nltk_load(\"tokenizers/punkt/english.pickle\")\n",
        "TB = TreebankWordTokenizer()\n",
        "\n",
        "@dataclass\n",
        "class SentenceSpan:\n",
        "    start: int\n",
        "    end: int\n",
        "    text: str\n",
        "\n",
        "def sent_spans(text: str) -> List[SentenceSpan]:\n",
        "    return [SentenceSpan(s, e, text[s:e]) for (s, e) in PUNKT.span_tokenize(text)]\n",
        "\n",
        "@dataclass\n",
        "class TokenSpan:\n",
        "    start: int\n",
        "    end: int\n",
        "    text: str\n",
        "    is_stop: bool\n",
        "\n",
        "def token_spans(text: str) -> List[TokenSpan]:\n",
        "    toks = []\n",
        "    for (s, e) in TB.span_tokenize(text):\n",
        "        w = text[s:e]\n",
        "        toks.append(TokenSpan(s, e, w, w.lower() in STOPWORDS))\n",
        "    return toks\n",
        "\n",
        "print({\"cell_id\": \"2.2\", \"status\": \"ready\", \"stopwords\": len(STOPWORDS)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrjXX6dLzUN1"
      },
      "outputs": [],
      "source": [
        "# cell 2.3: NLTK — function-word & burstiness (doc-level) + sliding windows\n",
        "from __future__ import annotations\n",
        "\n",
        "import json, math, statistics\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---- Output dirs\n",
        "OUTDIR = Path(\"outputs/nltk\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS  = OUTDIR / \"plots\";    PLOTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _cv(vals: List[int|float]) -> float:\n",
        "    vals = [float(v) for v in vals if v is not None]\n",
        "    if not vals: return float(\"nan\")\n",
        "    mu = statistics.mean(vals)\n",
        "    if mu == 0 or len(vals) < 2: return float(\"nan\")\n",
        "    return statistics.pstdev(vals) / mu\n",
        "\n",
        "# ---- DOC-LEVEL FEATURES ------------------------------------------------------\n",
        "def doc_sentence_token_stats(text: str) -> Dict[str, Any]:\n",
        "    sents = sent_spans(text)\n",
        "    sent_token_counts = []\n",
        "    sent_stop_counts  = []\n",
        "    for s in sents:\n",
        "        toks = token_spans(s.text)\n",
        "        sent_token_counts.append(len(toks))\n",
        "        sent_stop_counts.append(sum(t.is_stop for t in toks))\n",
        "\n",
        "    tokens = token_spans(text)\n",
        "    n_tokens = len(tokens)\n",
        "    n_stop   = sum(t.is_stop for t in tokens)\n",
        "\n",
        "    stop_counts = Counter(t.text.lower() for t in tokens if t.is_stop)\n",
        "    top_fw = stop_counts.most_common(50)\n",
        "    total_stop = max(1, sum(stop_counts.values()))\n",
        "    # Normalize function-word profile by total stopwords (reviewer request)\n",
        "    top_fw_norm = {k: v / total_stop for k, v in top_fw}\n",
        "\n",
        "    return {\n",
        "        \"n_sent\": len(sents),\n",
        "        \"n_tokens\": n_tokens,\n",
        "        \"stopword_ratio\": (n_stop / n_tokens) if n_tokens else float(\"nan\"),\n",
        "        \"sent_len_mean\": statistics.mean(sent_token_counts) if sent_token_counts else float(\"nan\"),\n",
        "        \"sent_len_median\": statistics.median(sent_token_counts) if sent_token_counts else float(\"nan\"),\n",
        "        \"sent_len_max\": max(sent_token_counts) if sent_token_counts else float(\"nan\"),\n",
        "        \"burstiness_token_cv\": _cv(sent_token_counts),\n",
        "        \"burstiness_stopword_cv\": _cv(sent_stop_counts),\n",
        "        \"function_word_top50\": top_fw_norm,\n",
        "    }\n",
        "\n",
        "# ---- INPUT DISCOVERY (df_docs or DOCS) --------------------------------------\n",
        "def _gather_docs() -> pd.DataFrame:\n",
        "    # Accept df_docs (Module 1) or DOCS (list of dicts). Expect keys:\n",
        "    # article_id, version_id, version_tag, and one of text_norm/text_clean/text\n",
        "    cols_pref = [\"text_norm\", \"text_clean\", \"text\"]\n",
        "    try:\n",
        "        df = df_docs.copy()\n",
        "    except NameError:\n",
        "        try:\n",
        "            df = pd.DataFrame(DOCS)\n",
        "        except NameError:\n",
        "            raise RuntimeError(\"Module 2.3: No input found. Expect df_docs or DOCS.\")\n",
        "\n",
        "    if \"article_id\" not in df.columns:\n",
        "        if \"slug\" in df.columns:\n",
        "            df[\"article_id\"] = df[\"slug\"]\n",
        "        else:\n",
        "            df[\"article_id\"] = df.index.astype(str)\n",
        "    if \"version_id\" not in df.columns:\n",
        "        df[\"version_id\"] = 1\n",
        "    if \"version_tag\" not in df.columns:\n",
        "        df[\"version_tag\"] = \"v\" + df[\"version_id\"].astype(str)\n",
        "\n",
        "    for c in cols_pref:\n",
        "        if c in df.columns:\n",
        "            df[\"text_basis\"] = df[c]\n",
        "            df[\"span_basis\"] = c\n",
        "            break\n",
        "    else:\n",
        "        raise RuntimeError(\"Module 2.3: No text column found among text_norm/text_clean/text.\")\n",
        "\n",
        "    return df[[\"article_id\",\"version_id\",\"version_tag\",\"text_basis\",\"span_basis\"]].copy()\n",
        "\n",
        "df2_input = _gather_docs()\n",
        "\n",
        "# ---- Build doc-level table df_nltk ------------------------------------------\n",
        "rows = []\n",
        "for art, ver, vtag, txt, _span in df2_input.itertuples(index=False):\n",
        "    st = doc_sentence_token_stats(txt)\n",
        "    st[\"article_id\"]  = str(art)\n",
        "    st[\"version_id\"]  = int(ver)\n",
        "    st[\"version_tag\"] = str(vtag)\n",
        "    rows.append(st)\n",
        "\n",
        "df_nltk = pd.DataFrame(rows)\n",
        "# Expand function-word profile to wide columns\n",
        "fw_df = pd.DataFrame.from_records(\n",
        "    ({**{\"article_id\": r[\"article_id\"], \"version_id\": r[\"version_id\"]},\n",
        "      **{f\"fw::{k}\": v for k, v in r[\"function_word_top50\"].items()}} for r in rows)\n",
        ").fillna(0.0)\n",
        "df_nltk = df_nltk.drop(columns=[\"function_word_top50\"]).merge(fw_df, on=[\"article_id\",\"version_id\"], how=\"left\")\n",
        "\n",
        "# Dtypes for stability\n",
        "_df_types = {\n",
        "    \"article_id\": \"string\", \"version_id\": \"int64\", \"version_tag\": \"string\",\n",
        "    \"n_sent\": \"int64\", \"n_tokens\": \"int64\",\n",
        "    \"stopword_ratio\": \"float64\", \"sent_len_mean\": \"float64\", \"sent_len_median\": \"float64\",\n",
        "    \"sent_len_max\": \"float64\", \"burstiness_token_cv\": \"float64\", \"burstiness_stopword_cv\": \"float64\",\n",
        "}\n",
        "for k, t in _df_types.items():\n",
        "    if k in df_nltk.columns:\n",
        "        df_nltk[k] = df_nltk[k].astype(t)\n",
        "\n",
        "# ---- WINDOW-LEVEL BUILDER (expert spec) -------------------------------------\n",
        "def build_windows(\n",
        "    docs: pd.DataFrame,\n",
        "    window_sents: int = 3,\n",
        "    stride_sents: int = 1,\n",
        "    keep_tail: bool = False,\n",
        "    span_on: str = \"auto\",  # kept for signature parity; span is already chosen in df2_input\n",
        ") -> pd.DataFrame:\n",
        "    recs = []\n",
        "    for art, ver, vtag, txt, span_basis in docs[[\"article_id\",\"version_id\",\"version_tag\",\"text_basis\",\"span_basis\"]].itertuples(index=False):\n",
        "        s_spans = list(PUNKT.span_tokenize(txt))\n",
        "        n_s = len(s_spans)\n",
        "        if n_s == 0:\n",
        "            continue\n",
        "\n",
        "        i = 0\n",
        "        win_id = 1\n",
        "        while i < n_s:\n",
        "            j = i + window_sents\n",
        "            partial = False\n",
        "            if j > n_s:\n",
        "                if not keep_tail:\n",
        "                    break\n",
        "                j = n_s\n",
        "                partial = True\n",
        "\n",
        "            # Per-sentence tokenization in the window\n",
        "            sent_token_counts = []\n",
        "            stop_count = 0\n",
        "            tokens_alpha = []\n",
        "            total_tokens = 0\n",
        "\n",
        "            for (ss, ee) in s_spans[i:j]:\n",
        "                s_txt = txt[ss:ee]\n",
        "                toks = list(TB.tokenize(s_txt))\n",
        "                total_tokens += len(toks)\n",
        "                sent_token_counts.append(len(toks))\n",
        "                stop_count += sum(1 for w in toks if w.lower() in STOPWORDS)\n",
        "                tokens_alpha.extend([w.lower() for w in toks if w.isalpha()])\n",
        "\n",
        "            n_sents_win = (j - i)\n",
        "            n_tokens_win = total_tokens\n",
        "            mean_len = statistics.mean(sent_token_counts) if n_sents_win > 0 else float(\"nan\")\n",
        "            std_len  = statistics.pstdev(sent_token_counts) if n_sents_win > 1 else float(\"nan\")\n",
        "\n",
        "            type_counts = Counter(tokens_alpha)\n",
        "            types_total = len(type_counts)\n",
        "            hapax = sum(1 for _, c in type_counts.items() if c == 1)\n",
        "            hapax_rate = (hapax / types_total) if types_total > 0 else float(\"nan\")\n",
        "\n",
        "            stop_rate = (stop_count / n_tokens_win) if n_tokens_win > 0 else float(\"nan\")\n",
        "            content_rate = 1.0 - stop_rate if not math.isnan(stop_rate) else float(\"nan\")\n",
        "\n",
        "            # Burstiness within window (guards per spec)\n",
        "            burst_cv = _cv(sent_token_counts) if (n_sents_win >= 2 and n_tokens_win >= 10) else float(\"nan\")\n",
        "\n",
        "            # Top-K content token dispersion across sentences in the window\n",
        "            K = 20\n",
        "            per_sent_counts = []\n",
        "            per_sent_token_lists = []\n",
        "            for (ss, ee) in s_spans[i:j]:\n",
        "                s_txt = txt[ss:ee]\n",
        "                toks = [w.lower() for w in TB.tokenize(s_txt)]\n",
        "                content = [w for w in toks if w.isalpha() and w not in STOPWORDS]\n",
        "                per_sent_token_lists.append(content)\n",
        "                per_sent_counts.append(Counter(content))\n",
        "            global_counts = Counter([w for lst in per_sent_token_lists for w in lst])\n",
        "            topk = [w for (w, _) in global_counts.most_common(min(K, len(global_counts)))]\n",
        "            cvs = []\n",
        "            if (n_sents_win >= 2 and n_tokens_win >= 10) and topk:\n",
        "                for w in topk:\n",
        "                    vec = [cnt.get(w, 0) for cnt in per_sent_counts]\n",
        "                    cvs.append(_cv(vec))\n",
        "            burst_topk_mean_cv = (statistics.mean([v for v in cvs if not math.isnan(v)]) if cvs else float(\"nan\"))\n",
        "\n",
        "            char_start = s_spans[i][0]\n",
        "            char_end   = s_spans[j-1][1]\n",
        "\n",
        "            recs.append({\n",
        "                \"article_id\": str(art),\n",
        "                \"version_id\": int(ver),\n",
        "                \"version_tag\": str(vtag),\n",
        "                \"doc_id\": f\"{art}-v{ver}\",\n",
        "                \"win_id\": int(win_id),\n",
        "                \"win_label\": f\"v{ver}-w{win_id}\",\n",
        "                \"span_basis\": span_basis,\n",
        "                \"char_start\": int(char_start),\n",
        "                \"char_end\": int(char_end),\n",
        "                \"sent_start_index\": int(i),\n",
        "                \"sent_end_index\": int(j-1),\n",
        "                \"is_partial_tail\": bool(partial),\n",
        "                \"n_sents_win\": int(n_sents_win),\n",
        "                \"n_tokens_win\": int(n_tokens_win),\n",
        "                \"mean_sent_len_tok_win\": float(mean_len) if not math.isnan(mean_len) else float(\"nan\"),\n",
        "                \"std_sent_len_tok_win\": float(std_len) if not math.isnan(std_len) else float(\"nan\"),\n",
        "                \"stopword_rate_win\": float(stop_rate) if not math.isnan(stop_rate) else float(\"nan\"),\n",
        "                \"content_rate_win\": float(content_rate) if not math.isnan(content_rate) else float(\"nan\"),\n",
        "                \"hapax_rate_win\": float(hapax_rate) if not math.isnan(hapax_rate) else float(\"nan\"),\n",
        "                \"function_word_rate_nltk_win\": float(stop_rate) if not math.isnan(stop_rate) else float(\"nan\"),\n",
        "                \"burstiness_token_cv_win\": float(burst_cv) if not math.isnan(burst_cv) else float(\"nan\"),\n",
        "                \"burstiness_topk_mean_cv_win\": float(burst_topk_mean_cv) if not math.isnan(burst_topk_mean_cv) else float(\"nan\"),\n",
        "            })\n",
        "\n",
        "            win_id += 1\n",
        "            i += stride_sents\n",
        "            if i >= n_s:\n",
        "                break\n",
        "\n",
        "    dfw = pd.DataFrame.from_records(recs)\n",
        "\n",
        "    # Dtype enforcement\n",
        "    dtypes = {\n",
        "        \"article_id\":\"string\",\"version_id\":\"int64\",\"version_tag\":\"string\",\"doc_id\":\"string\",\n",
        "        \"win_id\":\"int64\",\"win_label\":\"string\",\"span_basis\":\"string\",\n",
        "        \"char_start\":\"int64\",\"char_end\":\"int64\",\"sent_start_index\":\"int64\",\"sent_end_index\":\"int64\",\n",
        "        \"is_partial_tail\":\"boolean\",\n",
        "        \"n_sents_win\":\"int64\",\"n_tokens_win\":\"int64\",\n",
        "        \"mean_sent_len_tok_win\":\"float64\",\"std_sent_len_tok_win\":\"float64\",\n",
        "        \"stopword_rate_win\":\"float64\",\"content_rate_win\":\"float64\",\"hapax_rate_win\":\"float64\",\n",
        "        \"function_word_rate_nltk_win\":\"float64\",\n",
        "        \"burstiness_token_cv_win\":\"float64\",\"burstiness_topk_mean_cv_win\":\"float64\",\n",
        "    }\n",
        "    for col, dt in dtypes.items():\n",
        "        if col in dfw.columns:\n",
        "            dfw[col] = dfw[col].astype(dt)\n",
        "\n",
        "    dfw = dfw.sort_values([\"article_id\",\"version_id\",\"win_id\"], kind=\"stable\").reset_index(drop=True)\n",
        "    return dfw\n",
        "\n",
        "# Build windows per expert spec and persist\n",
        "df_nltk_win = build_windows(df2_input, window_sents=3, stride_sents=1, keep_tail=False)\n",
        "\n",
        "# Save window artifact\n",
        "(df_nltk_win).to_parquet(OUTDIR / \"fw_burstiness_windows.parquet\", index=False)\n",
        "\n",
        "# Update metadata\n",
        "meta_path = OUTDIR / \"metadata.json\"\n",
        "meta_update = {\n",
        "    \"module\": \"2\",\n",
        "    \"window_sents\": 3,\n",
        "    \"stride_sents\": 1,\n",
        "    \"keep_tail\": False,\n",
        "    \"tokenizer_word\": \"NLTK Treebank\",\n",
        "    \"tokenizer_sent\": \"NLTK Punkt (english)\",\n",
        "}\n",
        "try:\n",
        "    if meta_path.exists():\n",
        "        with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "    else:\n",
        "        meta = {}\n",
        "    meta.update(meta_update)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "except Exception as e:\n",
        "    print(json.dumps({\"metadata_write_error\": str(e)}))\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\":\"2.3\",\n",
        "    \"status\":\"pass\",\n",
        "    \"n_docs\": int(df_nltk.shape[0]) if 'df_nltk' in globals() else None,\n",
        "    \"n_windows\": int(df_nltk_win.shape[0]),\n",
        "    \"artifacts\": {\n",
        "        \"windows_parquet\": \"outputs/nltk/fw_burstiness_windows.parquet\",\n",
        "        \"metadata\": \"outputs/nltk/metadata.json\"\n",
        "    }\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OugIjKBUzW4R"
      },
      "outputs": [],
      "source": [
        "# cell 2.4: NLTK — visuals (robust, no duplicates, per-version overlays)\n",
        "import os, json, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Preconditions\n",
        "if \"df_nltk\" not in globals():\n",
        "    raise RuntimeError(\"Module 2.4: df_nltk not found. Run 2.3 first.\")\n",
        "\n",
        "need = {\"article_id\",\"version_id\",\"stopword_ratio\",\"sent_len_mean\",\"burstiness_token_cv\"}\n",
        "missing = need - set(df_nltk.columns)\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Module 2.4: df_nltk is missing columns: {sorted(missing)}\")\n",
        "\n",
        "# enforce numeric version_id for correct ordering\n",
        "try:\n",
        "    df_nltk[\"version_id\"] = pd.to_numeric(df_nltk[\"version_id\"], errors=\"raise\").astype(\"int64\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Module 2.4: version_id must be numeric. {e}\")\n",
        "\n",
        "PLOTS = Path(\"outputs/nltk/plots\"); PLOTS.mkdir(parents=True, exist_ok=True)\n",
        "MAX_ARTICLES = int(os.environ.get(\"LSA_MAX_ARTICLES\", \"10\"))\n",
        "manifest = []\n",
        "\n",
        "def _safe_slug(s: str) -> str:\n",
        "    # keep only [-_.a-zA-Z0-9]; map others to '_'\n",
        "    import re\n",
        "    return re.sub(r\"[^-_.a-zA-Z0-9]\", \"_\", str(s))\n",
        "\n",
        "def _finite_percentiles(series: pd.Series, lo=5, hi=95):\n",
        "    vals = series.replace([np.inf, -np.inf], np.nan).astype(float).dropna().to_numpy()\n",
        "    if vals.size == 0:\n",
        "        return None\n",
        "    lo_v, hi_v = np.percentile(vals, [lo, hi])\n",
        "    if not np.isfinite(hi_v) or hi_v == lo_v:\n",
        "        # widen tiny ranges\n",
        "        hi_v = lo_v + 1e-9\n",
        "    return (lo_v, hi_v)\n",
        "\n",
        "def _save_show(fig, path: Path):\n",
        "    fig.savefig(path, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    manifest.append(str(path))\n",
        "\n",
        "# --- 1) Global violin: burstiness_token_cv (guard empty)\n",
        "vals = df_nltk[\"burstiness_token_cv\"].replace([np.inf,-np.inf], np.nan).dropna().to_numpy()\n",
        "if vals.size == 0:\n",
        "    fig, ax = plt.subplots(dpi=120)\n",
        "    ax.text(0.5, 0.5, \"No burstiness data to plot\", ha=\"center\", va=\"center\")\n",
        "    ax.set_axis_off()\n",
        "    out = PLOTS / \"global_burstiness_violin_EMPTY.png\"\n",
        "    _save_show(fig, out)\n",
        "    global_ok = False\n",
        "else:\n",
        "    fig, ax = plt.subplots(dpi=120)\n",
        "    ax.violinplot(vals, showmeans=True, showextrema=True, showmedians=True)\n",
        "    ax.set_title(\"Global burstiness (token CV across sentences)\")\n",
        "    ax.set_ylabel(\"CV\")\n",
        "    out = PLOTS / \"global_burstiness_violin.png\"\n",
        "    _save_show(fig, out)\n",
        "    global_ok = True\n",
        "\n",
        "# --- 2) Per-article radar with version overlays (v1..v4…), normalized by global 5–95th pct\n",
        "cols = [\"stopword_ratio\", \"sent_len_mean\", \"burstiness_token_cv\"]\n",
        "norms = {}\n",
        "for c in cols:\n",
        "    p = _finite_percentiles(df_nltk[c])\n",
        "    norms[c] = p  # could be None if no finite data\n",
        "\n",
        "def _radar_overlay(df_art: pd.DataFrame, art_label: str):\n",
        "    # normalize each version’s vector using global norms (fallback to per-article if needed)\n",
        "    labels = cols\n",
        "    # compute fallback norms per-article if global missing\n",
        "    local_norms = {}\n",
        "    for c in cols:\n",
        "        local_norms[c] = norms[c] or _finite_percentiles(df_art[c]) or (0.0, 1.0)\n",
        "\n",
        "    # build angle grid\n",
        "    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "\n",
        "    fig = plt.figure(dpi=120)\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "    for _, row in df_art.sort_values(\"version_id\").iterrows():\n",
        "        vec = []\n",
        "        for c in labels:\n",
        "            lo_v, hi_v = local_norms[c]\n",
        "            x = float(row[c])\n",
        "            if not np.isfinite(x):\n",
        "                vec.append(0.0)\n",
        "            else:\n",
        "                vec.append(np.clip((x - lo_v) / (hi_v - lo_v), 0.0, 1.0))\n",
        "        vec += vec[:1]\n",
        "        ax.plot(angles, vec, marker=\"o\", label=f\"v{int(row['version_id'])}\")\n",
        "        ax.fill(angles, vec, alpha=0.05)\n",
        "\n",
        "    ax.set_xticks(angles[:-1]); ax.set_xticklabels(labels)\n",
        "    ax.set_title(f\"Lex profile (overlay by version) — {art_label}\", pad=12)\n",
        "    ax.set_rlabel_position(0)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.25, 1.10))\n",
        "    return fig\n",
        "\n",
        "radar_ok = True\n",
        "try:\n",
        "    shown = 0\n",
        "    for art, g in df_nltk.groupby(\"article_id\", sort=False):\n",
        "        if shown >= MAX_ARTICLES: break\n",
        "        fig = _radar_overlay(g, art_label=str(art))\n",
        "        out = PLOTS / f\"radar_overlay_{_safe_slug(art)}.png\"\n",
        "        _save_show(fig, out)\n",
        "        shown += 1\n",
        "except Exception as e:\n",
        "    radar_ok = False\n",
        "    print(json.dumps({\"plot\":\"per-article_radar_overlay\",\"error\":str(e)}))\n",
        "\n",
        "# --- 3) Per-article trends across versions (stopword_ratio & burstiness)\n",
        "trends_ok = True\n",
        "try:\n",
        "    shown = 0\n",
        "    for art, g in df_nltk.groupby(\"article_id\", sort=False):\n",
        "        if shown >= MAX_ARTICLES: break\n",
        "        gg = g.sort_values(\"version_id\")\n",
        "        # Stopword ratio trend\n",
        "        fig, ax = plt.subplots(dpi=120)\n",
        "        ax.plot(gg[\"version_id\"], gg[\"stopword_ratio\"], marker=\"o\")\n",
        "        ax.set_title(f\"Stopword ratio across versions — {art}\")\n",
        "        ax.set_xlabel(\"version_id\"); ax.set_ylabel(\"stopword_ratio\")\n",
        "        ax.set_xticks(gg[\"version_id\"])\n",
        "        out_sr = PLOTS / f\"trend_stopword_ratio_{_safe_slug(art)}.png\"\n",
        "        _save_show(fig, out_sr)\n",
        "\n",
        "        # Burstiness trend\n",
        "        fig, ax = plt.subplots(dpi=120)\n",
        "        ax.plot(gg[\"version_id\"], gg[\"burstiness_token_cv\"], marker=\"o\")\n",
        "        ax.set_title(f\"Burstiness (token CV) across versions — {art}\")\n",
        "        ax.set_xlabel(\"version_id\"); ax.set_ylabel(\"burstiness_token_cv\")\n",
        "        ax.set_xticks(gg[\"version_id\"])\n",
        "        out_cv = PLOTS / f\"trend_burstiness_cv_{_safe_slug(art)}.png\"\n",
        "        _save_show(fig, out_cv)\n",
        "\n",
        "        shown += 1\n",
        "except Exception as e:\n",
        "    trends_ok = False\n",
        "    print(json.dumps({\"plot\":\"per-article_trends\",\"error\":str(e)}))\n",
        "\n",
        "# --- 4) Manifest of generated plot files\n",
        "(PLOTS / \"plots_index.json\").write_text(json.dumps({\"files\": manifest}, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\":\"2.4\",\n",
        "    \"status\":\"pass\",\n",
        "    \"plots\": {\n",
        "        \"global_burstiness_violin\": bool(global_ok),\n",
        "        \"per_article_radar_overlay\": bool(radar_ok),\n",
        "        \"per_article_trends\": bool(trends_ok),\n",
        "    },\n",
        "    \"plots_dir\": str(PLOTS),\n",
        "    \"manifest_count\": len(manifest)\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOvAnVT7lmt3"
      },
      "outputs": [],
      "source": [
        "# cell 2.4add — fill the gaps only (doc parquet, deltas, minimal metadata, radar filename copy)\n",
        "from __future__ import annotations\n",
        "import json, math, statistics, shutil, re\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "OUTDIR = Path(\"outputs/nltk\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "PLOTS  = OUTDIR / \"plots\";     PLOTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "doc_path    = OUTDIR / \"fw_burstiness.parquet\"\n",
        "deltas_path = OUTDIR / \"fw_burstiness_deltas.parquet\"\n",
        "meta_path   = OUTDIR / \"metadata.json\"\n",
        "\n",
        "# --- reuse tokenizers from 2.2; load if not in memory ---\n",
        "try:\n",
        "    STOPWORDS, PUNKT, TB  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.data import load as nltk_load\n",
        "    from nltk.tokenize import TreebankWordTokenizer\n",
        "    STOPWORDS = set(stopwords.words(\"english\"))\n",
        "    PUNKT = nltk_load(\"tokenizers/punkt/english.pickle\")\n",
        "    TB = TreebankWordTokenizer()\n",
        "\n",
        "def _cv(vals: List[int|float]) -> float:\n",
        "    vals = [float(v) for v in vals if v is not None]\n",
        "    if not vals: return float(\"nan\")\n",
        "    mu = statistics.mean(vals)\n",
        "    if mu == 0 or len(vals) < 2: return float(\"nan\")\n",
        "    return statistics.pstdev(vals) / mu\n",
        "\n",
        "def _gather_docs() -> pd.DataFrame:\n",
        "    # prefer df2_input from 2.3; otherwise use df_docs/DOCS\n",
        "    if \"df2_input\" in globals():\n",
        "        base = df2_input.copy()\n",
        "        return base[[\"article_id\",\"version_id\",\"version_tag\",\"text_basis\"]]\n",
        "    try:\n",
        "        base = df_docs.copy()\n",
        "    except NameError:\n",
        "        base = pd.DataFrame(DOCS)\n",
        "    if \"article_id\" not in base.columns:\n",
        "        base[\"article_id\"] = (base[\"slug\"] if \"slug\" in base.columns else base.index).astype(str)\n",
        "    if \"version_id\" not in base.columns:\n",
        "        base[\"version_id\"] = 1\n",
        "    if \"version_tag\" not in base.columns:\n",
        "        base[\"version_tag\"] = \"v\" + base[\"version_id\"].astype(str)\n",
        "    for c in (\"text_norm\",\"text_clean\",\"text\"):\n",
        "        if c in base.columns:\n",
        "            base[\"text_basis\"] = base[c]; break\n",
        "    else:\n",
        "        raise RuntimeError(\"2.4add: no text column found (text_norm/text_clean/text).\")\n",
        "    return base[[\"article_id\",\"version_id\",\"version_tag\",\"text_basis\"]]\n",
        "\n",
        "def _doc_features(text: str) -> Dict[str, Any]:\n",
        "    s_spans = list(PUNKT.span_tokenize(text))\n",
        "    n_sents = len(s_spans)\n",
        "    sent_token_counts, per_sent_content = [], []\n",
        "    stop_count, total_tokens = 0, 0\n",
        "    tokens_alpha = []\n",
        "    for (s, e) in s_spans:\n",
        "        s_txt = text[s:e]\n",
        "        toks = [w for w in TB.tokenize(s_txt)]\n",
        "        total_tokens += len(toks)\n",
        "        sent_token_counts.append(len(toks))\n",
        "        stop_count += sum(1 for w in toks if w.lower() in STOPWORDS)\n",
        "        alpha = [w.lower() for w in toks if w.isalpha()]\n",
        "        tokens_alpha.extend(alpha)\n",
        "        per_sent_content.append(Counter([w for w in alpha if w not in STOPWORDS]))\n",
        "    type_counts = Counter(tokens_alpha)\n",
        "    types_total = len(type_counts)\n",
        "    hapax = sum(1 for _, c in type_counts.items() if c == 1)\n",
        "    hapax_rate = (hapax / types_total) if types_total > 0 else float(\"nan\")\n",
        "    mean_len = statistics.mean(sent_token_counts) if n_sents > 0 else float(\"nan\")\n",
        "    std_len  = statistics.pstdev(sent_token_counts) if n_sents > 1 else float(\"nan\")\n",
        "    burst_cv = _cv(sent_token_counts) if (n_sents >= 2 and total_tokens >= 10) else float(\"nan\")\n",
        "    # top-K content token dispersion\n",
        "    K = 20\n",
        "    global_counts = Counter([w for lst in ([list(c.elements()) for c in per_sent_content]) for w in lst])\n",
        "    topk = [w for (w, _) in global_counts.most_common(min(K, len(global_counts)))]\n",
        "    cvs = []\n",
        "    if (n_sents >= 2 and total_tokens >= 10) and topk:\n",
        "        for w in topk:\n",
        "            vec = [c.get(w, 0) for c in per_sent_content]\n",
        "            cvs.append(_cv(vec))\n",
        "    burst_topk_mean_cv = (statistics.mean([v for v in cvs if not math.isnan(v)]) if cvs else float(\"nan\"))\n",
        "    stop_rate = (stop_count / total_tokens) if total_tokens > 0 else float(\"nan\")\n",
        "    content_rate = 1.0 - stop_rate if not math.isnan(stop_rate) else float(\"nan\")\n",
        "    return {\n",
        "        \"n_tokens_nltk\": int(total_tokens),\n",
        "        \"n_sents_nltk\": int(n_sents),\n",
        "        \"mean_sent_len_tok_nltk\": float(mean_len) if not math.isnan(mean_len) else float(\"nan\"),\n",
        "        \"std_sent_len_tok_nltk\": float(std_len) if not math.isnan(std_len) else float(\"nan\"),\n",
        "        \"stopword_rate\": float(stop_rate) if not math.isnan(stop_rate) else float(\"nan\"),\n",
        "        \"content_rate\": float(content_rate) if not math.isnan(content_rate) else float(\"nan\"),\n",
        "        \"hapax_rate\": float(hapax_rate) if not math.isnan(hapax_rate) else float(\"nan\"),\n",
        "        \"function_word_rate_nltk\": float(stop_rate) if not math.isnan(stop_rate) else float(\"nan\"),\n",
        "        \"burstiness_token_cv\": float(burst_cv) if not math.isnan(burst_cv) else float(\"nan\"),\n",
        "        \"burstiness_topk_mean_cv\": float(burst_topk_mean_cv) if not math.isnan(burst_topk_mean_cv) else float(\"nan\"),\n",
        "    }\n",
        "\n",
        "made = {\"doc\": False, \"deltas\": False, \"radar_copied\": 0, \"metadata_updated\": False}\n",
        "\n",
        "# --- (1) doc parquet: only if missing ---\n",
        "if not doc_path.exists():\n",
        "    docs = _gather_docs()\n",
        "    rows = []\n",
        "    for art, ver, vtag, txt in docs.itertuples(index=False):\n",
        "        f = _doc_features(txt)\n",
        "        f.update({\"article_id\": str(art), \"version_id\": int(ver), \"version_tag\": str(vtag), \"doc_id\": f\"{art}-v{ver}\"})\n",
        "        rows.append(f)\n",
        "    df_doc = pd.DataFrame(rows).sort_values([\"article_id\",\"version_id\"])\n",
        "    # dtypes\n",
        "    dtypes = {\n",
        "        \"doc_id\":\"string\",\"article_id\":\"string\",\"version_id\":\"int64\",\"version_tag\":\"string\",\n",
        "        \"n_tokens_nltk\":\"int64\",\"n_sents_nltk\":\"int64\",\n",
        "        \"mean_sent_len_tok_nltk\":\"float64\",\"std_sent_len_tok_nltk\":\"float64\",\n",
        "        \"stopword_rate\":\"float64\",\"content_rate\":\"float64\",\"hapax_rate\":\"float64\",\"function_word_rate_nltk\":\"float64\",\n",
        "        \"burstiness_token_cv\":\"float64\",\"burstiness_topk_mean_cv\":\"float64\",\n",
        "    }\n",
        "    for c,t in dtypes.items():\n",
        "        if c in df_doc.columns: df_doc[c] = df_doc[c].astype(t)\n",
        "    df_doc.to_parquet(doc_path, index=False)\n",
        "    made[\"doc\"] = True\n",
        "else:\n",
        "    df_doc = pd.read_parquet(doc_path)\n",
        "\n",
        "# --- (2) deltas parquet: only if missing (uses doc parquet just built/loaded) ---\n",
        "if not deltas_path.exists():\n",
        "    num_cols = [\n",
        "        \"n_tokens_nltk\",\"n_sents_nltk\",\"mean_sent_len_tok_nltk\",\"std_sent_len_tok_nltk\",\n",
        "        \"stopword_rate\",\"content_rate\",\"hapax_rate\",\"function_word_rate_nltk\",\n",
        "        \"burstiness_token_cv\",\"burstiness_topk_mean_cv\",\n",
        "    ]\n",
        "    delta_rows = []\n",
        "    for art, g in df_doc.groupby(\"article_id\", sort=False):\n",
        "        g = g.sort_values(\"version_id\")\n",
        "        for i in range(len(g)-1):\n",
        "            a, b = g.iloc[i], g.iloc[i+1]\n",
        "            if int(b[\"version_id\"]) - int(a[\"version_id\"]) != 1:\n",
        "                raise AssertionError(f\"Non-adjacent versions for {art}: {a['version_id']}→{b['version_id']}\")\n",
        "            rec = {\"article_id\": str(art), \"from_version\": int(a[\"version_id\"]), \"to_version\": int(b[\"version_id\"])}\n",
        "            for c in num_cols:\n",
        "                rec[f\"delta_{c}\"] = float(b[c]) - float(a[c]) if pd.notna(b[c]) and pd.notna(a[c]) else float(\"nan\")\n",
        "            delta_rows.append(rec)\n",
        "    pd.DataFrame(delta_rows).astype({\"article_id\":\"string\",\"from_version\":\"int64\",\"to_version\":\"int64\"}).to_parquet(deltas_path, index=False)\n",
        "    made[\"deltas\"] = True\n",
        "\n",
        "# --- (3) metadata: merge a few essentials (non-destructive) ---\n",
        "try:\n",
        "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\")) if meta_path.exists() else {}\n",
        "except Exception:\n",
        "    meta = {}\n",
        "per_counts = df_doc.groupby(\"article_id\")[\"version_id\"].nunique()\n",
        "meta.update({\n",
        "    \"module\": \"2\",\n",
        "    \"version_order_source\": meta.get(\"version_order_source\", \"filename_prefix\"),\n",
        "    \"articles\": sorted(df_doc[\"article_id\"].astype(str).unique().tolist()),\n",
        "    \"n_articles\": int(df_doc[\"article_id\"].nunique()),\n",
        "    \"versions_per_article_min\": int(per_counts.min()) if len(per_counts) else 0,\n",
        "    \"versions_per_article_max\": int(per_counts.max()) if len(per_counts) else 0,\n",
        "    \"expected_versions\": meta.get(\"expected_versions\", 4),\n",
        "})\n",
        "meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "made[\"metadata_updated\"] = True\n",
        "\n",
        "# --- (4) filenames: copy radar_overlay_* → stopword_radar_* if needed (no re-render) ---\n",
        "for p in PLOTS.glob(\"radar_overlay_*.png\"):\n",
        "    target = PLOTS / p.name.replace(\"radar_overlay_\", \"stopword_radar_\")\n",
        "    if not target.exists():\n",
        "        shutil.copy2(p, target)\n",
        "        made[\"radar_copied\"] += 1\n",
        "\n",
        "print(json.dumps({\"cell_id\":\"2.4add\",\"status\":\"pass\",\"made\": made}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0i28wLk8SyZ"
      },
      "outputs": [],
      "source": [
        "# cell 2.3b-min — doc-level table + adjacent deltas (+ light metadata)\n",
        "from __future__ import annotations\n",
        "import json, math, statistics\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "OUTDIR = Path(\"outputs/nltk\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reuse tokenizers if present; otherwise load them.\n",
        "try:\n",
        "    STOPWORDS, PUNKT, TB  # from 2.2\n",
        "except NameError:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.data import load as nltk_load\n",
        "    from nltk.tokenize import TreebankWordTokenizer\n",
        "    STOPWORDS = set(stopwords.words(\"english\"))\n",
        "    PUNKT = nltk_load(\"tokenizers/punkt/english.pickle\")\n",
        "    TB = TreebankWordTokenizer()\n",
        "\n",
        "def _cv(vals: List[int|float]) -> float:\n",
        "    vals = [float(v) for v in vals if v is not None]\n",
        "    if not vals: return float(\"nan\")\n",
        "    mu = statistics.mean(vals)\n",
        "    if mu == 0 or len(vals) < 2: return float(\"nan\")\n",
        "    return statistics.pstdev(vals) / mu\n",
        "\n",
        "def _gather_docs_for_doclevel() -> pd.DataFrame:\n",
        "    # Prefer df2_input built in 2.3; else rebuild from df_docs/DOCS.\n",
        "    if \"df2_input\" in globals():\n",
        "        return df2_input[[\"article_id\",\"version_id\",\"version_tag\",\"text_basis\"]].copy()\n",
        "    try:\n",
        "        base = df_docs.copy()\n",
        "    except NameError:\n",
        "        base = pd.DataFrame(DOCS)\n",
        "    if \"article_id\" not in base.columns:\n",
        "        base[\"article_id\"] = (base[\"slug\"] if \"slug\" in base.columns else base.index).astype(str)\n",
        "    if \"version_id\" not in base.columns:\n",
        "        base[\"version_id\"] = 1\n",
        "    if \"version_tag\" not in base.columns:\n",
        "        base[\"version_tag\"] = \"v\" + base[\"version_id\"].astype(str)\n",
        "    for c in (\"text_norm\",\"text_clean\",\"text\"):\n",
        "        if c in base.columns:\n",
        "            base[\"text_basis\"] = base[c]; break\n",
        "    else:\n",
        "        raise RuntimeError(\"2.3b-min: no text column among text_norm/text_clean/text.\")\n",
        "    return base[[\"article_id\",\"version_id\",\"version_tag\",\"text_basis\"]].copy()\n",
        "\n",
        "def _doc_features(text: str) -> Dict[str, Any]:\n",
        "    s_spans = list(PUNKT.span_tokenize(text))\n",
        "    n_sents = len(s_spans)\n",
        "    sent_token_counts, per_sent_content = [], []\n",
        "    stop_count, total_tokens = 0, 0\n",
        "    tokens_alpha = []\n",
        "\n",
        "    for (s, e) in s_spans:\n",
        "        s_txt = text[s:e]\n",
        "        toks = [w for w in TB.tokenize(s_txt)]\n",
        "        total_tokens += len(toks)\n",
        "        sent_token_counts.append(len(toks))\n",
        "        stop_count += sum(1 for w in toks if w.lower() in STOPWORDS)\n",
        "        alpha = [w.lower() for w in toks if w.isalpha()]\n",
        "        tokens_alpha.extend(alpha)\n",
        "        per_sent_content.append(Counter([w for w in alpha if w not in STOPWORDS]))\n",
        "\n",
        "    type_counts = Counter(tokens_alpha)\n",
        "    types_total = len(type_counts)\n",
        "    hapax = sum(1 for _, c in type_counts.items() if c == 1)\n",
        "    hapax_rate = (hapax / types_total) if types_total > 0 else float(\"nan\")\n",
        "\n",
        "    mean_len = statistics.mean(sent_token_counts) if n_sents > 0 else float(\"nan\")\n",
        "    std_len  = statistics.pstdev(sent_token_counts) if n_sents > 1 else float(\"nan\")\n",
        "\n",
        "    burst_cv = _cv(sent_token_counts) if (n_sents >= 2 and total_tokens >= 10) else float(\"nan\")\n",
        "\n",
        "    # CV of per-sentence counts for top-K content tokens (K=20)\n",
        "    K = 20\n",
        "    global_counts = Counter([w for lst in ([list(c.elements()) for c in per_sent_content]) for w in lst])\n",
        "    topk = [w for (w, _) in global_counts.most_common(min(K, len(global_counts)))]\n",
        "    cvs = []\n",
        "    if (n_sents >= 2 and total_tokens >= 10) and topk:\n",
        "        for w in topk:\n",
        "            vec = [c.get(w, 0) for c in per_sent_content]\n",
        "            cvs.append(_cv(vec))\n",
        "    burst_topk_mean_cv = (statistics.mean([v for v in cvs if not math.isnan(v)]) if cvs else float(\"nan\"))\n",
        "\n",
        "    stop_rate = (stop_count / total_tokens) if total_tokens > 0 else float(\"nan\")\n",
        "    content_rate = 1.0 - stop_rate if not math.isnan(stop_rate) else float(\"nan\")\n",
        "\n",
        "    return {\n",
        "        \"n_tokens_nltk\": int(total_tokens),\n",
        "        \"n_sents_nltk\": int(n_sents),\n",
        "        \"mean_sent_len_tok_nltk\": float(mean_len) if not math.isnan(mean_len) else float(\"nan\"),\n",
        "        \"std_sent_len_tok_nltk\": float(std_len) if not math.isnan(std_len) else float(\"nan\"),\n",
        "        \"stopword_rate\": float(stop_rate) if not math.isnan(stop_rate) else float(\"nan\"),\n",
        "        \"content_rate\": float(content_rate) if not math.isnan(content_rate) else float(\"nan\"),\n",
        "        \"hapax_rate\": float(hapax_rate) if not math.isnan(hapax_rate) else float(\"nan\"),\n",
        "        \"function_word_rate_nltk\": float(stop_rate) if not math.isnan(stop_rate) else float(\"nan\"),\n",
        "        \"burstiness_token_cv\": float(burst_cv) if not math.isnan(burst_cv) else float(\"nan\"),\n",
        "        \"burstiness_topk_mean_cv\": float(burst_topk_mean_cv) if not math.isnan(burst_topk_mean_cv) else float(\"nan\"),\n",
        "    }\n",
        "\n",
        "docs_df = _gather_docs_for_doclevel()\n",
        "doc_rows = []\n",
        "for art, ver, vtag, txt in docs_df.itertuples(index=False):\n",
        "    f = _doc_features(txt)\n",
        "    f[\"article_id\"], f[\"version_id\"], f[\"version_tag\"] = str(art), int(ver), str(vtag)\n",
        "    f[\"doc_id\"] = f\"{art}-v{ver}\"\n",
        "    doc_rows.append(f)\n",
        "\n",
        "df_doc = pd.DataFrame(doc_rows).sort_values([\"article_id\",\"version_id\"], kind=\"stable\")\n",
        "# dtypes\n",
        "doc_types = {\n",
        "    \"doc_id\":\"string\",\"article_id\":\"string\",\"version_id\":\"int64\",\"version_tag\":\"string\",\n",
        "    \"n_tokens_nltk\":\"int64\",\"n_sents_nltk\":\"int64\",\n",
        "    \"mean_sent_len_tok_nltk\":\"float64\",\"std_sent_len_tok_nltk\":\"float64\",\n",
        "    \"stopword_rate\":\"float64\",\"content_rate\":\"float64\",\"hapax_rate\":\"float64\",\"function_word_rate_nltk\":\"float64\",\n",
        "    \"burstiness_token_cv\":\"float64\",\"burstiness_topk_mean_cv\":\"float64\",\n",
        "}\n",
        "for c,t in doc_types.items():\n",
        "    if c in df_doc.columns: df_doc[c] = df_doc[c].astype(t)\n",
        "df_doc.to_parquet(OUTDIR / \"fw_burstiness.parquet\", index=False)\n",
        "\n",
        "# adjacent deltas\n",
        "num_cols = [\n",
        "    \"n_tokens_nltk\",\"n_sents_nltk\",\"mean_sent_len_tok_nltk\",\"std_sent_len_tok_nltk\",\n",
        "    \"stopword_rate\",\"content_rate\",\"hapax_rate\",\"function_word_rate_nltk\",\n",
        "    \"burstiness_token_cv\",\"burstiness_topk_mean_cv\",\n",
        "]\n",
        "delta_rows = []\n",
        "for art, g in df_doc.groupby(\"article_id\", sort=False):\n",
        "    g = g.sort_values(\"version_id\")\n",
        "    for i in range(len(g)-1):\n",
        "        a, b = g.iloc[i], g.iloc[i+1]\n",
        "        if int(b[\"version_id\"]) - int(a[\"version_id\"]) != 1:\n",
        "            raise AssertionError(f\"Non-adjacent versions for {art}: {a['version_id']}→{b['version_id']}\")\n",
        "        rec = {\"article_id\": str(art), \"from_version\": int(a[\"version_id\"]), \"to_version\": int(b[\"version_id\"])}\n",
        "        for c in num_cols:\n",
        "            rec[f\"delta_{c}\"] = float(b[c]) - float(a[c]) if pd.notna(b[c]) and pd.notna(a[c]) else float(\"nan\")\n",
        "        delta_rows.append(rec)\n",
        "\n",
        "df_deltas = pd.DataFrame(delta_rows).astype({\"article_id\":\"string\",\"from_version\":\"int64\",\"to_version\":\"int64\"})\n",
        "df_deltas.to_parquet(OUTDIR / \"fw_burstiness_deltas.parquet\", index=False)\n",
        "\n",
        "# minimal metadata enrichment (merge)\n",
        "meta_path = OUTDIR / \"metadata.json\"\n",
        "try:\n",
        "    base = json.loads(meta_path.read_text(encoding=\"utf-8\")) if meta_path.exists() else {}\n",
        "except Exception:\n",
        "    base = {}\n",
        "per_counts = df_doc.groupby(\"article_id\")[\"version_id\"].nunique()\n",
        "base.update({\n",
        "    \"module\": \"2\",\n",
        "    \"version_order_source\": \"filename_prefix\",\n",
        "    \"articles\": sorted(df_doc[\"article_id\"].astype(str).unique().tolist()),\n",
        "    \"n_articles\": int(df_doc[\"article_id\"].nunique()),\n",
        "    \"versions_per_article_min\": int(per_counts.min()) if len(per_counts) else 0,\n",
        "    \"versions_per_article_max\": int(per_counts.max()) if len(per_counts) else 0,\n",
        "    \"expected_versions\": 4,\n",
        "})\n",
        "meta_path.write_text(json.dumps(base, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print({\n",
        "    \"cell_id\":\"2.3b-min\",\n",
        "    \"status\":\"pass\",\n",
        "    \"doc_rows\": int(df_doc.shape[0]),\n",
        "    \"delta_rows\": int(df_deltas.shape[0]),\n",
        "    \"artifacts\": [\"fw_burstiness.parquet\",\"fw_burstiness_deltas.parquet\",\"metadata.json\"]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4z2NhpH8U-o"
      },
      "outputs": [],
      "source": [
        "# cell 2.4fix — only backfill missing per-article plots; do nothing if already present\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if \"df_nltk\" not in globals():\n",
        "    raise RuntimeError(\"2.4fix: df_nltk not found. Run 2.3/2.4 first.\")\n",
        "\n",
        "PLOTS = Path(\"outputs/nltk/plots\"); PLOTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _safe_slug(s: str) -> str:\n",
        "    return re.sub(r\"[^-_.a-zA-Z0-9]\", \"_\", str(s))\n",
        "\n",
        "# global norms for radar (stopword_ratio, sent_len_mean, burstiness_token_cv)\n",
        "cols = [\"stopword_ratio\", \"sent_len_mean\", \"burstiness_token_cv\"]\n",
        "norms = {}\n",
        "for c in cols:\n",
        "    col = pd.to_numeric(df_nltk[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    if col.empty:\n",
        "        norms[c] = (0.0, 1.0)\n",
        "    else:\n",
        "        lo, hi = np.percentile(col.to_numpy(), [5, 95]); hi = lo + 1e-9 if hi == lo else hi\n",
        "        norms[c] = (float(lo), float(hi))\n",
        "\n",
        "made = {\"radar\": 0, \"trend_sr\": 0, \"trend_cv\": 0}\n",
        "for art, g in df_nltk.groupby(\"article_id\", sort=False):\n",
        "    gg = g.sort_values(\"version_id\")\n",
        "    s = _safe_slug(art)\n",
        "\n",
        "    # radar overlay per article (spec filename)\n",
        "    rp = PLOTS / f\"stopword_radar_{s}.png\"\n",
        "    if not rp.exists():\n",
        "        labels = cols\n",
        "        angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist(); angles += angles[:1]\n",
        "        fig = plt.figure(dpi=120); ax = plt.subplot(111, polar=True)\n",
        "        for _, row in gg.iterrows():\n",
        "            vec=[]\n",
        "            for c in labels:\n",
        "                lo, hi = norms[c]; x = float(row[c]) if pd.notna(row[c]) else np.nan\n",
        "                if not np.isfinite(x): vec.append(0.0)\n",
        "                else: vec.append(np.clip((x - lo)/(hi - lo), 0.0, 1.0))\n",
        "            vec += vec[:1]\n",
        "            ax.plot(angles, vec, marker=\"o\", label=f\"v{int(row['version_id'])}\")\n",
        "            ax.fill(angles, vec, alpha=0.05)\n",
        "        ax.set_xticks(angles[:-1]); ax.set_xticklabels(labels)\n",
        "        ax.set_title(f\"Lex profile (overlay by version) — {art}\", pad=12)\n",
        "        ax.set_rlabel_position(0); ax.grid(True, alpha=0.3); ax.legend(loc=\"upper right\", bbox_to_anchor=(1.25,1.10))\n",
        "        fig.savefig(rp, bbox_inches=\"tight\"); plt.close(fig)\n",
        "        made[\"radar\"] += 1\n",
        "\n",
        "    # trends: stopword ratio\n",
        "    p1 = PLOTS / f\"trend_stopword_ratio_{s}.png\"\n",
        "    if not p1.exists():\n",
        "        fig, ax = plt.subplots(dpi=120)\n",
        "        ax.plot(gg[\"version_id\"], gg[\"stopword_ratio\"], marker=\"o\")\n",
        "        ax.set_title(f\"Stopword ratio across versions — {art}\")\n",
        "        ax.set_xlabel(\"version_id\"); ax.set_ylabel(\"stopword_ratio\")\n",
        "        ax.set_xticks(gg[\"version_id\"])\n",
        "        fig.savefig(p1, bbox_inches=\"tight\"); plt.close(fig)\n",
        "        made[\"trend_sr\"] += 1\n",
        "\n",
        "    # trends: burstiness CV\n",
        "    p2 = PLOTS / f\"trend_burstiness_cv_{s}.png\"\n",
        "    if not p2.exists():\n",
        "        fig, ax = plt.subplots(dpi=120)\n",
        "        ax.plot(gg[\"version_id\"], gg[\"burstiness_token_cv\"], marker=\"o\")\n",
        "        ax.set_title(f\"Burstiness (token CV) across versions — {art}\")\n",
        "        ax.set_xlabel(\"version_id\"); ax.set_ylabel(\"burstiness_token_cv\")\n",
        "        ax.set_xticks(gg[\"version_id\"])\n",
        "        fig.savefig(p2, bbox_inches=\"tight\"); plt.close(fig)\n",
        "        made[\"trend_cv\"] += 1\n",
        "\n",
        "# refresh index\n",
        "( PLOTS / \"plots_index.json\" ).write_text(\n",
        "    json.dumps({\"files\": sorted([p.name for p in PLOTS.glob(\"*.png\")])}, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print({\"cell_id\":\"2.4fix\",\"status\":\"pass\",\"made\": made, \"plots_dir\": str(PLOTS)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBs__CpswfZ5"
      },
      "outputs": [],
      "source": [
        "# cell 3.1 — spaCy setup: install, model load, and environment report (CPU-only)\n",
        "# Safe, wheels-first install; fail-fast if model cannot be loaded.\n",
        "\n",
        "import sys, subprocess, json, os\n",
        "from pathlib import Path\n",
        "\n",
        "def _pip(*args: str):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", *args])\n",
        "\n",
        "# Install spacy if missing\n",
        "try:\n",
        "    import spacy  # type: ignore\n",
        "except Exception:\n",
        "    _pip(\"install\", \"-q\", \"--no-cache-dir\", \"--only-binary=:all:\", \"spacy>=3.7,<3.8\")\n",
        "    import spacy  # type: ignore\n",
        "\n",
        "# Try to load the small English pipeline; download if missing\n",
        "def _ensure_en_sm():\n",
        "    try:\n",
        "        return spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
        "    except Exception:\n",
        "        try:\n",
        "            from spacy.cli import download\n",
        "            download(\"en_core_web_sm\")\n",
        "            return spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                \"Module 3: Could not load/download 'en_core_web_sm'. \"\n",
        "                \"Please ensure internet access and rerun this cell.\"\n",
        "            ) from e\n",
        "\n",
        "nlp = _ensure_en_sm()\n",
        "nlp.max_length = int(os.environ.get(\"LSA_SPACY_MAXLEN\", \"2000000\"))  # guard for long docs\n",
        "\n",
        "# Threading: spaCy is already efficient on CPU with single process; expose env toggle\n",
        "NPROC = int(os.environ.get(\"LSA_SPACY_PROCS\", \"1\"))\n",
        "BATCH = int(os.environ.get(\"LSA_SPACY_BATCH\", \"16\"))\n",
        "\n",
        "# Output dirs (Module 3)\n",
        "SP_OUT = Path(\"outputs/spacy\"); (SP_OUT / \"plots\").mkdir(parents=True, exist_ok=True)\n",
        "(SP_OUT / \"bundles\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"3.1\",\n",
        "    \"status\": \"pass\",\n",
        "    \"spacy_version\": spacy.__version__,\n",
        "    \"model\": \"en_core_web_sm\",\n",
        "    \"components_enabled\": [p for p in nlp.pipe_names if p != \"ner\"],\n",
        "    \"nlp_max_length\": nlp.max_length,\n",
        "    \"n_process\": NPROC,\n",
        "    \"batch_size\": BATCH,\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxF2UPWDxbjE"
      },
      "outputs": [],
      "source": [
        "# cell 3.1b — ensure pyarrow present for Parquet IO (safe no-op if already installed)\n",
        "import sys, subprocess\n",
        "try:\n",
        "    import pyarrow # noqa\n",
        "    print({\"cell_id\":\"3.1b\",\"status\":\"pass\",\"pyarrow\":pyarrow.__version__})\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--only-binary=:all:\", \"pyarrow>=14,<16\"])\n",
        "    import pyarrow\n",
        "    print({\"cell_id\":\"3.1b\",\"status\":\"installed\",\"pyarrow\":pyarrow.__version__})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7JQ6KlnwheD"
      },
      "outputs": [],
      "source": [
        "# cell 3.2 — Inputs & ordering: gather documents and Module-2 windows (if present)\n",
        "# Rules:\n",
        "# - Version order comes from numeric version_id (derived from filename prefix 01–04 in earlier modules).\n",
        "# - For windows, prefer the authoritative M2 parquet: outputs/nltk/fw_burstiness_windows.parquet\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Try to reuse df2_input from Module 2; else fall back to df_docs/DOCS (as in Modules 1–2).\n",
        "def _gather_docs():\n",
        "    # Expect columns: article_id, version_id, version_tag, text, text_clean (optional)\n",
        "    if \"df2_input\" in globals():\n",
        "        df = df2_input.copy()\n",
        "        # df2_input guaranteed to have article_id, version_id, version_tag and text_basis/span_basis used in M2; recover raw text columns if present\n",
        "        # If raw text columns are not present, assume text_basis is the preferred doc-level string.\n",
        "        if \"text\" not in df.columns and \"text_clean\" not in df.columns:\n",
        "            df[\"text\"] = df[\"text_basis\"]\n",
        "        return df[[\"article_id\",\"version_id\",\"version_tag\",\"text\",\"text_clean\"]].copy() if \"text_clean\" in df.columns else df[[\"article_id\",\"version_id\",\"version_tag\",\"text\"]].copy()\n",
        "    try:\n",
        "        base = df_docs.copy()\n",
        "    except NameError:\n",
        "        try:\n",
        "            base = pd.DataFrame(DOCS)\n",
        "        except NameError:\n",
        "            raise RuntimeError(\"Module 3: Could not find df2_input / df_docs / DOCS. Please run Modules 1–2 or provide documents.\")\n",
        "    if \"article_id\" not in base.columns:\n",
        "        base[\"article_id\"] = (base[\"slug\"] if \"slug\" in base.columns else base.index).astype(str)\n",
        "    if \"version_id\" not in base.columns:\n",
        "        base[\"version_id\"] = 1\n",
        "    if \"version_tag\" not in base.columns:\n",
        "        base[\"version_tag\"] = \"v\" + base[\"version_id\"].astype(str)\n",
        "    # Ensure at least 'text'\n",
        "    for c in (\"text\",\"text_clean\",\"text_basis\"):\n",
        "        if c in base.columns:\n",
        "            break\n",
        "    else:\n",
        "        raise RuntimeError(\"Module 3: No text-like column found (text/text_clean/text_basis).\")\n",
        "    # Normalize columns\n",
        "    if \"text\" not in base.columns and \"text_basis\" in base.columns:\n",
        "        base[\"text\"] = base[\"text_basis\"]\n",
        "    cols = [\"article_id\",\"version_id\",\"version_tag\",\"text\"] + ([\"text_clean\"] if \"text_clean\" in base.columns else [])\n",
        "    return base[cols].copy()\n",
        "\n",
        "DOCS_DF = _gather_docs()\n",
        "DOCS_DF[\"article_id\"] = DOCS_DF[\"article_id\"].astype(\"string\")\n",
        "DOCS_DF[\"version_id\"] = pd.to_numeric(DOCS_DF[\"version_id\"], errors=\"raise\").astype(\"int64\")\n",
        "DOCS_DF[\"version_tag\"] = DOCS_DF[\"version_tag\"].astype(\"string\")\n",
        "DOCS_DF = DOCS_DF.sort_values([\"article_id\",\"version_id\"], kind=\"stable\").reset_index(drop=True)\n",
        "\n",
        "# Try to load M2 windows (recommended path)\n",
        "M2_WIN_PATH = Path(\"outputs/nltk/fw_burstiness_windows.parquet\")\n",
        "DFW = None\n",
        "if M2_WIN_PATH.exists():\n",
        "    DFW = pd.read_parquet(M2_WIN_PATH)\n",
        "    # Keep only required columns for alignment\n",
        "    need = [\"article_id\",\"version_id\",\"version_tag\",\"doc_id\",\"win_id\",\"win_label\",\n",
        "            \"span_basis\",\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\",\"is_partial_tail\"]\n",
        "    missing = [c for c in need if c not in DFW.columns]\n",
        "    if missing:\n",
        "        raise RuntimeError(f\"Module 3: windows parquet missing required columns: {missing}\")\n",
        "    DFW = DFW[need].copy()\n",
        "    # Normalize dtypes\n",
        "    DFW[\"article_id\"] = DFW[\"article_id\"].astype(\"string\")\n",
        "    DFW[\"version_id\"] = pd.to_numeric(DFW[\"version_id\"], errors=\"raise\").astype(\"int64\")\n",
        "    for c in (\"win_id\",\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\"):\n",
        "        DFW[c] = pd.to_numeric(DFW[c], errors=\"raise\").astype(\"int64\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"3.2\",\n",
        "    \"status\": \"pass\",\n",
        "    \"docs_rows\": int(DOCS_DF.shape[0]),\n",
        "    \"windows_detected\": bool(DFW is not None),\n",
        "    \"windows_rows\": int(DFW.shape[0]) if DFW is not None else 0,\n",
        "    \"version_order_source\": \"filename_prefix (carried via version_id)\"\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DZLy_m7wjca"
      },
      "outputs": [],
      "source": [
        "# cell 3.3 — Discourse markers: ensure lexicon exists and load (greedy multiword-first)\n",
        "# Path: lexicons/discourse_markers_en.txt (one per line). We create a default if missing.\n",
        "\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "\n",
        "LEX_DIR = Path(\"lexicons\"); LEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LEX_PATH = LEX_DIR / \"discourse_markers_en.txt\"\n",
        "\n",
        "_DEFAULT_MARKERS = [\n",
        "    # multiwords first (we'll sort by token count at load time anyway)\n",
        "    \"on the other hand\", \"as a result\", \"for example\", \"for instance\", \"in contrast\",\n",
        "    \"in particular\", \"in summary\", \"in fact\", \"by contrast\", \"in addition\",\n",
        "    \"as a consequence\", \"to the contrary\", \"on the contrary\", \"as a result\",\n",
        "    \"at the same time\", \"in other words\", \"for that reason\",\n",
        "    # single words\n",
        "    \"however\", \"moreover\", \"therefore\", \"meanwhile\", \"nonetheless\",\n",
        "    \"furthermore\", \"consequently\", \"overall\", \"specifically\", \"additionally\"\n",
        "]\n",
        "\n",
        "if not LEX_PATH.exists():\n",
        "    LEX_PATH.write_text(\"\\n\".join(_DEFAULT_MARKERS) + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "def load_markers(path: Path):\n",
        "    items = []\n",
        "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
        "        s = line.strip().lower()\n",
        "        if not s or s.startswith(\"#\"):\n",
        "            continue\n",
        "        items.append(s)\n",
        "    # sort by (token count desc, length desc) for greedy matching\n",
        "    items = sorted(set(items), key=lambda x: (-(len(x.split())), -len(x)))\n",
        "    # precompile regex with word boundaries, case-insensitive\n",
        "    patterns = [(m, re.compile(rf\"\\b{re.escape(m)}\\b\", flags=re.IGNORECASE)) for m in items]\n",
        "    return items, patterns\n",
        "\n",
        "DM_MARKERS, DM_PATTERNS = load_markers(LEX_PATH)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"3.3\",\n",
        "    \"status\": \"pass\",\n",
        "    \"lexicon_path\": str(LEX_PATH),\n",
        "    \"lexicon_size\": len(DM_MARKERS),\n",
        "    \"sample\": DM_MARKERS[:8]\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fazhBb0wmPk"
      },
      "outputs": [],
      "source": [
        "# cell 3.4 — Parsing & sentence-level features (parse each version once, per needed basis)\n",
        "# We parse doc-level basis (text_clean if present else text) AND any span_basis required by M2 windows.\n",
        "\n",
        "import json, math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# ---- basis planning: which strings do we need to parse per (article_id, version_id)?\n",
        "def _doc_basis_for_doclevel(row) -> str:\n",
        "    return \"text_clean\" if \"text_clean\" in row.index and isinstance(row[\"text_clean\"], str) and row[\"text_clean\"] else \"text\"\n",
        "\n",
        "NEEDED_BASIS = set()\n",
        "# Always include the doc-level basis\n",
        "for _, r in DOCS_DF.iterrows():\n",
        "    NEEDED_BASIS.add((_doc_basis_for_doclevel(r),))\n",
        "# Add span_basis from M2 windows if present\n",
        "if DFW is not None:\n",
        "    for b in DFW[\"span_basis\"].dropna().astype(str).unique().tolist():\n",
        "        if b in (\"text\",\"text_clean\"):\n",
        "            NEEDED_BASIS.add((b,))\n",
        "\n",
        "NEEDED_BASIS = {b for (b,) in NEEDED_BASIS}\n",
        "if not NEEDED_BASIS:\n",
        "    NEEDED_BASIS = {\"text\"}\n",
        "\n",
        "# ---- helper: finite predicate detection (English heuristic)\n",
        "FINITE_TAGS = {\"VBD\",\"VBP\",\"VBZ\"}  # common finite verb tags in UD/ptb mapping\n",
        "def is_finite_pred(tok) -> bool:\n",
        "    if tok.pos_ != \"VERB\":\n",
        "        return False\n",
        "    # Exclude auxiliaries unless they are tagged as finite and no main VERB in the sentence.\n",
        "    if tok.pos_ == \"AUX\":\n",
        "        return False\n",
        "    if \"Fin\" in tok.morph.get(\"VerbForm\"):\n",
        "        return True\n",
        "    if tok.tag_ in FINITE_TAGS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "SUBORD_SET = {\"mark\",\"advcl\",\"ccomp\",\"xcomp\",\"acl\",\"relcl\",\"csubj\",\"csubjpass\",\"csubj:pass\"}\n",
        "COORD_SET  = {\"cc\",\"conj\",\"parataxis\"}\n",
        "\n",
        "@dataclass\n",
        "class SentStats:\n",
        "    start_char: int\n",
        "    end_char: int\n",
        "    n_tokens: int\n",
        "    depth: int\n",
        "    clauses: int\n",
        "    coord: int\n",
        "    subord: int\n",
        "\n",
        "@dataclass\n",
        "class ParsedCache:\n",
        "    article_id: str\n",
        "    version_id: int\n",
        "    basis: str  # \"text\" or \"text_clean\"\n",
        "    n_tokens: int\n",
        "    sent_stats: List[SentStats]\n",
        "    text_lower: str  # for discourse marker matching\n",
        "\n",
        "# Compute sentence-level stats for a spaCy Doc\n",
        "def _sentence_stats(doc) -> Tuple[List[SentStats], int]:\n",
        "    stats: List[SentStats] = []\n",
        "    total_tokens = 0\n",
        "    for sent in doc.sents:\n",
        "        toks = [t for t in sent if not t.is_space]\n",
        "        total_tokens += len(toks)\n",
        "        # token depth relative to sentence root\n",
        "        root = sent.root\n",
        "        max_depth = 0\n",
        "        for t in toks:\n",
        "            d = 0\n",
        "            cur = t\n",
        "            # walk up to the sentence root; guard infinite loops\n",
        "            while cur != root and d <= 80:\n",
        "                cur = cur.head\n",
        "                d += 1\n",
        "            if d > max_depth:\n",
        "                max_depth = d\n",
        "        # clause count (finite predicates)\n",
        "        clauses = sum(1 for t in toks if is_finite_pred(t))\n",
        "        # coord/subord counts\n",
        "        subord = 0\n",
        "        for t in toks:\n",
        "            dep = t.dep_.lower()\n",
        "            if dep == \"acl\":\n",
        "                # count as subord if looks like relative clause\n",
        "                if \"Relcl=Yes\".lower() in str(t.morph).lower():\n",
        "                    subord += 1\n",
        "            if dep in SUBORD_SET:\n",
        "                subord += 1\n",
        "        coord = sum(1 for t in toks if t.dep_.lower() in COORD_SET)\n",
        "        stats.append(SentStats(\n",
        "            start_char=sent.start_char,\n",
        "            end_char=sent.end_char,\n",
        "            n_tokens=len(toks),\n",
        "            depth=max_depth,\n",
        "            clauses=clauses,\n",
        "            coord=coord,\n",
        "            subord=subord\n",
        "        ))\n",
        "    return stats, total_tokens\n",
        "\n",
        "# Build parse tasks for required bases\n",
        "parse_tasks = []\n",
        "for (art, ver), g in DOCS_DF.groupby([\"article_id\",\"version_id\"], sort=False):\n",
        "    row = g.iloc[0]\n",
        "    # doc-level basis\n",
        "    b0 = _doc_basis_for_doclevel(row)\n",
        "    parse_tasks.append((art, int(ver), b0, str(row.get(b0) if b0 in row.index else row[\"text\"])))\n",
        "    # any span-basis required by windows\n",
        "    if DFW is not None:\n",
        "        bases_needed = set(DFW[(DFW[\"article_id\"]==art) & (DFW[\"version_id\"]==int(ver))][\"span_basis\"].unique().tolist())\n",
        "        for b in sorted(bases_needed):\n",
        "            if b not in (\"text\",\"text_clean\"):\n",
        "                continue\n",
        "            if b != b0:\n",
        "                parse_tasks.append((art, int(ver), b, str(row.get(b) if b in row.index else row[\"text\"])))\n",
        "\n",
        "# Deduplicate identical (art,ver,basis)\n",
        "seen = set(); unique_tasks = []\n",
        "for art, ver, basis, text in parse_tasks:\n",
        "    key = (art, ver, basis)\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    unique_tasks.append((art, ver, basis, text))\n",
        "\n",
        "# Run nlp.pipe once over all texts\n",
        "docs_stream = (t[3] for t in unique_tasks)\n",
        "parsed = []\n",
        "for (doc, (art, ver, basis, text)) in zip(nlp.pipe(docs_stream, batch_size=BATCH, n_process=NPROC), unique_tasks):\n",
        "    s_stats, ntoks = _sentence_stats(doc)\n",
        "    parsed.append(ParsedCache(\n",
        "        article_id=str(art),\n",
        "        version_id=int(ver),\n",
        "        basis=basis,\n",
        "        n_tokens=int(ntoks),\n",
        "        sent_stats=s_stats,\n",
        "        text_lower=text.lower()\n",
        "    ))\n",
        "\n",
        "# Cache lookup: (article_id, version_id, basis) -> ParsedCache\n",
        "PARSE_CACHE: Dict[Tuple[str,int,str], ParsedCache] = {(p.article_id, p.version_id, p.basis): p for p in parsed}\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"3.4\",\n",
        "    \"status\": \"pass\",\n",
        "    \"parsed_docs\": len(parsed),\n",
        "    \"bases\": sorted(list(NEEDED_BASIS)),\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXxGNvAExNmC"
      },
      "outputs": [],
      "source": [
        "# cell 3.4b — refine clause detector (count finite AUX when no finite VERB), rebuild parse cache\n",
        "import spacy, numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List\n",
        "import statistics\n",
        "\n",
        "# ------------- reuse globals from earlier cells -------------\n",
        "# expects: nlp, DOCS_DF, DFW (or None), DM_PATTERNS, BATCH, NPROC\n",
        "\n",
        "# New clause detector\n",
        "FINITE_TAGS = {\"VBD\",\"VBP\",\"VBZ\"}\n",
        "def is_finite_pred(tok, sent_has_finite_main: bool = True) -> bool:\n",
        "    # Count VERB tokens that are finite\n",
        "    if tok.pos_ == \"VERB\":\n",
        "        if \"Fin\" in tok.morph.get(\"VerbForm\"): return True\n",
        "        if tok.tag_ in FINITE_TAGS: return True\n",
        "        return False\n",
        "    # Count AUX only if sentence has no finite VERB\n",
        "    if tok.pos_ == \"AUX\" and not sent_has_finite_main:\n",
        "        if \"Fin\" in tok.morph.get(\"VerbForm\"): return True\n",
        "        if tok.tag_ in FINITE_TAGS: return True\n",
        "    return False\n",
        "\n",
        "@dataclass\n",
        "class SentStats:\n",
        "    start_char: int\n",
        "    end_char: int\n",
        "    n_tokens: int\n",
        "    depth: int\n",
        "    clauses: int\n",
        "    coord: int\n",
        "    subord: int\n",
        "\n",
        "@dataclass\n",
        "class ParsedCache:\n",
        "    article_id: str\n",
        "    version_id: int\n",
        "    basis: str\n",
        "    n_tokens: int\n",
        "    sent_stats: List[SentStats]\n",
        "    text_lower: str\n",
        "\n",
        "SUBORD_SET = {\"mark\",\"advcl\",\"ccomp\",\"xcomp\",\"acl\",\"relcl\",\"csubj\",\"csubjpass\",\"csubj:pass\"}\n",
        "COORD_SET  = {\"cc\",\"conj\",\"parataxis\"}\n",
        "\n",
        "def _sentence_stats(doc) -> Tuple[List[SentStats], int]:\n",
        "    stats = []\n",
        "    total_tokens = 0\n",
        "    for sent in doc.sents:\n",
        "        toks = [t for t in sent if not t.is_space]\n",
        "        total_tokens += len(toks)\n",
        "        root = sent.root\n",
        "        # depth\n",
        "        max_depth = 0\n",
        "        for t in toks:\n",
        "            d = 0; cur = t\n",
        "            while cur != root and d <= 80:\n",
        "                cur = cur.head; d += 1\n",
        "            if d > max_depth: max_depth = d\n",
        "        # clause logic (finite VERB presence?)\n",
        "        sent_has_finite_main = any(\n",
        "            (tt.pos_ == \"VERB\") and ((\"Fin\" in tt.morph.get(\"VerbForm\")) or (tt.tag_ in FINITE_TAGS))\n",
        "            for tt in toks\n",
        "        )\n",
        "        clauses = sum(1 for t in toks if is_finite_pred(t, sent_has_finite_main))\n",
        "        # coord/subord\n",
        "        subord = sum(1 for t in toks if (t.dep_.lower() in SUBORD_SET) or (t.dep_.lower()==\"acl\" and \"relcl=yes\" in str(t.morph).lower()))\n",
        "        coord  = sum(1 for t in toks if t.dep_.lower() in COORD_SET)\n",
        "        stats.append(SentStats(sent.start_char, sent.end_char, len(toks), max_depth, clauses, coord, subord))\n",
        "    return stats, total_tokens\n",
        "\n",
        "# Rebuild parse tasks as in 3.4\n",
        "def _doc_basis_for_doclevel(row) -> str:\n",
        "    return \"text_clean\" if (\"text_clean\" in row.index and isinstance(row[\"text_clean\"], str) and row[\"text_clean\"]) else \"text\"\n",
        "\n",
        "parse_tasks = []\n",
        "for (art, ver), g in DOCS_DF.groupby([\"article_id\",\"version_id\"], sort=False):\n",
        "    row = g.iloc[0]\n",
        "    b0 = _doc_basis_for_doclevel(row)\n",
        "    parse_tasks.append((art, int(ver), b0, str(row.get(b0) if b0 in row.index else row[\"text\"])))\n",
        "    if DFW is not None:\n",
        "        bases_needed = set(DFW[(DFW[\"article_id\"]==art) & (DFW[\"version_id\"]==int(ver))][\"span_basis\"].unique().tolist())\n",
        "        for b in sorted(bases_needed):\n",
        "            if b in (\"text\",\"text_clean\") and b != b0:\n",
        "                parse_tasks.append((art, int(ver), b, str(row.get(b) if b in row.index else row[\"text\"])))\n",
        "\n",
        "# De-dup\n",
        "seen=set(); unique_tasks=[]\n",
        "for art, ver, basis, text in parse_tasks:\n",
        "    key=(art,ver,basis)\n",
        "    if key in seen: continue\n",
        "    seen.add(key); unique_tasks.append((art,ver,basis,text))\n",
        "\n",
        "# Parse and rebuild cache\n",
        "docs_stream = (t[3] for t in unique_tasks)\n",
        "parsed=[]\n",
        "for (doc, (art,ver,basis,text)) in zip(nlp.pipe(docs_stream, batch_size=BATCH, n_process=NPROC), unique_tasks):\n",
        "    s_stats, ntoks = _sentence_stats(doc)\n",
        "    parsed.append(ParsedCache(str(art), int(ver), basis, int(ntoks), s_stats, text.lower()))\n",
        "PARSE_CACHE = {(p.article_id, p.version_id, p.basis): p for p in parsed}\n",
        "\n",
        "print({\"cell_id\":\"3.4b\",\"status\":\"pass\",\"parsed_docs\":len(parsed)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ILit9VfwqVQ"
      },
      "outputs": [],
      "source": [
        "# cell 3.5 — Document-level metrics (syntax + discourse) and write parquet\n",
        "# Output: outputs/spacy/syntax_discourse.parquet\n",
        "\n",
        "import json, math, statistics\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _doc_dm_counts(text_lower: str) -> Dict[str, Any]:\n",
        "    # Greedy non-overlapping match using precompiled patterns (multiword-first)\n",
        "    used = [False] * len(text_lower)\n",
        "    total = 0\n",
        "    types = set()\n",
        "    for marker, pat in DM_PATTERNS:\n",
        "        for m in pat.finditer(text_lower):\n",
        "            a, b = m.span()\n",
        "            if any(used[a:b]):  # skip overlaps\n",
        "                continue\n",
        "            # mark range used\n",
        "            for i in range(a, b):\n",
        "                used[i] = True\n",
        "            total += 1\n",
        "            types.add(marker)\n",
        "    return {\"dm_total\": total, \"dm_types\": len(types)}\n",
        "\n",
        "def _safe_cv(mean_val: float, std_val: float) -> float:\n",
        "    if mean_val is None or std_val is None:\n",
        "        return float(\"nan\")\n",
        "    if not np.isfinite(mean_val) or mean_val <= 0:\n",
        "        return float(\"nan\")\n",
        "    return float(std_val) / float(mean_val)\n",
        "\n",
        "rows = []\n",
        "for (art, ver), g in DOCS_DF.groupby([\"article_id\",\"version_id\"], sort=False):\n",
        "    row = g.iloc[0]\n",
        "    # choose doc-basis preference: text_clean if available else text\n",
        "    basis = \"text_clean\" if (\"text_clean\" in row.index and isinstance(row[\"text_clean\"], str) and row[\"text_clean\"]) else \"text\"\n",
        "    cache = PARSE_CACHE.get((str(art), int(ver), basis))\n",
        "    if cache is None:\n",
        "        # Fall back to any parsed basis\n",
        "        cache = PARSE_CACHE.get((str(art), int(ver), \"text\")) or PARSE_CACHE.get((str(art), int(ver), \"text_clean\"))\n",
        "        basis = cache.basis if cache else basis\n",
        "\n",
        "    if cache is None:\n",
        "        raise RuntimeError(f\"Module 3: Missing parsed cache for ({art}, v{ver}).\")\n",
        "\n",
        "    sents = cache.sent_stats\n",
        "    n_sents = len(sents)\n",
        "    sent_lengths = [s.n_tokens for s in sents]\n",
        "    depths = [s.depth for s in sents]\n",
        "\n",
        "    mean_len = statistics.mean(sent_lengths) if n_sents > 0 else float(\"nan\")\n",
        "    std_len  = statistics.pstdev(sent_lengths) if n_sents > 1 else float(\"nan\")\n",
        "\n",
        "    depth_mean = statistics.mean(depths) if n_sents > 0 else float(\"nan\")\n",
        "    depth_std  = statistics.pstdev(depths) if n_sents > 1 else float(\"nan\")\n",
        "    depth_med  = (float(np.median(depths)) if n_sents > 0 else float(\"nan\"))\n",
        "    depth_cv   = _safe_cv(depth_mean, depth_std)\n",
        "    depth_p90  = (float(np.percentile(depths, 90)) if n_sents > 0 else float(\"nan\"))\n",
        "    depth_max  = (float(np.max(depths)) if n_sents > 0 else float(\"nan\"))\n",
        "\n",
        "    clauses_ps = [s.clauses for s in sents]\n",
        "    clauses_mean = statistics.mean(clauses_ps) if n_sents > 0 else float(\"nan\")\n",
        "    clauses_med  = (float(np.median(clauses_ps)) if n_sents > 0 else float(\"nan\"))\n",
        "    clauses_max  = (float(np.max(clauses_ps)) if n_sents > 0 else float(\"nan\"))\n",
        "\n",
        "    coord_sum = sum(s.coord for s in sents)\n",
        "    subord_sum = sum(s.subord for s in sents)\n",
        "    doc_tokens = cache.n_tokens if cache.n_tokens > 0 else 1\n",
        "    coord_rate = float(coord_sum) / float(doc_tokens)\n",
        "    subord_rate = float(subord_sum) / float(doc_tokens)\n",
        "    eps = 1e-8\n",
        "    coord_subord_ratio = coord_rate / max(subord_rate, eps)\n",
        "\n",
        "    dm = _doc_dm_counts(cache.text_lower)\n",
        "    dm_total = dm[\"dm_total\"]\n",
        "    dm_types = dm[\"dm_types\"]\n",
        "    dm_density = 100.0 * (dm_total / float(doc_tokens)) if doc_tokens > 0 else float(\"nan\")\n",
        "    dm_ttr = (float(dm_types) / float(dm_total)) if dm_total > 0 else float(\"nan\")\n",
        "\n",
        "    rows.append({\n",
        "        \"article_id\": str(art),\n",
        "        \"version_id\": int(ver),\n",
        "        \"version_tag\": str(g.iloc[0][\"version_tag\"]),\n",
        "        \"doc_id\": f\"{art}-v{ver}\",\n",
        "        \"n_sents_spacy\": int(n_sents),\n",
        "        \"mean_sent_len_tok_spacy\": float(mean_len) if np.isfinite(mean_len) else float(\"nan\"),\n",
        "        \"std_sent_len_tok_spacy\": float(std_len) if np.isfinite(std_len) else float(\"nan\"),\n",
        "        \"depth_mean\": float(depth_mean) if np.isfinite(depth_mean) else float(\"nan\"),\n",
        "        \"depth_median\": float(depth_med) if np.isfinite(depth_med) else float(\"nan\"),\n",
        "        \"depth_std\": float(depth_std) if np.isfinite(depth_std) else float(\"nan\"),\n",
        "        \"depth_cv\": float(depth_cv) if np.isfinite(depth_cv) else float(\"nan\"),\n",
        "        \"depth_p90\": float(depth_p90) if np.isfinite(depth_p90) else float(\"nan\"),\n",
        "        \"depth_max\": float(depth_max) if np.isfinite(depth_max) else float(\"nan\"),\n",
        "        \"clauses_per_sent_mean\": float(clauses_mean) if np.isfinite(clauses_mean) else float(\"nan\"),\n",
        "        \"clauses_per_sent_median\": float(clauses_med) if np.isfinite(clauses_med) else float(\"nan\"),\n",
        "        \"clauses_per_sent_max\": float(clauses_max) if np.isfinite(clauses_max) else float(\"nan\"),\n",
        "        \"coord_rate\": float(coord_rate),\n",
        "        \"subord_rate\": float(subord_rate),\n",
        "        \"coord_subord_ratio\": float(coord_subord_ratio),\n",
        "        \"dm_density_per_100toks\": float(dm_density) if np.isfinite(dm_density) else float(\"nan\"),\n",
        "        \"dm_types\": int(dm_types),\n",
        "        \"dm_type_token_ratio\": float(dm_ttr) if np.isfinite(dm_ttr) else float(\"nan\"),\n",
        "    })\n",
        "\n",
        "DF_DOC3 = pd.DataFrame(rows).sort_values([\"article_id\",\"version_id\"], kind=\"stable\")\n",
        "# Enforce dtypes\n",
        "types = {\n",
        "    \"article_id\":\"string\",\"version_id\":\"int64\",\"version_tag\":\"string\",\"doc_id\":\"string\",\n",
        "    \"n_sents_spacy\":\"int64\",\"mean_sent_len_tok_spacy\":\"float64\",\"std_sent_len_tok_spacy\":\"float64\",\n",
        "    \"depth_mean\":\"float64\",\"depth_median\":\"float64\",\"depth_std\":\"float64\",\"depth_cv\":\"float64\",\"depth_p90\":\"float64\",\"depth_max\":\"float64\",\n",
        "    \"clauses_per_sent_mean\":\"float64\",\"clauses_per_sent_median\":\"float64\",\"clauses_per_sent_max\":\"float64\",\n",
        "    \"coord_rate\":\"float64\",\"subord_rate\":\"float64\",\"coord_subord_ratio\":\"float64\",\n",
        "    \"dm_density_per_100toks\":\"float64\",\"dm_types\":\"int64\",\"dm_type_token_ratio\":\"float64\",\n",
        "}\n",
        "for c,t in types.items():\n",
        "    DF_DOC3[c] = DF_DOC3[c].astype(t)\n",
        "\n",
        "out_path = SP_OUT / \"syntax_discourse.parquet\"\n",
        "DF_DOC3.to_parquet(out_path, index=False)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"3.5\",\n",
        "    \"status\": \"pass\",\n",
        "    \"rows\": int(DF_DOC3.shape[0]),\n",
        "    \"artifact\": str(out_path)\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxONgkl1ws8G"
      },
      "outputs": [],
      "source": [
        "# cell 3.6 — Window-level metrics aligned to Module 2 windows (if available)\n",
        "# Output: outputs/spacy/syntax_discourse_windows.parquet\n",
        "# Policy: include sentences fully contained in [char_start:char_end). Record policy in metadata later.\n",
        "\n",
        "import json, math, statistics\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "if DFW is None:\n",
        "    print(json.dumps({\n",
        "        \"cell_id\": \"3.6\",\n",
        "        \"status\": \"skip\",\n",
        "        \"reason\": \"Module 2 windows not found; only document-level metrics written.\"\n",
        "    }, indent=2))\n",
        "else:\n",
        "    win_rows: List[Dict[str,Any]] = []\n",
        "    for (art, ver), g in DFW.groupby([\"article_id\",\"version_id\"], sort=False):\n",
        "        # Choose basis strings we must use per-window\n",
        "        for _, w in g.sort_values(\"win_id\").iterrows():\n",
        "            basis = str(w[\"span_basis\"]) if str(w[\"span_basis\"]) in (\"text\",\"text_clean\") else \"text\"\n",
        "            cache = PARSE_CACHE.get((str(art), int(ver), basis))\n",
        "            if cache is None:\n",
        "                # fall back to any available basis (shouldn't happen if 3.4 ran)\n",
        "                cache = PARSE_CACHE.get((str(art), int(ver), \"text\")) or PARSE_CACHE.get((str(art), int(ver), \"text_clean\"))\n",
        "                basis = cache.basis if cache else basis\n",
        "            if cache is None:\n",
        "                raise RuntimeError(f\"Module 3: Missing parsed cache for window ({art}, v{ver}, basis={basis}).\")\n",
        "\n",
        "            a, b = int(w[\"char_start\"]), int(w[\"char_end\"])\n",
        "            # sentence selection: full containment\n",
        "            s_in = [s for s in cache.sent_stats if s.start_char >= a and s.end_char <= b]\n",
        "            n_s = len(s_in)\n",
        "            n_toks = sum(s.n_tokens for s in s_in)\n",
        "\n",
        "            depths = [s.depth for s in s_in]\n",
        "            depth_mean = statistics.mean(depths) if n_s > 0 else float(\"nan\")\n",
        "            depth_std  = statistics.pstdev(depths) if n_s > 1 else float(\"nan\")\n",
        "            depth_max  = float(np.max(depths)) if n_s > 0 else float(\"nan\")\n",
        "\n",
        "            clauses_ps = [s.clauses for s in s_in]\n",
        "            clauses_mean = statistics.mean(clauses_ps) if n_s > 0 else float(\"nan\")\n",
        "            clauses_max  = float(np.max(clauses_ps)) if n_s > 0 else float(\"nan\")\n",
        "\n",
        "            coord_sum = sum(s.coord for s in s_in)\n",
        "            subord_sum = sum(s.subord for s in s_in)\n",
        "            denom = max(n_toks, 1)\n",
        "            coord_rate = float(coord_sum) / float(denom)\n",
        "            subord_rate = float(subord_sum) / float(denom)\n",
        "            eps = 1e-8\n",
        "            ratio = coord_rate / max(subord_rate, eps)\n",
        "\n",
        "            # discourse markers in the window substring\n",
        "            # NOTE: Using the same basis string as parsing\n",
        "            # Find the source text (lowercased) that corresponds to basis\n",
        "            text_lower = cache.text_lower\n",
        "            win_text_lower = text_lower[a:b] if 0 <= a <= b <= len(text_lower) else \"\"\n",
        "            used = [False] * len(win_text_lower)\n",
        "            dm_total = 0; dm_types_set = set()\n",
        "            for marker, pat in DM_PATTERNS:\n",
        "                for m in pat.finditer(win_text_lower):\n",
        "                    x, y = m.span()\n",
        "                    if any(used[x:y]):\n",
        "                        continue\n",
        "                    for i in range(x, y):\n",
        "                        used[i] = True\n",
        "                    dm_total += 1\n",
        "                    dm_types_set.add(marker)\n",
        "            dm_types = len(dm_types_set)\n",
        "            dm_density = 100.0 * (dm_total / float(denom)) if denom > 0 else float(\"nan\")\n",
        "            dm_ttr = float(dm_types) / float(dm_total) if dm_total > 0 else float(\"nan\")\n",
        "\n",
        "            win_rows.append({\n",
        "                # keys & spans (mirror M2)\n",
        "                \"article_id\": str(art),\n",
        "                \"version_id\": int(ver),\n",
        "                \"version_tag\": str(w[\"version_tag\"]),\n",
        "                \"doc_id\": str(w[\"doc_id\"]),\n",
        "                \"win_id\": int(w[\"win_id\"]),\n",
        "                \"win_label\": str(w[\"win_label\"]),\n",
        "                \"span_basis\": basis,\n",
        "                \"char_start\": int(w[\"char_start\"]),\n",
        "                \"char_end\": int(w[\"char_end\"]),\n",
        "                \"sent_start_index\": int(w[\"sent_start_index\"]),\n",
        "                \"sent_end_index\": int(w[\"sent_end_index\"]),\n",
        "                \"is_partial_tail\": bool(w[\"is_partial_tail\"]),\n",
        "                # aggregates\n",
        "                \"n_sents_win\": int(n_s),\n",
        "                \"n_tokens_win\": int(n_toks),\n",
        "                \"depth_mean_win\": float(depth_mean) if np.isfinite(depth_mean) else float(\"nan\"),\n",
        "                \"depth_std_win\": float(depth_std) if np.isfinite(depth_std) else float(\"nan\"),\n",
        "                \"depth_max_win\": float(depth_max) if np.isfinite(depth_max) else float(\"nan\"),\n",
        "                \"clauses_per_sent_mean_win\": float(clauses_mean) if np.isfinite(clauses_mean) else float(\"nan\"),\n",
        "                \"clauses_per_sent_max_win\": float(clauses_max) if np.isfinite(clauses_max) else float(\"nan\"),\n",
        "                \"coord_rate_win\": float(coord_rate),\n",
        "                \"subord_rate_win\": float(subord_rate),\n",
        "                \"coord_subord_ratio_win\": float(ratio),\n",
        "                \"dm_density_per_100toks_win\": float(dm_density) if np.isfinite(dm_density) else float(\"nan\"),\n",
        "                \"dm_types_win\": int(dm_types),\n",
        "                \"dm_type_token_ratio_win\": float(dm_ttr) if np.isfinite(dm_ttr) else float(\"nan\"),\n",
        "            })\n",
        "\n",
        "    DF_WIN3 = pd.DataFrame(win_rows).sort_values([\"article_id\",\"version_id\",\"win_id\"], kind=\"stable\")\n",
        "    # Enforce dtypes\n",
        "    types = {\n",
        "        \"article_id\":\"string\",\"version_id\":\"int64\",\"version_tag\":\"string\",\"doc_id\":\"string\",\n",
        "        \"win_id\":\"int64\",\"win_label\":\"string\",\"span_basis\":\"string\",\n",
        "        \"char_start\":\"int64\",\"char_end\":\"int64\",\"sent_start_index\":\"int64\",\"sent_end_index\":\"int64\",\n",
        "        \"is_partial_tail\":\"boolean\",\n",
        "        \"n_sents_win\":\"int64\",\"n_tokens_win\":\"int64\",\n",
        "        \"depth_mean_win\":\"float64\",\"depth_std_win\":\"float64\",\"depth_max_win\":\"float64\",\n",
        "        \"clauses_per_sent_mean_win\":\"float64\",\"clauses_per_sent_max_win\":\"float64\",\n",
        "        \"coord_rate_win\":\"float64\",\"subord_rate_win\":\"float64\",\"coord_subord_ratio_win\":\"float64\",\n",
        "        \"dm_density_per_100toks_win\":\"float64\",\"dm_types_win\":\"int64\",\"dm_type_token_ratio_win\":\"float64\",\n",
        "    }\n",
        "    for c,t in types.items():\n",
        "        DF_WIN3[c] = DF_WIN3[c].astype(t)\n",
        "    out_path_w = SP_OUT / \"syntax_discourse_windows.parquet\"\n",
        "    DF_WIN3.to_parquet(out_path_w, index=False)\n",
        "\n",
        "    print(json.dumps({\n",
        "        \"cell_id\": \"3.6\",\n",
        "        \"status\": \"pass\",\n",
        "        \"rows\": int(DF_WIN3.shape[0]),\n",
        "        \"artifact\": str(out_path_w)\n",
        "    }, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta2haWWBwvn-"
      },
      "outputs": [],
      "source": [
        "# cell 3.7 — Visuals: (a) length vs depth scatter, (b) coord vs subord stacked bars (per article)\n",
        "# Files: outputs/spacy/plots/len_vs_depth_<slug>.png, coord_subord_stack_<slug>.png\n",
        "# Cap at 10 slugs alphabetically.\n",
        "\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "PLOTS = SP_OUT / \"plots\"; PLOTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _safe_slug(s: str) -> str:\n",
        "    return re.sub(r\"[^-_.a-zA-Z0-9]\", \"_\", str(s))\n",
        "\n",
        "# Build a helper map for per-sentence scatter (use parsed doc cache)\n",
        "def _sent_pairs_for(art: str, ver: int, basis: str):\n",
        "    c = PARSE_CACHE.get((art, ver, basis))\n",
        "    if c is None:\n",
        "        # try any basis parsed for this doc\n",
        "        c = PARSE_CACHE.get((art, ver, \"text\")) or PARSE_CACHE.get((art, ver, \"text_clean\"))\n",
        "    if c is None:\n",
        "        return []\n",
        "    return [(s.n_tokens, s.depth) for s in c.sent_stats]\n",
        "\n",
        "# Decide per-article basis for visuals: same doc-level basis we used in 3.5\n",
        "def _basis_for_visual(row) -> str:\n",
        "    return \"text_clean\" if (\"text_clean\" in DOCS_DF.columns and isinstance(row.get(\"text_clean\"), str) and row.get(\"text_clean\")) else \"text\"\n",
        "\n",
        "# Pick up to 10 slugs alphabetically\n",
        "slugs = sorted(DF_DOC3[\"article_id\"].astype(str).unique().tolist())[:10]\n",
        "\n",
        "for art in slugs:\n",
        "    gg = DF_DOC3[DF_DOC3[\"article_id\"]==art].sort_values(\"version_id\")\n",
        "    # (a) length vs depth scatter\n",
        "    fig = plt.figure(dpi=120)\n",
        "    ax = plt.gca()\n",
        "    for _, r in gg.iterrows():\n",
        "        basis = _basis_for_visual(DOCS_DF[(DOCS_DF[\"article_id\"]==art) & (DOCS_DF[\"version_id\"]==r[\"version_id\"])].iloc[0])\n",
        "        pairs = _sent_pairs_for(str(art), int(r[\"version_id\"]), basis)\n",
        "        if pairs:\n",
        "            xs, ys = zip(*pairs)\n",
        "            ax.scatter(xs, ys, alpha=0.35, label=f\"v{int(r['version_id'])}\")\n",
        "    ax.set_title(f\"Sentence length vs depth — {art}\")\n",
        "    ax.set_xlabel(\"Sentence length (tokens, spaCy)\")\n",
        "    ax.set_ylabel(\"Sentence depth (max head distance)\")\n",
        "    ax.legend()\n",
        "    out1 = PLOTS / f\"len_vs_depth_{_safe_slug(art)}.png\"\n",
        "    fig.savefig(out1, bbox_inches=\"tight\"); plt.close(fig)\n",
        "\n",
        "    # (b) coord vs subord stacked bars (per version)\n",
        "    fig = plt.figure(dpi=120); ax = plt.gca()\n",
        "    xs = [f\"v{int(v)}\" for v in gg[\"version_id\"].tolist()]\n",
        "    coord = gg[\"coord_rate\"].to_numpy()\n",
        "    subord = gg[\"subord_rate\"].to_numpy()\n",
        "    ax.bar(xs, subord, label=\"subord_rate\")\n",
        "    ax.bar(xs, coord, bottom=subord, label=\"coord_rate\")\n",
        "    ax.set_title(f\"Coordination vs Subordination — {art}\")\n",
        "    ax.set_ylabel(\"rate per token\")\n",
        "    ax.legend()\n",
        "    out2 = PLOTS / f\"coord_subord_stack_{_safe_slug(art)}.png\"\n",
        "    fig.savefig(out2, bbox_inches=\"tight\"); plt.close(fig)\n",
        "\n",
        "# Update plots index\n",
        "( PLOTS / \"plots_index.json\" ).write_text(\n",
        "    json.dumps({\"files\": sorted([p.name for p in PLOTS.glob(\"*.png\")])}, indent=2),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"3.7\",\n",
        "    \"status\": \"pass\",\n",
        "    \"plotted_articles\": len(slugs),\n",
        "    \"plots_dir\": str(PLOTS)\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZscCaJuwxl-"
      },
      "outputs": [],
      "source": [
        "# cell 3.8 — Metadata (Module 3) and write to outputs/spacy/metadata.json\n",
        "# Records pipeline, versioning, window alignment policy, lexicon info, and basic counts.\n",
        "\n",
        "import json, sys\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "META_PATH = SP_OUT / \"metadata.json\"\n",
        "\n",
        "per_counts = DF_DOC3.groupby(\"article_id\")[\"version_id\"].nunique()\n",
        "meta_update = {\n",
        "    \"module\": \"3\",\n",
        "    \"version_order_source\": \"filename_prefix\",\n",
        "    \"spacy_version\": spacy.__version__,\n",
        "    \"model\": \"en_core_web_sm\",\n",
        "    \"components_enabled\": [p for p in nlp.pipe_names if p != \"ner\"],\n",
        "    \"articles\": sorted(DF_DOC3[\"article_id\"].astype(str).unique().tolist()),\n",
        "    \"n_articles\": int(DF_DOC3[\"article_id\"].nunique()),\n",
        "    \"versions_per_article_min\": int(per_counts.min()) if len(per_counts) else 0,\n",
        "    \"versions_per_article_max\": int(per_counts.max()) if len(per_counts) else 0,\n",
        "    \"expected_versions\": 4,\n",
        "    \"window_alignment_policy\": \"from_module_2_spans\" if DFW is not None else \"not_applicable\",\n",
        "    \"alignment_rule\": \"full_containment\",  # sentences fully within [char_start:char_end)\n",
        "    \"epsilon_ratio\": 1e-8,\n",
        "    \"lexicon_path\": str(LEX_PATH),\n",
        "    \"lexicon_size\": len(DM_MARKERS),\n",
        "    \"plotted_articles_capped\": (DF_DOC3['article_id'].nunique() > 10),\n",
        "}\n",
        "\n",
        "# Merge (do not overwrite other module metadata if present)\n",
        "base = {}\n",
        "if META_PATH.exists():\n",
        "    try:\n",
        "        base = json.loads(META_PATH.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        base = {}\n",
        "base.update(meta_update)\n",
        "META_PATH.write_text(json.dumps(base, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"3.8\",\n",
        "    \"status\": \"pass\",\n",
        "    \"metadata_path\": str(META_PATH)\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n943E_fnxjko"
      },
      "outputs": [],
      "source": [
        "# cell 3.8a — enrich metadata with windows_available, basis map, and lexicon SHA256\n",
        "import json, hashlib\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "def _basis_for_doc(row):\n",
        "    return \"text_clean\" if (\"text_clean\" in row.index and isinstance(row[\"text_clean\"], str) and row[\"text_clean\"]) else \"text\"\n",
        "\n",
        "basis_map = {\n",
        "    str(art): _basis_for_doc(DOCS_DF[(DOCS_DF[\"article_id\"]==art) & (DOCS_DF[\"version_id\"]==ver)].iloc[0])\n",
        "    for (art, ver) in DF_DOC3[[\"article_id\",\"version_id\"]].itertuples(index=False, name=None)\n",
        "}\n",
        "# windows flag\n",
        "windows_available = DFW is not None\n",
        "\n",
        "# lexicon hash\n",
        "h = hashlib.sha256()\n",
        "with open(LEX_PATH, \"rb\") as f:\n",
        "    for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "        h.update(chunk)\n",
        "lex_sha = h.hexdigest()\n",
        "\n",
        "META_PATH = SP_OUT / \"metadata.json\"\n",
        "base = {}\n",
        "if META_PATH.exists():\n",
        "    try:\n",
        "        base = json.loads(META_PATH.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        base = {}\n",
        "base.update({\n",
        "    \"windows_available\": bool(windows_available),\n",
        "    \"document_basis_by_article\": basis_map,\n",
        "    \"lexicon_sha256\": lex_sha,\n",
        "})\n",
        "META_PATH.write_text(json.dumps(base, indent=2), encoding=\"utf-8\")\n",
        "print({\"cell_id\":\"3.8a\",\"status\":\"pass\",\"windows_available\":windows_available,\"lexicon_sha256\":lex_sha[:12]+\"...\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM7Mf28VMMhl"
      },
      "outputs": [],
      "source": [
        "# cell 3.8b — metadata add-ons (label sets, DM match policy, sentence-boundary disagreement diagnostic)\n",
        "# Place AFTER 3.8. Safe to re-run. Requires: SP_OUT, DFW (or None), PARSE_CACHE, LEX_PATH, DM_PATTERNS.\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "META_PATH = SP_OUT / \"metadata.json\"\n",
        "base = {}\n",
        "if META_PATH.exists():\n",
        "    try:\n",
        "        base = json.loads(META_PATH.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        base = {}\n",
        "\n",
        "# 1) Document the exact label sets and DM matching policy\n",
        "coord_labels_used  = [\"cc\", \"conj\", \"parataxis\"]\n",
        "subord_labels_used = [\"mark\", \"advcl\", \"ccomp\", \"xcomp\", \"acl\", \"relcl\", \"csubj\", \"csubj:pass\"]\n",
        "dm_match_policy    = \"lowercase, multiword-first, word-boundary, non-overlapping\"\n",
        "\n",
        "# 2) Sentence-boundary disagreement diagnostic (window-level)\n",
        "#    We count a window as 'disagreement' if ANY spaCy sentence partially overlaps the window span.\n",
        "def _has_partial_overlap(sent_span, win_span):\n",
        "    (sa, sb), (wa, wb) = sent_span, win_span\n",
        "    if sa >= wb or sb <= wa:  # no overlap\n",
        "        return False\n",
        "    fully = (sa >= wa) and (sb <= wb)\n",
        "    return not fully  # partial overlap → True\n",
        "\n",
        "disagree_windows = 0\n",
        "total_windows = 0\n",
        "\n",
        "if DFW is not None and len(DFW):\n",
        "    for (_, _ver), g in DFW.groupby([\"article_id\", \"version_id\"], sort=False):\n",
        "        for _, w in g.iterrows():\n",
        "            basis = str(w[\"span_basis\"]) if str(w[\"span_basis\"]) in (\"text\",\"text_clean\") else \"text\"\n",
        "            cache = PARSE_CACHE.get((str(w[\"article_id\"]), int(w[\"version_id\"]), basis))\n",
        "            if cache is None:\n",
        "                cache = PARSE_CACHE.get((str(w[\"article_id\"]), int(w[\"version_id\"]), \"text\")) or \\\n",
        "                        PARSE_CACHE.get((str(w[\"article_id\"]), int(w[\"version_id\"]), \"text_clean\"))\n",
        "            total_windows += 1\n",
        "            if cache is None:\n",
        "                # Cannot assess; treat as neutral (not disagreement) but note later.\n",
        "                continue\n",
        "            wa, wb = int(w[\"char_start\"]), int(w[\"char_end\"])\n",
        "            any_partial = any(_has_partial_overlap((s.start_char, s.end_char), (wa, wb)) for s in cache.sent_stats)\n",
        "            if any_partial:\n",
        "                disagree_windows += 1\n",
        "\n",
        "disagree_rate = (disagree_windows / total_windows) if total_windows else None\n",
        "\n",
        "# 3) Merge into metadata\n",
        "base.update({\n",
        "    \"coord_labels_used\": coord_labels_used,\n",
        "    \"subord_labels_used\": subord_labels_used,\n",
        "    \"dm_match\": dm_match_policy,\n",
        "    \"sent_boundary_disagreement_policy\": \"count window if any spaCy sentence partially overlaps\",\n",
        "    \"sent_boundary_disagreement_rate_windows\": (float(disagree_rate) if disagree_rate is not None else None),\n",
        "})\n",
        "\n",
        "META_PATH.write_text(json.dumps(base, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print({\n",
        "    \"cell_id\": \"3.8b\",\n",
        "    \"status\": \"pass\",\n",
        "    \"windows_checked\": int(total_windows),\n",
        "    \"windows_with_partial_sentence\": int(disagree_windows),\n",
        "    \"disagreement_rate\": None if disagree_rate is None else round(disagree_rate, 4),\n",
        "    \"metadata_path\": str(META_PATH),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PcdtqfeMSpS"
      },
      "outputs": [],
      "source": [
        "# cell 3.9 — acceptance & schema audit (writes outputs/spacy/audit.json)\n",
        "# Place BEFORE 3.Z. Safe to re-run. Validates required columns and basic window invariants.\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "AUDIT_PATH = SP_OUT / \"audit.json\"\n",
        "\n",
        "results = {\"cell_id\": \"3.9\", \"status\": \"pass\", \"checks\": {}}\n",
        "\n",
        "# --- Load artifacts\n",
        "doc_path = SP_OUT / \"syntax_discourse.parquet\"\n",
        "win_path = SP_OUT / \"syntax_discourse_windows.parquet\"\n",
        "\n",
        "df_doc = pd.read_parquet(doc_path) if doc_path.exists() else None\n",
        "df_win = pd.read_parquet(win_path) if win_path.exists() else None\n",
        "\n",
        "# --- Expected schemas per spec\n",
        "exp_doc_cols = [\n",
        "    \"article_id\",\"version_id\",\"version_tag\",\"doc_id\",\n",
        "    \"n_sents_spacy\",\"mean_sent_len_tok_spacy\",\"std_sent_len_tok_spacy\",\n",
        "    \"depth_mean\",\"depth_median\",\"depth_std\",\"depth_cv\",\"depth_p90\",\"depth_max\",\n",
        "    \"clauses_per_sent_mean\",\"clauses_per_sent_median\",\"clauses_per_sent_max\",\n",
        "    \"coord_rate\",\"subord_rate\",\"coord_subord_ratio\",\n",
        "    \"dm_density_per_100toks\",\"dm_types\",\"dm_type_token_ratio\",\n",
        "]\n",
        "exp_win_cols = [\n",
        "    \"article_id\",\"version_id\",\"version_tag\",\"doc_id\",\"win_id\",\"win_label\",\n",
        "    \"span_basis\",\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\",\"is_partial_tail\",\n",
        "    \"n_sents_win\",\"n_tokens_win\",\n",
        "    \"depth_mean_win\",\"depth_std_win\",\"depth_max_win\",\n",
        "    \"clauses_per_sent_mean_win\",\"clauses_per_sent_max_win\",\n",
        "    \"coord_rate_win\",\"subord_rate_win\",\"coord_subord_ratio_win\",\n",
        "    \"dm_density_per_100toks_win\",\"dm_types_win\",\"dm_type_token_ratio_win\",\n",
        "]\n",
        "\n",
        "def _schema_report(df, expected):\n",
        "    if df is None:\n",
        "        return {\"present\": False, \"missing\": expected, \"extra\": []}\n",
        "    cols = list(map(str, df.columns))\n",
        "    missing = [c for c in expected if c not in cols]\n",
        "    extra   = [c for c in cols if c not in expected]\n",
        "    return {\"present\": True, \"missing\": missing, \"extra\": extra}\n",
        "\n",
        "results[\"checks\"][\"doc_schema\"] = _schema_report(df_doc, exp_doc_cols)\n",
        "results[\"checks\"][\"win_schema\"] = _schema_report(df_win, exp_win_cols)\n",
        "\n",
        "# --- Doc-level numeric sanity\n",
        "if df_doc is not None and len(df_doc):\n",
        "    depth_nonneg = bool((df_doc[\"depth_mean\"].fillna(0) >= 0).all() and (df_doc[\"depth_max\"].fillna(0) >= 0).all())\n",
        "    cv_ok = bool(((df_doc[\"depth_cv\"].isna()) | (np.isfinite(df_doc[\"depth_cv\"]))).all())\n",
        "    ratio_ok = bool(np.isfinite(df_doc[\"coord_subord_ratio\"].to_numpy(dtype=float)).all())\n",
        "    results[\"checks\"][\"doc_values\"] = {\n",
        "        \"depth_nonnegative\": depth_nonneg,\n",
        "        \"depth_cv_finite_or_nan\": cv_ok,\n",
        "        \"coord_subord_ratio_finite\": ratio_ok,\n",
        "        \"rows\": int(df_doc.shape[0]),\n",
        "    }\n",
        "\n",
        "# --- Window invariants & monotonicity\n",
        "if df_win is not None and len(df_win):\n",
        "    inv = {\"groups_checked\": 0, \"win_id_contiguous\": True, \"char_spans_monotonic\": True}\n",
        "    for (art, ver), g in df_win.groupby([\"article_id\",\"version_id\"], sort=False):\n",
        "        inv[\"groups_checked\"] += 1\n",
        "        g = g.sort_values(\"win_id\")\n",
        "        # contiguous win_id starting at 1\n",
        "        expected_seq = list(range(1, len(g) + 1))\n",
        "        got_seq = g[\"win_id\"].tolist()\n",
        "        if got_seq != expected_seq:\n",
        "            inv[\"win_id_contiguous\"] = False\n",
        "        # monotonic char_start/char_end\n",
        "        cs = g[\"char_start\"].to_numpy()\n",
        "        ce = g[\"char_end\"].to_numpy()\n",
        "        if not (np.all(cs[:-1] <= cs[1:]) and np.all(ce[:-1] <= ce[1:]) and np.all(ce - cs >= 0)):\n",
        "            inv[\"char_spans_monotonic\"] = False\n",
        "    results[\"checks\"][\"windows_invariants\"] = inv\n",
        "\n",
        "# --- Write audit & print summary\n",
        "AUDIT_PATH.write_text(json.dumps(results, indent=2), encoding=\"utf-8\")\n",
        "print(json.dumps(results, indent=2))\n",
        "print({\"audit_path\": str(AUDIT_PATH)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFTubl2s9TlE"
      },
      "outputs": [],
      "source": [
        "# 4.1 — transformers|torch: install (CPU by default)\n",
        "# Safe, wheels-first installs. CPU torch so this runs anywhere; GPU will be picked up in 4.2 if available.\n",
        "import sys, subprocess, pkgutil\n",
        "\n",
        "def _pip(args):\n",
        "    return subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *args])\n",
        "\n",
        "# Install torch (CPU wheel), transformers 4.x, accelerate\n",
        "need = []\n",
        "if pkgutil.find_loader(\"torch\") is None:\n",
        "    need += [\"torch==2.*\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"]\n",
        "if pkgutil.find_loader(\"transformers\") is None:\n",
        "    need += [\"transformers>=4.40,<5\"]\n",
        "if pkgutil.find_loader(\"accelerate\") is None:\n",
        "    need += [\"accelerate>=0.30,<1\"]\n",
        "\n",
        "if need:\n",
        "    print(\"Installing:\", need)\n",
        "    _pip(need)\n",
        "else:\n",
        "    print(\"✓ torch/transformers/accelerate already available\")\n",
        "\n",
        "# NLTK punkt may be used for sentence splits to stay aligned with Module 2\n",
        "try:\n",
        "    import nltk\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except Exception:\n",
        "    _pip([\"nltk>=3.8,<4\"])\n",
        "    import nltk\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "print(\"✓ 4.1 complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaWX4hvC9VMd"
      },
      "outputs": [],
      "source": [
        "# 4.2 — transformers: load tokenizer/model (distilgpt2) [CPU/GPU auto]\n",
        "from pathlib import Path\n",
        "import os, json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Respect earlier foundations; remain portable if not set\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "OUT_DIR  = (BASE_DIR / \"outputs\" / \"transformers\").resolve()\n",
        "PLOTS_DIR = OUT_DIR / \"plots\"\n",
        "for p in (OUT_DIR, PLOTS_DIR):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = os.environ.get(\"LSA_PPL_MODEL\", \"distilgpt2\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() and os.environ.get(\"LSA_ALLOW_CUDA\",\"1\")==\"1\" else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "# causal LM needs a pad token; use EOS for left padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model.eval().to(DEVICE)\n",
        "\n",
        "meta = {\n",
        "    \"version_order_source\": \"filename_prefix\",\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"torch_version\": torch.__version__,\n",
        "    \"transformers_version\": __import__(\"transformers\").__version__,\n",
        "    \"device\": DEVICE,\n",
        "}\n",
        "with open(OUT_DIR / \"metadata.json\", \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "print(\"✓ loaded\", MODEL_NAME, \"on\", DEVICE, \"→ metadata.json updated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlHGwQi8JZjb"
      },
      "outputs": [],
      "source": [
        "# 4.2a — Module 4 health check (confirms model runs and reports a tiny PPL)\n",
        "import os, math, json, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = os.environ.get(\"LSA_PPL_MODEL\", \"distilgpt2\")\n",
        "device = \"cuda\" if torch.cuda.is_available() and os.environ.get(\"LSA_ALLOW_CUDA\",\"1\")==\"1\" else \"cpu\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"left\"\n",
        "mdl = AutoModelForCausalLM.from_pretrained(model_name).eval().to(device)\n",
        "\n",
        "txt = \"This is a quick check.\"\n",
        "enc = tok(txt, return_tensors=\"pt\")\n",
        "ids, attn = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
        "labels = ids.clone(); labels[attn==0] = -100\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = mdl(ids.to(device), attention_mask=attn.to(device), labels=labels.to(device))\n",
        "nll = float(out.loss.detach().cpu())\n",
        "ppl = float(math.exp(nll))\n",
        "\n",
        "print(json.dumps({\n",
        "  \"device\": device,\n",
        "  \"torch_version\": torch.__version__,\n",
        "  \"model\": model_name,\n",
        "  \"sample_nll\": round(nll, 4),\n",
        "  \"sample_ppl\": round(ppl, 2)\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmgPW-uHOKzs"
      },
      "outputs": [],
      "source": [
        "# 4.2a — quick health check: confirm model runs on a tiny string\n",
        "import os, math, json, torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = os.environ.get(\"LSA_PPL_MODEL\", \"distilgpt2\")\n",
        "device = \"cuda\" if torch.cuda.is_available() and os.environ.get(\"LSA_ALLOW_CUDA\",\"1\")==\"1\" else \"cpu\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tok.pad_token = tok.eos_token; tok.padding_side = \"left\"\n",
        "mdl = AutoModelForCausalLM.from_pretrained(model_name).eval().to(device)\n",
        "\n",
        "txt = \"This is a quick check.\"\n",
        "enc = tok(txt, return_tensors=\"pt\")\n",
        "ids, attn = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
        "labels = ids.clone(); labels[attn==0] = -100\n",
        "with torch.no_grad():\n",
        "    out = mdl(ids.to(device), attention_mask=attn.to(device), labels=labels.to(device))\n",
        "nll = float(out.loss.detach().cpu()); ppl = float(math.exp(nll))\n",
        "print(json.dumps({\"device\": device, \"torch\": torch.__version__, \"model\": model_name,\n",
        "                  \"sample_nll\": round(nll,4), \"sample_ppl\": round(ppl,2)}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-72zEXlo9XGA"
      },
      "outputs": [],
      "source": [
        "# 4.3 — transformers: sentence pseudo-perplexity (doc + Module-2 windows)\n",
        "import os, math, json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import nltk\n",
        "\n",
        "# ---------- Foundations / inputs ----------\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "OUT_DIR  = (BASE_DIR / \"outputs\" / \"transformers\").resolve()\n",
        "PLOTS_DIR = OUT_DIR / \"plots\"\n",
        "for p in (OUT_DIR, PLOTS_DIR):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Try to reuse in-memory DOCS from previous modules; else fall back to df_docs; last resort scan input_docs\n",
        "def _discover_docs() -> List[Dict]:\n",
        "    # Expect DOCS list of dicts with article_id, version_id, version_tag, doc_id, text[, text_clean]\n",
        "    if \"DOCS\" in globals() and isinstance(globals()[\"DOCS\"], list):\n",
        "        return globals()[\"DOCS\"]\n",
        "    if \"df_docs\" in globals():\n",
        "        rows = globals()[\"df_docs\"].to_dict(\"records\")\n",
        "        return rows\n",
        "    # Fallback: scan SOURCE_DIR for 01..04 files (basic)\n",
        "    SOURCE_DIR = Path(os.environ.get(\"LSA_SOURCE_DIR\", BASE_DIR / \"input_docs\")).resolve()\n",
        "    if not SOURCE_DIR.exists():\n",
        "        raise FileNotFoundError(f\"No DOCS in memory and SOURCE_DIR missing: {SOURCE_DIR}\")\n",
        "    files = sorted([p for p in SOURCE_DIR.iterdir() if p.is_file()])\n",
        "    docs = []\n",
        "    for p in files:\n",
        "        name = p.stem\n",
        "        # crude parse \"01-title...\" → version_id=1, article_id=name without prefix after hyphen\n",
        "        try:\n",
        "            v = int(name[:2])\n",
        "        except Exception:\n",
        "            continue\n",
        "        art = name[3:] if len(name) > 3 and name[2] in (\"-\",\"_\") else name\n",
        "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        docs.append({\n",
        "            \"article_id\": art,\n",
        "            \"version_id\": v,\n",
        "            \"version_tag\": f\"v{v}\",\n",
        "            \"doc_id\": f\"{art}__v{v}\",\n",
        "            \"text\": txt\n",
        "        })\n",
        "    if not docs:\n",
        "        raise RuntimeError(\"Could not discover any documents.\")\n",
        "    return docs\n",
        "\n",
        "DOCS = _discover_docs()\n",
        "\n",
        "# Enforce filename-prefix ordering (01..04)\n",
        "DOCS = sorted(DOCS, key=lambda d: (str(d.get(\"article_id\",\"\")), int(d.get(\"version_id\", 0))))\n",
        "\n",
        "# Load Module-2 windows (optional but preferred)\n",
        "M2_WIN_PATH = BASE_DIR / \"outputs\" / \"nltk\" / \"fw_burstiness_windows.parquet\"\n",
        "DF_WINS = None\n",
        "if M2_WIN_PATH.exists():\n",
        "    DF_WINS = pd.read_parquet(M2_WIN_PATH)\n",
        "    # basic sanity\n",
        "    for c in [\"article_id\",\"version_id\",\"win_id\",\"span_basis\",\"char_start\",\"char_end\",\"win_label\"]:\n",
        "        if c not in DF_WINS.columns:\n",
        "            DF_WINS = None\n",
        "            break\n",
        "\n",
        "# ---------- Sentence splitting (NLTK Punkt to match Module 2) ----------\n",
        "_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "def sent_spans(text: str) -> List[Tuple[int,int]]:\n",
        "    return list(_tokenizer.span_tokenize(text))\n",
        "\n",
        "def split_sents_by_spans(text: str, spans: List[Tuple[int,int]]) -> List[str]:\n",
        "    return [text[a:b] for (a,b) in spans]\n",
        "\n",
        "# ---------- PPL / NLL per sentence ----------\n",
        "MODEL_NAME = os.environ.get(\"LSA_PPL_MODEL\", \"distilgpt2\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() and os.environ.get(\"LSA_ALLOW_CUDA\",\"1\")==\"1\" else \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).eval().to(DEVICE)\n",
        "\n",
        "MAX_BPE = int(os.environ.get(\"LSA_PPL_MAX_BPE\", \"128\"))\n",
        "BATCH = int(os.environ.get(\"LSA_PPL_BATCH\", \"8\"))\n",
        "\n",
        "def ppl_on_sentences(sents: List[str]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    if not sents:\n",
        "        return pd.DataFrame(rows, columns=[\"sent\",\"nll\",\"ppl\",\"bpe_len\"])\n",
        "    # chunked batches\n",
        "    for i in range(0, len(sents), BATCH):\n",
        "        batch = sents[i:i+BATCH]\n",
        "        enc = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_BPE\n",
        "        )\n",
        "        input_ids = enc[\"input_ids\"]\n",
        "        attn = enc[\"attention_mask\"]\n",
        "        # mask pad labels\n",
        "        labels = input_ids.clone()\n",
        "        labels[attn == 0] = -100\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids.to(DEVICE), attention_mask=attn.to(DEVICE), labels=labels.to(DEVICE))\n",
        "            # HF returns mean loss over non-ignored tokens for each sample when reduction='mean'\n",
        "            # We recompute token-level losses per sample by normalizing with valid token counts\n",
        "            # But .loss is averaged over batch; so use per-token NLL via logits\n",
        "            # Simpler, acceptable proxy: use out.loss per batch + per-sample valid lengths → approximate equally.\n",
        "            # For exact per-sample NLL, run loop:\n",
        "            logits = out.logits.detach().cpu()\n",
        "        # Per-sample exact NLL/token\n",
        "        for b in range(input_ids.size(0)):\n",
        "            ids = input_ids[b].cpu()\n",
        "            mask = attn[b].cpu()\n",
        "            # shift for causal LM\n",
        "            target = ids.clone()\n",
        "            target[mask==0] = -100\n",
        "            # Compute token log-probs for each position\n",
        "            with torch.no_grad():\n",
        "                # logits[b, :-1] predicts ids[1:]\n",
        "                log_probs = torch.nn.functional.log_softmax(logits[b, :-1], dim=-1)\n",
        "                tgt = ids[1:]\n",
        "                m = mask[1:]\n",
        "                valid = m.nonzero(as_tuple=False).squeeze(-1)\n",
        "                if valid.numel() < 3:  # skip ultra-short sentences\n",
        "                    nll = math.nan\n",
        "                    ppl = math.nan\n",
        "                    used = int(valid.numel())\n",
        "                else:\n",
        "                    sel_logits = log_probs[valid, tgt[valid]]\n",
        "                    nll = float(-sel_logits.mean().item())\n",
        "                    ppl = float(math.exp(nll))\n",
        "                    used = int(valid.numel())\n",
        "            rows.append({\n",
        "                \"sent\": batch[b],\n",
        "                \"nll\": nll,\n",
        "                \"ppl\": ppl,\n",
        "                \"bpe_len\": used\n",
        "            })\n",
        "        # free\n",
        "        del logits, enc, input_ids, attn, labels\n",
        "        torch.cuda.empty_cache() if DEVICE==\"cuda\" else None\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "# ---------- Aggregations ----------\n",
        "def agg_stats(vals: pd.Series, prefix: str) -> Dict[str, float]:\n",
        "    a = np.array(vals.dropna().tolist(), dtype=float)\n",
        "    if a.size == 0:\n",
        "        return {f\"{prefix}_mean\": np.nan, f\"{prefix}_median\": np.nan,\n",
        "                f\"{prefix}_p25\": np.nan, f\"{prefix}_p75\": np.nan,\n",
        "                f\"{prefix}_p90\": np.nan, f\"{prefix}_max\": np.nan,\n",
        "                f\"{prefix}_std\": np.nan, f\"{prefix}_cv\": np.nan}\n",
        "    dd = {\n",
        "        f\"{prefix}_mean\": float(np.mean(a)),\n",
        "        f\"{prefix}_median\": float(np.median(a)),\n",
        "        f\"{prefix}_p25\": float(np.percentile(a, 25)),\n",
        "        f\"{prefix}_p75\": float(np.percentile(a, 75)),\n",
        "        f\"{prefix}_p90\": float(np.percentile(a, 90)),\n",
        "        f\"{prefix}_max\": float(np.max(a)),\n",
        "        f\"{prefix}_std\": float(np.std(a, ddof=0)),\n",
        "    }\n",
        "    dd[f\"{prefix}_cv\"] = float(dd[f\"{prefix}_std\"]/dd[f\"{prefix}_mean\"]) if dd[f\"{prefix}_mean\"] not in (0.0, np.nan) else np.nan\n",
        "    return dd\n",
        "\n",
        "# Collect per-version rows\n",
        "ver_rows = []\n",
        "win_rows = []\n",
        "\n",
        "for doc in DOCS:\n",
        "    art = str(doc[\"article_id\"])\n",
        "    vid = int(doc[\"version_id\"])\n",
        "    vtag = str(doc.get(\"version_tag\", f\"v{vid}\"))\n",
        "    did  = str(doc.get(\"doc_id\", f\"{art}__v{vid}\"))\n",
        "\n",
        "    basis = \"text_clean\" if \"text_clean\" in doc and isinstance(doc[\"text_clean\"], str) and len(doc[\"text_clean\"])>0 else \"text\"\n",
        "    text = doc[basis]\n",
        "\n",
        "    # Sentence list (full doc)\n",
        "    spans = sent_spans(text)\n",
        "    sents = split_sents_by_spans(text, spans)\n",
        "    df_sent = ppl_on_sentences(sents)\n",
        "    # guard: drop very short\n",
        "    df_sent = df_sent[df_sent[\"bpe_len\"] >= 3]\n",
        "\n",
        "    # Version-level aggregates\n",
        "    ver_info = {\n",
        "        \"article_id\": art, \"version_id\": vid, \"version_tag\": vtag, \"doc_id\": did,\n",
        "        \"n_sents_ver\": int(df_sent.shape[0]),\n",
        "        \"n_bpe_tokens_sum_ver\": int(df_sent[\"bpe_len\"].dropna().sum()) if not df_sent.empty else 0,\n",
        "        \"truncation_max_bpe\": int(MAX_BPE),\n",
        "    }\n",
        "    ver_info.update(agg_stats(df_sent[\"ppl\"], \"ppl\"))\n",
        "    # rename ppl_* to ppl_*_ver and nll_* to nll_*_ver\n",
        "    ver_info = { (k.replace(\"ppl_\", \"ppl_\") + \"_ver\" if k.startswith(\"ppl_\") else\n",
        "                   k.replace(\"nll_\", \"nll_\") + \"_ver\" if k.startswith(\"nll_\") else k): v\n",
        "                for k,v in {**ver_info, **agg_stats(df_sent[\"nll\"], \"nll\")}.items() }\n",
        "    # Fix keys for counts after rename\n",
        "    ver_info[\"article_id\"]=art; ver_info[\"version_id\"]=vid; ver_info[\"version_tag\"]=vtag; ver_info[\"doc_id\"]=did\n",
        "    ver_rows.append(ver_info)\n",
        "\n",
        "    # Window-level (if DF_WINS present)\n",
        "    if DF_WINS is not None:\n",
        "        sub = DF_WINS[(DF_WINS.article_id==art) & (DF_WINS.version_id==vid)].copy()\n",
        "        if not sub.empty:\n",
        "            for _, w in sub.sort_values(\"win_id\").iterrows():\n",
        "                basis_w = w.get(\"span_basis\", basis)\n",
        "                src_text = doc.get(basis_w, text)\n",
        "                a, b = int(w.char_start), int(w.char_end)\n",
        "                win_txt = src_text[a:b] if 0 <= a <= b <= len(src_text) else \"\"\n",
        "                # split with NLTK to match Module 2 policy inside the window\n",
        "                w_spans = sent_spans(win_txt)\n",
        "                w_sents = split_sents_by_spans(win_txt, w_spans)\n",
        "                dfw = ppl_on_sentences(w_sents)\n",
        "                dfw = dfw[dfw[\"bpe_len\"] >= 3]\n",
        "                row = {\n",
        "                    \"article_id\": art, \"version_id\": vid, \"version_tag\": vtag, \"doc_id\": did,\n",
        "                    \"win_id\": int(w.win_id), \"win_label\": str(w.win_label),\n",
        "                    \"span_basis\": str(basis_w),\n",
        "                    \"char_start\": int(w.char_start), \"char_end\": int(w.char_end),\n",
        "                    \"sent_start_index\": int(w.sent_start_index) if \"sent_start_index\" in w else -1,\n",
        "                    \"sent_end_index\": int(w.sent_end_index) if \"sent_end_index\" in w else -1,\n",
        "                    \"is_partial_tail\": bool(w.is_partial_tail) if \"is_partial_tail\" in w else False,\n",
        "                    \"n_sents_win\": int(dfw.shape[0]),\n",
        "                    \"n_bpe_tokens_sum_win\": int(dfw[\"bpe_len\"].dropna().sum()) if not dfw.empty else 0,\n",
        "                }\n",
        "                row.update({k+\"_win\": v for k,v in agg_stats(dfw[\"ppl\"], \"ppl\").items()})\n",
        "                row.update({k+\"_win\": v for k,v in agg_stats(dfw[\"nll\"], \"nll\").items()})\n",
        "                # prune duplicate suffixes (agg_stats names already include _mean etc.)\n",
        "                clean = {}\n",
        "                for k,v in row.items():\n",
        "                    k = k.replace(\"ppl_p25_win_win\",\"ppl_p25_win\").replace(\"ppl_p75_win_win\",\"ppl_p75_win\").replace(\"ppl_p90_win_win\",\"ppl_p90_win\")\n",
        "                    k = k.replace(\"ppl_max_win_win\",\"ppl_max_win\").replace(\"ppl_median_win_win\",\"ppl_median_win\").replace(\"ppl_mean_win_win\",\"ppl_mean_win\").replace(\"ppl_std_win_win\",\"ppl_std_win\").replace(\"ppl_cv_win_win\",\"ppl_cv_win\")\n",
        "                    k = k.replace(\"nll_mean_win_win\",\"nll_mean_win\").replace(\"nll_std_win_win\",\"nll_std_win\").replace(\"nll_cv_win_win\",\"nll_cv_win\")\n",
        "                    clean[k]=v\n",
        "                win_rows.append(clean)\n",
        "\n",
        "# ---------- Save artifacts ----------\n",
        "df_ver = pd.DataFrame(ver_rows)\n",
        "# enforce dtypes\n",
        "if not df_ver.empty:\n",
        "    df_ver[\"version_id\"] = df_ver[\"version_id\"].astype(\"int64\")\n",
        "    for c in [\"n_sents_ver\",\"n_bpe_tokens_sum_ver\",\"truncation_max_bpe\"]:\n",
        "        df_ver[c] = df_ver[c].astype(\"int64\")\n",
        "df_ver.to_parquet(OUT_DIR / \"perplexity.parquet\", index=False)\n",
        "\n",
        "if win_rows:\n",
        "    df_win = pd.DataFrame(win_rows)\n",
        "    if not df_win.empty:\n",
        "        df_win[\"version_id\"] = df_win[\"version_id\"].astype(\"int64\")\n",
        "        df_win[\"win_id\"] = df_win[\"win_id\"].astype(\"int64\")\n",
        "        for c in [\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\",\"n_sents_win\",\"n_bpe_tokens_sum_win\"]:\n",
        "            if c in df_win.columns:\n",
        "                df_win[c] = df_win[c].astype(\"int64\")\n",
        "        df_win.to_parquet(OUT_DIR / \"perplexity_windows.parquet\", index=False)\n",
        "else:\n",
        "    df_win = pd.DataFrame()\n",
        "\n",
        "# Update metadata with counts & config\n",
        "meta_path = OUT_DIR / \"metadata.json\"\n",
        "meta = {}\n",
        "if meta_path.exists():\n",
        "    meta = json.loads(meta_path.read_text())\n",
        "meta.update({\n",
        "    \"batch_size_used\": int(os.environ.get(\"LSA_PPL_BATCH\", \"8\")),\n",
        "    \"truncation_max_bpe\": int(MAX_BPE),\n",
        "    \"articles\": int(len({d[\"article_id\"] for d in DOCS})),\n",
        "    \"versions_per_article_min\": int(min([d[\"version_id\"] for d in DOCS] or [0])),\n",
        "    \"versions_per_article_max\": int(max([d[\"version_id\"] for d in DOCS] or [0])),\n",
        "    \"windows_source\": \"module_2\" if DF_WINS is not None else \"absent\",\n",
        "    \"skipped_short_sentences\": int(((df_win.get(\"n_sents_win\", pd.Series(dtype=int))==0).sum()) if not df_win.empty else 0),\n",
        "})\n",
        "meta_path.write_text(json.dumps(meta, indent=2))\n",
        "\n",
        "print(f\"✓ wrote {OUT_DIR/'perplexity.parquet'} ({len(df_ver)} rows)\")\n",
        "print(f\"✓ windows: {'present → ' + str(len(df_win)) + ' rows' if not df_win.empty else 'absent or empty'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0pv8qc49aBM"
      },
      "outputs": [],
      "source": [
        "# 4.4 — transformers: visuals — PPL distributions & trends\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "OUT_DIR  = (BASE_DIR / \"outputs\" / \"transformers\").resolve()\n",
        "PLOTS_DIR = OUT_DIR / \"plots\"\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df_ver = pd.read_parquet(OUT_DIR / \"perplexity.parquet\")\n",
        "df_win = None\n",
        "pwin = OUT_DIR / \"perplexity_windows.parquet\"\n",
        "if pwin.exists():\n",
        "    df_win = pd.read_parquet(pwin)\n",
        "\n",
        "# ---- 1) Global histogram of per-sentence PPL (from window rows if present; else approximate from version means) ----\n",
        "def _hist_data():\n",
        "    if df_win is not None and not df_win.empty and \"ppl_mean_win\" in df_win.columns:\n",
        "        return df_win[\"ppl_mean_win\"].replace([np.inf, -np.inf], np.nan).dropna().values\n",
        "    # fallback: use version means (coarser)\n",
        "    return df_ver[\"ppl_mean_ver\"].replace([np.inf, -np.inf], np.nan).dropna().values\n",
        "\n",
        "vals = _hist_data()\n",
        "plt.figure(figsize=(8,4.5))\n",
        "if vals.size:\n",
        "    vmax = np.nanpercentile(vals, 99)\n",
        "    vals = np.clip(vals, 0, vmax)\n",
        "    plt.hist(vals, bins=40)\n",
        "    plt.xlabel(\"Per-sentence pseudo-perplexity (clipped at 99th pct)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Global distribution of sentence PPL\")\n",
        "else:\n",
        "    plt.text(0.5,0.5,\"No data\", ha=\"center\", va=\"center\", transform=plt.gca().transAxes)\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / \"ppl_hist_global.png\", dpi=150)\n",
        "plt.show(); plt.close()\n",
        "\n",
        "# ---- 2) Per-article version trend (mean PPL per version) ----\n",
        "def _safe_slug(s: str) -> str:\n",
        "    import re\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", str(s))\n",
        "\n",
        "for art, gg in df_ver.groupby(\"article_id\"):\n",
        "    g = gg.sort_values(\"version_id\")\n",
        "    plt.figure(figsize=(7.5,4.2))\n",
        "    plt.plot(g[\"version_id\"], g[\"ppl_mean_ver\"], marker=\"o\")\n",
        "    plt.xticks(g[\"version_id\"], [f\"v{v}\" for v in g[\"version_id\"]])\n",
        "    plt.xlabel(\"version\")\n",
        "    plt.ylabel(\"mean PPL\")\n",
        "    plt.title(f\"PPL by version — {art}\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    fname = PLOTS_DIR / f\"ppl_trend_version_{_safe_slug(art)}.png\"\n",
        "    plt.tight_layout(); plt.savefig(fname, dpi=150); plt.show(); plt.close()\n",
        "\n",
        "# ---- 3) Per-article window trend (within versions) ----\n",
        "if df_win is not None and not df_win.empty:\n",
        "    for art, gA in df_win.groupby(\"article_id\"):\n",
        "        plt.figure(figsize=(9,4.8))\n",
        "        for vid, gV in gA.groupby(\"version_id\"):\n",
        "            gV = gV.sort_values(\"win_id\")\n",
        "            plt.plot(gV[\"win_id\"], gV[\"ppl_mean_win\"], marker=\".\", alpha=0.8, label=f\"v{vid}\")\n",
        "        plt.xlabel(\"window id\")\n",
        "        plt.ylabel(\"mean PPL (window)\")\n",
        "        plt.title(f\"PPL across windows — {art}\")\n",
        "        plt.legend()\n",
        "        fname = PLOTS_DIR / f\"ppl_trend_windows_{_safe_slug(art)}.png\"\n",
        "        plt.tight_layout(); plt.savefig(fname, dpi=150); plt.show(); plt.close()\n",
        "\n",
        "print(\"✓ 4.4 visuals written to\", PLOTS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtfpiVG3XPMh"
      },
      "outputs": [],
      "source": [
        "# 5.1 — sentence-transformers: install (CPU-friendly)\n",
        "import sys, subprocess, pkgutil\n",
        "\n",
        "def _pip(args):\n",
        "    return subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *args])\n",
        "\n",
        "need = []\n",
        "# Ensure torch/transformers exist (from Module 4 ideally); install CPU wheel if missing\n",
        "if pkgutil.find_loader(\"torch\") is None:\n",
        "    need += [\"torch==2.*\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"]\n",
        "if pkgutil.find_loader(\"transformers\") is None:\n",
        "    need += [\"transformers>=4.40,<5\"]\n",
        "if pkgutil.find_loader(\"sentence_transformers\") is None:\n",
        "    need += [\"sentence-transformers==3.*\"]\n",
        "if pkgutil.find_loader(\"sklearn\") is None:\n",
        "    need += [\"scikit-learn==1.*\"]\n",
        "if pkgutil.find_loader(\"nltk\") is None:\n",
        "    need += [\"nltk>=3.8,<4\"]\n",
        "\n",
        "if need:\n",
        "    print(\"Installing:\", need)\n",
        "    _pip(need)\n",
        "else:\n",
        "    print(\"✓ Dependencies already installed\")\n",
        "\n",
        "# NLTK punkt for sentence spans (aligns with Module 2)\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except Exception:\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "print(\"✓ 5.1 complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EmPQmhxXRaL"
      },
      "outputs": [],
      "source": [
        "# 5.2 — sentence-transformers: imports & model init (MiniLM)\n",
        "from pathlib import Path\n",
        "import os, json\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Dirs\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "SEM_OUT  = (BASE_DIR / \"outputs\" / \"semantic\").resolve()\n",
        "PLOTS_DIR = SEM_OUT / \"plots\"\n",
        "for p in (SEM_OUT, PLOTS_DIR):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Model config\n",
        "MODEL_NAME = os.environ.get(\"LSA_SEM_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "EMB_BATCH  = int(os.environ.get(\"LSA_SEM_BATCH\", \"64\"))\n",
        "NORMALIZE  = True\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() and os.environ.get(\"LSA_ALLOW_CUDA\",\"1\")==\"1\" else \"cpu\"\n",
        "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
        "# Normalization makes cosine == dot\n",
        "model_kwargs = dict(normalize_embeddings=NORMALIZE, convert_to_numpy=True, batch_size=EMB_BATCH)\n",
        "\n",
        "def embed(texts):\n",
        "    # Returns float32 (n, d) with L2 normalization (if NORMALIZE)\n",
        "    if not texts:\n",
        "        return np.zeros((0, model.get_sentence_embedding_dimension()), dtype=np.float32)\n",
        "    embs = model.encode(texts, **model_kwargs)\n",
        "    # Ensure float32\n",
        "    return embs.astype(np.float32)\n",
        "\n",
        "meta = {\n",
        "    \"version_order_source\": \"filename_prefix\",\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"model_dim\": int(model.get_sentence_embedding_dimension()),\n",
        "    \"normalize_embeddings\": bool(NORMALIZE),\n",
        "    \"batch_size\": int(EMB_BATCH),\n",
        "    \"device\": DEVICE,\n",
        "}\n",
        "(SEM_OUT / \"metadata.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "print(\"✓ 5.2 ready →\", MODEL_NAME, \"on\", DEVICE, \"dim\", meta[\"model_dim\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c3FO_6-XTWn"
      },
      "outputs": [],
      "source": [
        "# 5.3 — embeddings → coherence, redundancy, drift (doc + windows)\n",
        "import os, math, json\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import nltk\n",
        "\n",
        "# --- Dirs & inputs ---\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "SEM_OUT  = (BASE_DIR / \"outputs\" / \"semantic\").resolve()\n",
        "PLOTS_DIR = SEM_OUT / \"plots\"\n",
        "for p in (SEM_OUT, PLOTS_DIR):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Discover DOCS: prefer in-memory, then df_docs, else scan input_docs (01..04)\n",
        "def _discover_docs() -> List[Dict]:\n",
        "    if \"DOCS\" in globals() and isinstance(globals()[\"DOCS\"], list):\n",
        "        return globals()[\"DOCS\"]\n",
        "    if \"df_docs\" in globals():\n",
        "        return globals()[\"df_docs\"].to_dict(\"records\")\n",
        "    SOURCE_DIR = Path(os.environ.get(\"LSA_SOURCE_DIR\", BASE_DIR / \"input_docs\"))\n",
        "    docs=[]\n",
        "    if SOURCE_DIR.exists():\n",
        "        for p in sorted(SOURCE_DIR.iterdir()):\n",
        "            if not p.is_file(): continue\n",
        "            stem = p.stem\n",
        "            try:\n",
        "                vid = int(stem[:2])\n",
        "            except Exception:\n",
        "                continue\n",
        "            art = stem[3:] if len(stem)>3 and stem[2] in (\"-\",\"_\") else stem\n",
        "            txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "            docs.append({\"article_id\": art, \"version_id\": vid, \"version_tag\": f\"v{vid}\", \"doc_id\": f\"{art}__v{vid}\", \"text\": txt})\n",
        "    if not docs:\n",
        "        raise RuntimeError(\"Module 5: No documents found. Provide DOCS/df_docs or set LSA_SOURCE_DIR.\")\n",
        "    return docs\n",
        "\n",
        "DOCS = _discover_docs()\n",
        "DOCS = sorted(DOCS, key=lambda d: (str(d.get(\"article_id\",\"\")), int(d.get(\"version_id\",0))))\n",
        "\n",
        "# Module-2 windows (authoritative)\n",
        "M2_WIN = BASE_DIR / \"outputs\" / \"nltk\" / \"fw_burstiness_windows.parquet\"\n",
        "DFW = pd.read_parquet(M2_WIN) if M2_WIN.exists() else None\n",
        "uses_m2 = bool(DFW is not None and set([\"article_id\",\"version_id\",\"win_id\",\"span_basis\",\"char_start\",\"char_end\",\"win_label\"]).issubset(DFW.columns))\n",
        "\n",
        "# Sentence spans via NLTK Punkt (align with Module 2)\n",
        "_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "def sent_spans(text: str) -> List[Tuple[int,int]]:\n",
        "    return list(_tokenizer.span_tokenize(text))\n",
        "\n",
        "def _safe_slug(s: str) -> str:\n",
        "    import re\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", str(s))\n",
        "\n",
        "# --- Core measures ---\n",
        "def _centroid(u: np.ndarray) -> np.ndarray:\n",
        "    if u.size == 0: return np.zeros((model.get_sentence_embedding_dimension(),), dtype=np.float32)\n",
        "    c = u.mean(axis=0)\n",
        "    # Normalize to unit (if zero-norm, keep zeros)\n",
        "    n = np.linalg.norm(c)\n",
        "    return c / n if n > 0 else c\n",
        "\n",
        "def coherence(embeds: np.ndarray) -> float:\n",
        "    # mean cosine to centroid\n",
        "    if embeds.shape[0] < 1: return np.nan\n",
        "    c = _centroid(embeds)\n",
        "    if not np.any(c): return np.nan\n",
        "    sims = embeds @ c  # normalized ⇒ dot == cosine\n",
        "    return float(np.nanmean(sims))\n",
        "\n",
        "def redundancy_from_pairwise(embeds: np.ndarray, topk: int = 1) -> float:\n",
        "    # mean of top-k neighbor cosine per row (ignoring self). topk=1 ⇒ neighbor_sim\n",
        "    n = embeds.shape[0]\n",
        "    if n <= 1: return np.nan\n",
        "    S = embeds @ embeds.T  # cosine\n",
        "    # mask self\n",
        "    np.fill_diagonal(S, -np.inf)\n",
        "    if topk == 1:\n",
        "        nn = np.max(S, axis=1)\n",
        "        return float(np.nanmean(nn))\n",
        "    k = min(topk, n-1)\n",
        "    # take top-k along each row\n",
        "    part = np.partition(S, -k, axis=1)[:, -k:]\n",
        "    # mean over top-k\n",
        "    vals = np.mean(part, axis=1)\n",
        "    return float(np.nanmean(vals))\n",
        "\n",
        "def pca_sem_var(embeds: np.ndarray, n_components: int = 8) -> float:\n",
        "    # Sum of top-K explained variance ratio (bounded ≤1). Needs ≥2 samples.\n",
        "    if embeds.shape[0] < 2: return np.nan\n",
        "    try:\n",
        "        K = min(n_components, embeds.shape[0], embeds.shape[1])\n",
        "        pca = PCA(n_components=K, svd_solver=\"randomized\", random_state=7)\n",
        "        pca.fit(embeds)\n",
        "        return float(np.sum(pca.explained_variance_ratio_))\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# --- Compute metrics ---\n",
        "doc_rows = []\n",
        "win_rows = []\n",
        "\n",
        "for doc in DOCS:\n",
        "    art = str(doc[\"article_id\"]); vid = int(doc[\"version_id\"]); vtag = str(doc.get(\"version_tag\", f\"v{vid}\")); did = str(doc.get(\"doc_id\", f\"{art}__v{vid}\"))\n",
        "    basis = \"text_clean\" if (\"text_clean\" in doc and isinstance(doc[\"text_clean\"], str) and doc[\"text_clean\"]) else \"text\"\n",
        "    text = str(doc.get(basis) if basis in doc else doc.get(\"text\",\"\"))\n",
        "    if not text:\n",
        "        doc_rows.append({\"article_id\": art, \"version_id\": vid, \"version_tag\": vtag, \"doc_id\": did,\n",
        "                         \"n_sents_sem\": 0, \"coherence_doc\": np.nan, \"dispersion_doc\": np.nan, \"redundancy_doc\": np.nan})\n",
        "        continue\n",
        "\n",
        "    spans = sent_spans(text)\n",
        "    sents = [text[a:b] for (a,b) in spans]\n",
        "    E = embed(sents)  # (n, d)\n",
        "    n_s = int(E.shape[0])\n",
        "\n",
        "    coh = coherence(E)\n",
        "    red_nn = redundancy_from_pairwise(E, topk=1)  # neighbor_sim over whole doc\n",
        "    doc_rows.append({\n",
        "        \"article_id\": art, \"version_id\": vid, \"version_tag\": vtag, \"doc_id\": did,\n",
        "        \"n_sents_sem\": n_s,\n",
        "        \"coherence_doc\": coh, \"dispersion_doc\": (1.0 - coh) if not math.isnan(coh) else np.nan,\n",
        "        \"redundancy_doc\": red_nn\n",
        "    })\n",
        "\n",
        "    # Window-level using M2 spans (full containment policy)\n",
        "    if uses_m2:\n",
        "        sub = DFW[(DFW.article_id==art) & (DFW.version_id==vid)].copy()\n",
        "        if not sub.empty:\n",
        "            for _, w in sub.sort_values(\"win_id\").iterrows():\n",
        "                basis_w = str(w[\"span_basis\"]) if str(w[\"span_basis\"]) in (\"text\",\"text_clean\") else basis\n",
        "                src_text = str(doc.get(basis_w, text))\n",
        "                wa, wb = int(w[\"char_start\"]), int(w[\"char_end\"])\n",
        "                # pick sentences whose spans are fully inside the window bounds (on the **same basis**)\n",
        "                # Recompute sentence spans if basis differs\n",
        "                if basis_w == basis:\n",
        "                    sspans = spans\n",
        "                    stexts = sents\n",
        "                else:\n",
        "                    sspans = sent_spans(src_text)\n",
        "                    stexts = [src_text[a:b] for (a,b) in sspans]\n",
        "                    # recompute embeddings on the alternate basis ONCE per window slice:\n",
        "                # Select indices\n",
        "                idxs = [i for i,(a,b) in enumerate(sspans) if (a >= wa and b <= wb)]\n",
        "                if len(idxs) == 0:\n",
        "                    # No full sentences inside window → emit NaNs\n",
        "                    win_rows.append({\n",
        "                        \"article_id\": art, \"version_id\": vid, \"version_tag\": vtag, \"doc_id\": did,\n",
        "                        \"win_id\": int(w.win_id), \"win_label\": str(w.win_label),\n",
        "                        \"span_basis\": basis_w, \"char_start\": wa, \"char_end\": wb,\n",
        "                        \"sent_start_index\": int(w.sent_start_index) if \"sent_start_index\" in w else -1,\n",
        "                        \"sent_end_index\": int(w.sent_end_index) if \"sent_end_index\" in w else -1,\n",
        "                        \"is_partial_tail\": bool(w.is_partial_tail) if \"is_partial_tail\" in w else False,\n",
        "                        \"n_sents_win\": 0,\n",
        "                        \"n_tokens_win\": int(w.n_tokens_win) if \"n_tokens_win\" in w else 0,\n",
        "                        \"coherence_win\": np.nan, \"neighbor_sim_win\": np.nan, \"sem_var_win\": np.nan,\n",
        "                        \"redundancy_win\": np.nan\n",
        "                    })\n",
        "                    continue\n",
        "                # get sentence subset embeddings (compute lazily per basis if needed)\n",
        "                if basis_w == basis:\n",
        "                    Ew = E[idxs]\n",
        "                else:\n",
        "                    Ew = embed([stexts[i] for i in idxs])\n",
        "                coh_w = coherence(Ew)\n",
        "                nn_w  = redundancy_from_pairwise(Ew, topk=1)\n",
        "                red3  = redundancy_from_pairwise(Ew, topk=3)\n",
        "                svar  = pca_sem_var(Ew, n_components=8)\n",
        "                win_rows.append({\n",
        "                    \"article_id\": art, \"version_id\": vid, \"version_tag\": vtag, \"doc_id\": did,\n",
        "                    \"win_id\": int(w.win_id), \"win_label\": str(w.win_label),\n",
        "                    \"span_basis\": basis_w, \"char_start\": wa, \"char_end\": wb,\n",
        "                    \"sent_start_index\": int(w.sent_start_index) if \"sent_start_index\" in w else -1,\n",
        "                    \"sent_end_index\": int(w.sent_end_index) if \"sent_end_index\" in w else -1,\n",
        "                    \"is_partial_tail\": bool(w.is_partial_tail) if \"is_partial_tail\" in w else False,\n",
        "                    \"n_sents_win\": int(Ew.shape[0]),\n",
        "                    \"n_tokens_win\": int(w.n_tokens_win) if \"n_tokens_win\" in w else int(sum(len(stexts[i].split()) for i in idxs)),\n",
        "                    \"coherence_win\": coh_w, \"neighbor_sim_win\": nn_w, \"sem_var_win\": svar,\n",
        "                    \"redundancy_win\": red3\n",
        "                })\n",
        "\n",
        "# --- Doc-level drift & deltas (adjacent only) ---\n",
        "df_doc = pd.DataFrame(doc_rows).sort_values([\"article_id\",\"version_id\"]).reset_index(drop=True)\n",
        "\n",
        "# Compute centroids once per doc to get drift\n",
        "centroids = {}\n",
        "for doc in DOCS:\n",
        "    art, vid = str(doc[\"article_id\"]), int(doc[\"version_id\"])\n",
        "    basis = \"text_clean\" if (\"text_clean\" in doc and isinstance(doc[\"text_clean\"], str) and doc[\"text_clean\"]) else \"text\"\n",
        "    text = str(doc.get(basis) if basis in doc else doc.get(\"text\",\"\"))\n",
        "    spans = sent_spans(text)\n",
        "    sents = [text[a:b] for (a,b) in spans]\n",
        "    E = embed(sents)\n",
        "    centroids[(art,vid)] = _centroid(E)\n",
        "\n",
        "drifts = []\n",
        "for art, g in df_doc.groupby(\"article_id\", sort=False):\n",
        "    g = g.sort_values(\"version_id\")\n",
        "    prev = None\n",
        "    for _, row in g.iterrows():\n",
        "        k = (art, int(row[\"version_id\"]))\n",
        "        c = centroids.get(k)\n",
        "        if prev is None or c is None or not np.any(c) or not np.any(prev):\n",
        "            drifts.append(np.nan)\n",
        "        else:\n",
        "            drifts.append(float(1.0 - float(np.dot(prev, c))))\n",
        "        prev = c\n",
        "df_doc[\"semantic_drift_from_prev\"] = drifts\n",
        "\n",
        "# Deltas table\n",
        "delta_rows = []\n",
        "for art, g in df_doc.groupby(\"article_id\", sort=False):\n",
        "    g = g.sort_values(\"version_id\")\n",
        "    for i in range(len(g)-1):\n",
        "        a = g.iloc[i]; b = g.iloc[i+1]\n",
        "        if int(b[\"version_id\"]) - int(a[\"version_id\"]) != 1: continue\n",
        "        delta_rows.append({\n",
        "            \"article_id\": art,\n",
        "            \"from_version\": int(a[\"version_id\"]),\n",
        "            \"to_version\": int(b[\"version_id\"]),\n",
        "            \"d_coherence_doc\": float(b[\"coherence_doc\"] - a[\"coherence_doc\"]) if (pd.notna(a[\"coherence_doc\"]) and pd.notna(b[\"coherence_doc\"])) else np.nan,\n",
        "            \"d_redundancy_doc\": float(b[\"redundancy_doc\"] - a[\"redundancy_doc\"]) if (pd.notna(a[\"redundancy_doc\"]) and pd.notna(b[\"redundancy_doc\"])) else np.nan,\n",
        "            \"semantic_drift\": float(df_doc.loc[b.name, \"semantic_drift_from_prev\"])\n",
        "        })\n",
        "df_delta = pd.DataFrame(delta_rows)\n",
        "\n",
        "# Window DF\n",
        "df_win = pd.DataFrame(win_rows).sort_values([\"article_id\",\"version_id\",\"win_id\"]).reset_index(drop=True)\n",
        "\n",
        "# --- Enforce dtypes & save ---\n",
        "if not df_doc.empty:\n",
        "    df_doc[\"version_id\"] = df_doc[\"version_id\"].astype(\"int64\")\n",
        "    df_doc[\"n_sents_sem\"] = df_doc[\"n_sents_sem\"].astype(\"int64\")\n",
        "df_doc.to_parquet(SEM_OUT / \"semantic_metrics.parquet\", index=False)\n",
        "\n",
        "if not df_win.empty:\n",
        "    for c in (\"version_id\",\"win_id\",\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\",\"n_sents_win\",\"n_tokens_win\"):\n",
        "        if c in df_win.columns:\n",
        "            df_win[c] = df_win[c].astype(\"int64\")\n",
        "    df_win.to_parquet(SEM_OUT / \"semantic_windows.parquet\", index=False)\n",
        "\n",
        "if not df_delta.empty:\n",
        "    for c in (\"from_version\",\"to_version\"):\n",
        "        df_delta[c] = df_delta[c].astype(\"int64\")\n",
        "    df_delta.to_parquet(SEM_OUT / \"semantic_deltas.parquet\", index=False)\n",
        "\n",
        "# Update metadata\n",
        "meta_path = SEM_OUT / \"metadata.json\"\n",
        "meta = {}\n",
        "if meta_path.exists():\n",
        "    meta = json.loads(meta_path.read_text())\n",
        "meta.update({\n",
        "    \"uses_m2_windows\": bool(uses_m2),\n",
        "    \"window_alignment_policy\": \"from_module_2_spans/full_containment\" if uses_m2 else \"none\",\n",
        "    \"articles\": int(len({d[\"article_id\"] for d in DOCS})),\n",
        "    \"versions_per_article_min\": int(min([d[\"version_id\"] for d in DOCS] or [0])),\n",
        "    \"versions_per_article_max\": int(max([d[\"version_id\"] for d in DOCS] or [0])),\n",
        "    \"expected_versions\": 4,\n",
        "    \"enable_nli\": False,\n",
        "})\n",
        "meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print({\n",
        "    \"doc_rows\": int(df_doc.shape[0]),\n",
        "    \"win_rows\": int(df_win.shape[0]) if not df_win.empty else 0,\n",
        "    \"delta_rows\": int(df_delta.shape[0]) if not df_delta.empty else 0,\n",
        "    \"saved\": [str(SEM_OUT / \"semantic_metrics.parquet\")] + \\\n",
        "             ([str(SEM_OUT / \"semantic_windows.parquet\")] if not df_win.empty else []) + \\\n",
        "             ([str(SEM_OUT / \"semantic_deltas.parquet\")] if not df_delta.empty else [])\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_xzcYIlXW1T"
      },
      "outputs": [],
      "source": [
        "# 5.3b — optional sampled NLI (adjacent versions). Default: skip. Set ENABLE_NLI=1 to run.\n",
        "import os, json, random, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ENABLE_NLI = os.environ.get(\"ENABLE_NLI\",\"0\") == \"1\"\n",
        "NLI_MODEL = os.environ.get(\"LSA_NLI_MODEL\",\"cross-encoder/nli-deberta-v3-base\")\n",
        "MAX_PAIRS = int(os.environ.get(\"LSA_NLI_MAX_PAIRS\",\"64\"))\n",
        "\n",
        "if ENABLE_NLI:\n",
        "    from sentence_transformers import CrossEncoder\n",
        "    from nltk import data as _ndata\n",
        "    _tokenizer = _ndata.load(\"tokenizers/punkt/english.pickle\")\n",
        "\n",
        "    def _sents(txt: str):\n",
        "        return [txt[a:b] for (a,b) in _tokenizer.span_tokenize(txt or \"\")]\n",
        "    # Expect DOCS & df_doc from 5.3\n",
        "    model = CrossEncoder(NLI_MODEL, automodel_args={\"torch_dtype\":\"auto\"})\n",
        "    results = []\n",
        "    for art, g in pd.DataFrame(DOCS).groupby(\"article_id\", sort=False):\n",
        "        g = g.sort_values(\"version_id\")\n",
        "        for i in range(len(g)-1):\n",
        "            A, B = g.iloc[i], g.iloc[i+1]\n",
        "            sA, sB = _sents(A.get(\"text_clean\", A[\"text\"])), _sents(B.get(\"text_clean\", B[\"text\"]))\n",
        "            # Align by index up to min len; sample pairs\n",
        "            pairs = [(sA[j], sB[j]) for j in range(min(len(sA), len(sB)))]\n",
        "            random.Random(7).shuffle(pairs)\n",
        "            pairs = pairs[:MAX_PAIRS]\n",
        "            if not pairs:\n",
        "                results.append({\"article_id\": art, \"from_version\": int(A[\"version_id\"]), \"to_version\": int(B[\"version_id\"]),\n",
        "                                \"nli_pairs_sampled\": 0, \"nli_p_contra_sampled\": math.nan, \"nli_p_entail_sampled\": math.nan})\n",
        "                continue\n",
        "            probs = model.predict(pairs, apply_softmax=True, batch_size=16)  # returns [p_contra, p_neutral, p_entail]\n",
        "            probs = np.asarray(probs)\n",
        "            results.append({\n",
        "                \"article_id\": art, \"from_version\": int(A[\"version_id\"]), \"to_version\": int(B[\"version_id\"]),\n",
        "                \"nli_pairs_sampled\": int(len(pairs)),\n",
        "                \"nli_p_contra_sampled\": float(np.mean(probs[:,0])),\n",
        "                \"nli_p_entail_sampled\": float(np.mean(probs[:,2])),\n",
        "            })\n",
        "    df_nli = pd.DataFrame(results)\n",
        "\n",
        "    # Merge into doc-level table for convenience\n",
        "    from pathlib import Path\n",
        "    BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "    SEM_OUT  = (BASE_DIR / \"outputs\" / \"semantic\").resolve()\n",
        "    doc_path = SEM_OUT / \"semantic_metrics.parquet\"\n",
        "    df_doc = pd.read_parquet(doc_path)\n",
        "    # Attach to 'to_version' rows\n",
        "    m = df_doc.merge(df_nli, left_on=[\"article_id\",\"version_id\"], right_on=[\"article_id\",\"to_version\"], how=\"left\")\n",
        "    m = m.drop(columns=[\"from_version\",\"to_version\"])\n",
        "    m.to_parquet(doc_path, index=False)\n",
        "\n",
        "    # update metadata\n",
        "    meta_path = SEM_OUT / \"metadata.json\"\n",
        "    meta = json.loads(meta_path.read_text()) if meta_path.exists() else {}\n",
        "    meta.update({\"enable_nli\": True, \"nli_model\": NLI_MODEL, \"nli_pairs_max\": MAX_PAIRS})\n",
        "    meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    print({\"nli_rows\": int(df_nli.shape[0]), \"merged_into_doc_table\": True})\n",
        "else:\n",
        "    print(\"5.3b skipped (ENABLE_NLI=0).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIkHpygycJEv"
      },
      "source": [
        "5.3b is the optional “tiny NLI” step that samples sentence pairs between adjacent versions and estimates contradiction/entailment rates.\n",
        "\n",
        "It’s off by default to keep runs fast and CPU-friendly. The cell checks the environment variable ENABLE_NLI; if it’s not set to \"1\", it prints 5.3b skipped (ENABLE_NLI=0). and exits without doing anything.\n",
        "\n",
        "If you want it on later, set os.environ[\"ENABLE_NLI\"] = \"1\" in a small setup cell, then re-run 5.3b. (It can be slow on CPU — that’s why it’s opt-in.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRTmu5yaXako"
      },
      "outputs": [],
      "source": [
        "# 5.4 — visuals: coherence distributions, trends, heatmap, window trends\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json, os, re\n",
        "\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "SEM_OUT  = (BASE_DIR / \"outputs\" / \"semantic\").resolve()\n",
        "PLOTS_DIR = SEM_OUT / \"plots\"\n",
        "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _safe_slug(s: str) -> str:\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", str(s))\n",
        "\n",
        "doc_path = SEM_OUT / \"semantic_metrics.parquet\"\n",
        "win_path = SEM_OUT / \"semantic_windows.parquet\"\n",
        "dfd = pd.read_parquet(doc_path) if doc_path.exists() else pd.DataFrame()\n",
        "dfw = pd.read_parquet(win_path) if win_path.exists() else pd.DataFrame()\n",
        "\n",
        "# 1) Global doc coherence violin\n",
        "plt.figure(figsize=(7.5,4.2))\n",
        "vals = dfd[\"coherence_doc\"].replace([np.inf,-np.inf], np.nan).dropna().to_numpy() if \"coherence_doc\" in dfd else np.array([])\n",
        "if vals.size:\n",
        "    plt.violinplot(vals, showmeans=True)\n",
        "    plt.ylabel(\"coherence_doc (cosine to centroid)\")\n",
        "    plt.title(\"Global distribution of semantic coherence (doc-level)\")\n",
        "else:\n",
        "    plt.text(0.5,0.5,\"No data\", ha=\"center\", va=\"center\", transform=plt.gca().transAxes)\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / \"coherence_doc_violin.png\", dpi=150)\n",
        "plt.close()\n",
        "\n",
        "# 2) Per-article trend lines (coherence_doc v1..v4)\n",
        "for art, g in dfd.groupby(\"article_id\"):\n",
        "    g = g.sort_values(\"version_id\")\n",
        "    plt.figure(figsize=(7.5,4.2))\n",
        "    plt.plot(g[\"version_id\"], g[\"coherence_doc\"], marker=\"o\")\n",
        "    plt.xticks(g[\"version_id\"], [f\"v{v}\" for v in g[\"version_id\"]])\n",
        "    plt.xlabel(\"version\"); plt.ylabel(\"coherence_doc\")\n",
        "    plt.title(f\"Coherence by version — {art}\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(PLOTS_DIR / f\"trend_coherence_{_safe_slug(art)}_.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# 3) Window coherence lines per article\n",
        "if not dfw.empty and \"coherence_win\" in dfw.columns:\n",
        "    for art, gA in dfw.groupby(\"article_id\"):\n",
        "        plt.figure(figsize=(9,4.8))\n",
        "        for vid, gV in gA.groupby(\"version_id\"):\n",
        "            gV = gV.sort_values(\"win_id\")\n",
        "            plt.plot(gV[\"win_id\"], gV[\"coherence_win\"], marker=\".\", alpha=0.85, label=f\"v{int(vid)}\")\n",
        "        plt.xlabel(\"window id\"); plt.ylabel(\"coherence_win\")\n",
        "        plt.title(f\"Window coherence — {art}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(PLOTS_DIR / f\"win_coherence_{_safe_slug(art)}.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "# 4) Cosine heatmap (first article/version; cap 120x120)\n",
        "# Recompute sentence embeddings for that pair on the same basis\n",
        "try:\n",
        "    import nltk, numpy as np\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    DEVICE = \"cuda\" if (os.environ.get(\"LSA_ALLOW_CUDA\",\"1\")==\"1\" and __import__(\"torch\").cuda.is_available()) else \"cpu\"\n",
        "    MODEL_NAME = os.environ.get(\"LSA_SEM_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
        "    model_kwargs = dict(normalize_embeddings=True, convert_to_numpy=True, batch_size=int(os.environ.get(\"LSA_SEM_BATCH\",\"64\")))\n",
        "    _tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "\n",
        "    if not dfd.empty:\n",
        "        first = dfd.sort_values([\"article_id\",\"version_id\"]).iloc[0]\n",
        "        art, vid = str(first[\"article_id\"]), int(first[\"version_id\"])\n",
        "        # pull text from DOCS/global (fallback scan)\n",
        "        def _find_doc(art, vid):\n",
        "            if \"DOCS\" in globals():\n",
        "                for d in DOCS:\n",
        "                    if str(d[\"article_id\"])==art and int(d[\"version_id\"])==vid:\n",
        "                        return d\n",
        "            return None\n",
        "        d = _find_doc(art, vid)\n",
        "        if d:\n",
        "            basis = \"text_clean\" if (\"text_clean\" in d and isinstance(d[\"text_clean\"], str) and d[\"text_clean\"]) else \"text\"\n",
        "            text = str(d.get(basis) if basis in d else d.get(\"text\",\"\"))\n",
        "            spans = list(_tokenizer.span_tokenize(text or \"\"))\n",
        "            sents = [text[a:b] for (a,b) in spans][:120]\n",
        "            if sents:\n",
        "                E = model.encode(sents, **model_kwargs).astype(np.float32)\n",
        "                S = E @ E.T\n",
        "                plt.figure(figsize=(6,5))\n",
        "                plt.imshow(S, vmin=0, vmax=1, aspect=\"auto\")\n",
        "                plt.colorbar(label=\"cosine\")\n",
        "                plt.title(f\"Sentence cosine heatmap — {art} v{vid} (n={len(sents)})\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(PLOTS_DIR / f\"cosine_heatmap_{_safe_slug(art)}_v{vid}.png\", dpi=150)\n",
        "                plt.close()\n",
        "except Exception as e:\n",
        "    # Best-effort; skip if anything goes sideways\n",
        "    pass\n",
        "\n",
        "# Write plots manifest\n",
        "import json\n",
        "manifest = {\"files\": sorted([p.name for p in PLOTS_DIR.glob(\"*.png\")])}\n",
        "(PLOTS_DIR / \"plots_index.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print({\"plots\": len(manifest[\"files\"]), \"dir\": str(PLOTS_DIR)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWqsLhqdjBQC"
      },
      "outputs": [],
      "source": [
        "# 5.8 — metadata enrich (Module 5)\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = Path.cwd().resolve()\n",
        "SEM_OUT  = (BASE_DIR / \"outputs\" / \"semantic\").resolve()\n",
        "PLOTS    = SEM_OUT / \"plots\"\n",
        "meta_p   = SEM_OUT / \"metadata.json\"\n",
        "\n",
        "# Load current metadata (create if missing)\n",
        "meta = {}\n",
        "if meta_p.exists():\n",
        "    meta = json.loads(meta_p.read_text())\n",
        "\n",
        "# Gather doc/window stats\n",
        "doc_p = SEM_OUT / \"semantic_metrics.parquet\"\n",
        "win_p = SEM_OUT / \"semantic_windows.parquet\"\n",
        "dlt_p = SEM_OUT / \"semantic_deltas.parquet\"\n",
        "\n",
        "arts = vers_min = vers_max = 0\n",
        "skipped_windows = 0\n",
        "total_windows = 0\n",
        "\n",
        "if doc_p.exists():\n",
        "    df_doc = pd.read_parquet(doc_p)\n",
        "    arts = df_doc[\"article_id\"].nunique()\n",
        "    vers_min = int(df_doc[\"version_id\"].min()) if not df_doc.empty else 0\n",
        "    vers_max = int(df_doc[\"version_id\"].max()) if not df_doc.empty else 0\n",
        "\n",
        "if win_p.exists():\n",
        "    df_win = pd.read_parquet(win_p)\n",
        "    total_windows = int(df_win.shape[0])\n",
        "    # Windows with <2 sentences → where coherence is NaN (by design) or explicit count <2 if present\n",
        "    if \"n_sents_win\" in df_win.columns:\n",
        "        skipped_windows = int((df_win[\"n_sents_win\"] < 2).sum())\n",
        "    else:\n",
        "        skipped_windows = int(df_win[\"coherence_win\"].isna().sum())\n",
        "\n",
        "# Plot manifest\n",
        "plots = sorted([p.name for p in PLOTS.glob(\"*.png\")]) if PLOTS.exists() else []\n",
        "(SEM_OUT / \"plots\" / \"plots_index.json\").write_text(\n",
        "    json.dumps({\"files\": plots}, indent=2), encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# Update metadata\n",
        "meta.update({\n",
        "    \"articles\": int(arts),\n",
        "    \"versions_per_article_min\": int(vers_min),\n",
        "    \"versions_per_article_max\": int(vers_max),\n",
        "    \"windows_available\": bool(win_p.exists()),\n",
        "    \"windows_total\": int(total_windows),\n",
        "    \"windows_with_lt2_sentences\": int(skipped_windows),\n",
        "    \"notes\": meta.get(\"notes\", []) + [\n",
        "        \"Window inclusion rule: full sentence containment within [char_start, char_end).\",\n",
        "        \"Windows with <2 sentences emit NaN for window metrics.\",\n",
        "        \"Cosine metrics use normalized embeddings; ranges within [0,1].\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "meta_p.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "print({\"metadata_updated\": str(meta_p), \"plots_index\": str(SEM_OUT / 'plots' / 'plots_index.json')})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmujUtWHjEPR"
      },
      "outputs": [],
      "source": [
        "# 5.9 — audit & schema/range checks for Module 5\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "BASE_DIR = Path.cwd().resolve()\n",
        "SEM_OUT  = (BASE_DIR / \"outputs\" / \"semantic\").resolve()\n",
        "\n",
        "doc_p = SEM_OUT / \"semantic_metrics.parquet\"\n",
        "win_p = SEM_OUT / \"semantic_windows.parquet\"\n",
        "dlt_p = SEM_OUT / \"semantic_deltas.parquet\"\n",
        "\n",
        "res = {\"exists\": {\"doc\": doc_p.exists(), \"win\": win_p.exists(), \"dlt\": dlt_p.exists()}}\n",
        "\n",
        "req_doc = {\"article_id\",\"version_id\",\"version_tag\",\"doc_id\",\"n_sents_sem\",\n",
        "           \"coherence_doc\",\"dispersion_doc\",\"redundancy_doc\",\"semantic_drift_from_prev\"}\n",
        "req_win = {\"article_id\",\"version_id\",\"version_tag\",\"doc_id\",\"win_id\",\"win_label\",\n",
        "           \"span_basis\",\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\",\"is_partial_tail\",\n",
        "           \"n_sents_win\",\"coherence_win\",\"neighbor_sim_win\",\"redundancy_win\"}\n",
        "req_dlt = {\"article_id\",\"from_version\",\"to_version\",\"semantic_drift\",\"d_coherence_doc\",\"d_redundancy_doc\"}\n",
        "\n",
        "if res[\"exists\"][\"doc\"]:\n",
        "    doc = pd.read_parquet(doc_p)\n",
        "    res[\"doc_missing\"] = sorted(req_doc - set(doc.columns))\n",
        "    res[\"doc_rows\"] = int(doc.shape[0])\n",
        "    res[\"coherence_doc_in_0_1\"] = bool(doc[\"coherence_doc\"].dropna().between(0,1).all()) if \"coherence_doc\" in doc else None\n",
        "    res[\"redundancy_doc_in_0_1\"] = bool(doc[\"redundancy_doc\"].dropna().between(0,1).all()) if \"redundancy_doc\" in doc else None\n",
        "\n",
        "if res[\"exists\"][\"win\"]:\n",
        "    win = pd.read_parquet(win_p).sort_values([\"article_id\",\"version_id\",\"win_id\"])\n",
        "    res[\"win_missing\"] = sorted(req_win - set(win.columns))\n",
        "    res[\"win_rows\"] = int(win.shape[0])\n",
        "    # contiguity per (article,version)\n",
        "    ok_ids=[]\n",
        "    for (a,v), g in win.groupby([\"article_id\",\"version_id\"], sort=False):\n",
        "        ok_ids.append(list(g[\"win_id\"]) == list(range(1, len(g)+1)))\n",
        "    res[\"win_id_contiguous\"] = all(ok_ids) if ok_ids else None\n",
        "    # window metric range sanity\n",
        "    if \"coherence_win\" in win:\n",
        "        wv = win[\"coherence_win\"].replace([np.inf,-np.inf], np.nan).dropna()\n",
        "        res[\"coherence_win_in_0_1\"] = bool(wv.between(0,1).all())\n",
        "    if \"neighbor_sim_win\" in win:\n",
        "        nv = win[\"neighbor_sim_win\"].replace([np.inf,-np.inf], np.nan).dropna()\n",
        "        res[\"neighbor_sim_win_in_0_1\"] = bool(nv.between(0,1).all())\n",
        "\n",
        "if res[\"exists\"][\"dlt\"]:\n",
        "    dlt = pd.read_parquet(dlt_p)\n",
        "    res[\"dlt_missing\"] = sorted(req_dlt - set(dlt.columns))\n",
        "    res[\"dlt_rows\"] = int(dlt.shape[0])\n",
        "    if {\"to_version\",\"from_version\"}.issubset(dlt.columns):\n",
        "        res[\"adjacent_only\"] = bool(((dlt[\"to_version\"] - dlt[\"from_version\"])==1).all())\n",
        "    if \"semantic_drift\" in dlt.columns:\n",
        "        res[\"semantic_drift_nonneg\"] = bool(dlt[\"semantic_drift\"].dropna().ge(0).all())\n",
        "\n",
        "print(json.dumps(res, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LGcUjAGBllc"
      },
      "outputs": [],
      "source": [
        "# 6.1 — BERTopic | UMAP | HDBSCAN: install (CPU) + deterministic seeds + thread caps\n",
        "import sys, subprocess, os, random, importlib.util, json\n",
        "from pathlib import Path\n",
        "\n",
        "def _pip(*args):\n",
        "    return subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *args])\n",
        "\n",
        "def _missing(modname: str) -> bool:\n",
        "    return importlib.util.find_spec(modname) is None\n",
        "\n",
        "need = []\n",
        "if _missing(\"bertopic\"):\n",
        "    need += ['bertopic==0.16.*']\n",
        "# UMAP can be found as \"umap\" (package \"umap-learn\")\n",
        "if _missing(\"umap\") and _missing(\"umap_learn\"):\n",
        "    need += ['umap-learn>=0.5.4,<0.6']\n",
        "if _missing(\"hdbscan\"):\n",
        "    need += ['hdbscan>=0.8.33']\n",
        "if _missing(\"sentence_transformers\"):\n",
        "    need += ['sentence-transformers>=2.2.2']\n",
        "if _missing(\"sklearn\"):\n",
        "    need += ['scikit-learn>=1.3,<2']\n",
        "if _missing(\"scipy\"):\n",
        "    need += ['scipy>=1.10,<2']\n",
        "if _missing(\"pyarrow\"):\n",
        "    need += ['pyarrow>=14']\n",
        "\n",
        "if need:\n",
        "    print(\"Installing:\", need)\n",
        "    _pip(*need)\n",
        "else:\n",
        "    print(\"✓ Dependencies already installed\")\n",
        "\n",
        "# Determinism + safe threading\n",
        "import numpy as np\n",
        "try:\n",
        "    import torch\n",
        "except Exception:\n",
        "    torch = None\n",
        "\n",
        "SEED = int(os.environ.get(\"LSA_SEED\", \"42\"))\n",
        "if torch is not None and hasattr(torch, \"manual_seed\"):\n",
        "    torch.manual_seed(SEED)\n",
        "\n",
        "# Output dirs + metadata shell\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "TOP_OUT  = (BASE_DIR / \"outputs\" / \"bertopic\").resolve()\n",
        "(TOP_OUT / \"embeddings\").mkdir(parents=True, exist_ok=True)\n",
        "(TOP_OUT / \"plots\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Record package versions\n",
        "def _ver(modname):\n",
        "    try:\n",
        "        m = __import__(modname)\n",
        "        return getattr(m, \"__version__\", \"unknown\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "vers = {\n",
        "    \"bertopic\": _ver(\"bertopic\"),\n",
        "    \"umap\": _ver(\"umap\"),\n",
        "    \"hdbscan\": _ver(\"hdbscan\"),\n",
        "    \"sentence_transformers\": _ver(\"sentence_transformers\"),\n",
        "    \"sklearn\": _ver(\"sklearn\"),\n",
        "    \"scipy\": _ver(\"scipy\"),\n",
        "    \"numpy\": _ver(\"numpy\"),\n",
        "    \"pandas\": _ver(\"pandas\"),\n",
        "    \"matplotlib\": _ver(\"matplotlib\"),\n",
        "    \"pyarrow\": _ver(\"pyarrow\"),\n",
        "    \"torch\": _ver(\"torch\"),\n",
        "}\n",
        "\n",
        "meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "print(\"✓ 6.1 ready → metadata stub at\", meta_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEykllxeBnU-"
      },
      "outputs": [],
      "source": [
        "# 6.2 — BERTopic: init & fit on window texts; write topics.parquet + topic_info.parquet\n",
        "from pathlib import Path\n",
        "import os, json, math, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from umap import UMAP\n",
        "import hdbscan\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# ---- Paths / inputs ----\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "TOP_OUT  = (BASE_DIR / \"outputs\" / \"bertopic\").resolve()\n",
        "PLOT_DIR = TOP_OUT / \"plots\"\n",
        "EMB_DIR  = TOP_OUT / \"embeddings\"\n",
        "M2_PATH  = BASE_DIR / \"outputs\" / \"nltk\" / \"fw_burstiness_windows.parquet\"\n",
        "if not M2_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing Module-2 windows: {M2_PATH}\")\n",
        "\n",
        "DFW = pd.read_parquet(M2_PATH).sort_values([\"article_id\",\"version_id\",\"win_id\"]).reset_index(drop=True)\n",
        "need_cols = {\"article_id\",\"version_id\",\"version_tag\",\"doc_id\",\"win_id\",\"win_label\",\n",
        "             \"span_basis\",\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\"}\n",
        "missing = need_cols - set(DFW.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Module-2 windows missing columns: {sorted(missing)}\")\n",
        "\n",
        "# ---- Discover documents ----\n",
        "def _discover_docs():\n",
        "    # Priority: global DOCS (list of dicts), then df_docs DataFrame, then input_docs folder\n",
        "    if \"DOCS\" in globals() and isinstance(globals()[\"DOCS\"], list):\n",
        "        return globals()[\"DOCS\"]\n",
        "    if \"df_docs\" in globals():\n",
        "        try:\n",
        "            return globals()[\"df_docs\"].to_dict(\"records\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    SRC = BASE_DIR / \"input_docs\"\n",
        "    docs=[]\n",
        "    if SRC.exists():\n",
        "        for p in sorted(SRC.iterdir()):\n",
        "            if not p.is_file():\n",
        "                continue\n",
        "            stem = p.stem\n",
        "            try:\n",
        "                vid = int(stem[:2])\n",
        "            except Exception:\n",
        "                continue\n",
        "            art = stem[3:] if len(stem)>3 and stem[2] in (\"-\",\"_\") else stem\n",
        "            txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "            docs.append({\"article_id\": art, \"version_id\": vid, \"version_tag\": f\"v{vid}\",\n",
        "                         \"doc_id\": f\"{art}__v{vid}\", \"text\": txt})\n",
        "    return docs\n",
        "\n",
        "DOCS = _discover_docs()\n",
        "if not DOCS:\n",
        "    raise RuntimeError(\"Module 6: No DOCS/df_docs/input_docs found for reconstructing window texts.\")\n",
        "\n",
        "def _basis_text(doc: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Always return a dict with keys 'text' and 'text_clean' (text_clean may be None).\n",
        "    No KeyError even if doc lacks 'text_clean'.\n",
        "    \"\"\"\n",
        "    txt = doc.get(\"text\", \"\") or \"\"\n",
        "    txt_clean = doc.get(\"text_clean\", None)\n",
        "    if isinstance(txt_clean, str) and txt_clean.strip():\n",
        "        return {\"text\": txt, \"text_clean\": txt_clean}\n",
        "    else:\n",
        "        return {\"text\": txt, \"text_clean\": None}\n",
        "\n",
        "# Map (article, version) → basis texts\n",
        "TEXTS = {(str(d.get(\"article_id\")), int(d.get(\"version_id\"))): _basis_text(d) for d in DOCS}\n",
        "\n",
        "# Track how many times we had to fallback from requested 'text_clean' to 'text'\n",
        "_basis_mismatch_count = [0]\n",
        "\n",
        "def _slice_text(art, vid, basis, a, b) -> str:\n",
        "    bt = TEXTS.get((str(art), int(vid)))\n",
        "    if not bt:\n",
        "        return \"\"\n",
        "    want_clean = (str(basis) == \"text_clean\")\n",
        "    src = bt.get(\"text_clean\") if want_clean else bt.get(\"text\")\n",
        "    if want_clean and (src is None or src == \"\"):\n",
        "        # Fallback to raw text, count a basis mismatch\n",
        "        _basis_mismatch_count[0] += 1\n",
        "        src = bt.get(\"text\", \"\")\n",
        "    if not isinstance(src, str):\n",
        "        src = str(src or \"\")\n",
        "    a = max(0, int(a)); b = max(a, int(b))\n",
        "    return src[a:b]\n",
        "\n",
        "# ---- Build corpus in stable order ----\n",
        "rows, texts = [], []\n",
        "for _, r in DFW.iterrows():\n",
        "    t = _slice_text(r[\"article_id\"], int(r[\"version_id\"]), str(r[\"span_basis\"]),\n",
        "                    int(r[\"char_start\"]), int(r[\"char_end\"]))\n",
        "    rows.append((r[\"article_id\"], int(r[\"version_id\"]), str(r[\"version_tag\"]), r[\"doc_id\"], int(r[\"win_id\"]), str(r[\"win_label\"]),\n",
        "                 str(r[\"span_basis\"]), int(r[\"char_start\"]), int(r[\"char_end\"]), int(r[\"sent_start_index\"]), int(r[\"sent_end_index\"])))\n",
        "    texts.append(t if isinstance(t, str) else \"\")\n",
        "\n",
        "keys_df = pd.DataFrame(rows, columns=[\"article_id\",\"version_id\",\"version_tag\",\"doc_id\",\"win_id\",\"win_label\",\n",
        "                                      \"span_basis\",\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\"])\n",
        "N = len(texts)\n",
        "if N == 0:\n",
        "    raise RuntimeError(\"Module 6: No windows to process (empty corpus).\")\n",
        "\n",
        "# ---- Embeddings (MiniLM, normalized); cache ----\n",
        "MODEL_NAME = os.environ.get(\"LSA_SEM_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "BATCH = int(os.environ.get(\"LSA_BERTOPIC_BATCH\",\"32\"))\n",
        "import torch\n",
        "DEVICE = \"cuda\" if (os.environ.get(\"LSA_ALLOW_CUDA\",\"0\")==\"1\" and torch.cuda.is_available()) else \"cpu\"\n",
        "st = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
        "emb = st.encode(texts, normalize_embeddings=True, convert_to_numpy=True, batch_size=BATCH).astype(np.float32)\n",
        "\n",
        "EMB_PATH = EMB_DIR / f\"sbert_{re.sub(r'[^A-Za-z0-9._-]+','_',MODEL_NAME)}.npy\"\n",
        "np.save(EMB_PATH, emb)\n",
        "(keys_df[[\"article_id\",\"version_id\",\"win_id\"]]\n",
        " .assign(row_id=np.arange(N))\n",
        " .to_parquet(EMB_DIR / \"embedding_rows.parquet\", index=False))\n",
        "print(f\"Embedded {N} windows → {EMB_PATH} (dim={emb.shape[1]}, device={DEVICE})\")\n",
        "\n",
        "# ---- Fit BERTopic (UMAP → HDBSCAN), with fallback ----\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric=\"cosine\",\n",
        "                  random_state=42, low_memory=True)\n",
        "hdb_params_attempted = []\n",
        "hdbscan_used = None\n",
        "\n",
        "def _make_hdb(min_cluster_size):\n",
        "    hdb_params_attempted.append(min_cluster_size)\n",
        "    return hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=None, metric=\"euclidean\",\n",
        "                           cluster_selection_method=\"eom\", prediction_data=False)\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", min_df=min(3, max(1, N//500)), ngram_range=(1,2))\n",
        "\n",
        "# Attempt 1: mcs=8\n",
        "hdb_model  = _make_hdb(8)\n",
        "topic_model = BERTopic(embedding_model=None, umap_model=umap_model, hdbscan_model=hdb_model,\n",
        "                       vectorizer_model=vectorizer, calculate_probabilities=False, verbose=False)\n",
        "topics, probs = topic_model.fit_transform(texts, embeddings=emb)\n",
        "\n",
        "# Attempt 2: mcs=5 if all noise\n",
        "fallback_used = None\n",
        "if all(t == -1 for t in topics):\n",
        "    hdb_model_small = _make_hdb(5)\n",
        "    topic_model_small = BERTopic(embedding_model=None, umap_model=umap_model, hdbscan_model=hdb_model_small,\n",
        "                                 vectorizer_model=vectorizer, calculate_probabilities=False, verbose=False)\n",
        "    topics, probs = topic_model_small.fit_transform(texts, embeddings=emb)\n",
        "    if not all(t == -1 for t in topics):\n",
        "        topic_model = topic_model_small\n",
        "        hdbscan_used = {\"min_cluster_size\": 5, \"min_samples\": None, \"metric\":\"euclidean\", \"cluster_selection_method\":\"eom\"}\n",
        "\n",
        "# Fallback 3: KMeans if still all noise\n",
        "if all(t == -1 for t in topics):\n",
        "    k = min(10, max(2, N // 50))\n",
        "    km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "    km_ids = km.fit_predict(emb)\n",
        "    topics = km_ids.tolist()\n",
        "    topic_model = None\n",
        "    fallback_used = {\"fallback\": \"kmeans\", \"k\": int(k)}\n",
        "else:\n",
        "    if hdbscan_used is None:\n",
        "        hdbscan_used = {\"min_cluster_size\": 8, \"min_samples\": None, \"metric\":\"euclidean\", \"cluster_selection_method\":\"eom\"}\n",
        "\n",
        "# ---- Labels & topic info ----\n",
        "def _topic_label_from_model(model, tid: int) -> str:\n",
        "    try:\n",
        "        if model is None or tid == -1:\n",
        "            return \"noise\" if tid == -1 else f\"km_{tid}\"\n",
        "        words = model.get_topics().get(tid, [])\n",
        "        toks = [w for (w, _score) in words[:3]] if words else []\n",
        "        return \" \".join(toks) if toks else f\"topic_{tid}\"\n",
        "    except Exception:\n",
        "        return f\"topic_{tid}\"\n",
        "\n",
        "labels = [_topic_label_from_model(topic_model, t) for t in topics]\n",
        "\n",
        "# topic_info parquet\n",
        "if topic_model is not None:\n",
        "    info = topic_model.get_topic_info().rename(columns={\"Topic\":\"topic_id\",\"Count\":\"size\",\"Name\":\"name\"})\n",
        "    def _top_terms(tid):\n",
        "        arr = topic_model.get_topics().get(tid, [])\n",
        "        return \", \".join([w for (w, _s) in arr[:10]]) if arr else \"\"\n",
        "    info[\"top_terms\"] = info[\"topic_id\"].apply(lambda x: _top_terms(x))\n",
        "    info.to_parquet(TOP_OUT / \"topic_info.parquet\", index=False)\n",
        "else:\n",
        "    pd.DataFrame({\"topic_id\": sorted(set(topics)),\n",
        "                  \"size\":[topics.count(t) for t in sorted(set(topics))],\n",
        "                  \"name\":[f\"km_{t}\" for t in sorted(set(topics))],\n",
        "                  \"top_terms\":[\"\"]*len(set(topics))}).to_parquet(TOP_OUT / \"topic_info.parquet\", index=False)\n",
        "\n",
        "# ---- Window-level assignments ----\n",
        "assign = keys_df.copy()\n",
        "assign[\"topic_id\"] = pd.Series(topics, dtype=\"int64\")\n",
        "assign[\"topic_label\"] = labels\n",
        "assign[\"model_name\"] = MODEL_NAME\n",
        "assign[\"topic_prob\"] = np.nan  # probabilities disabled\n",
        "\n",
        "assign.to_parquet(TOP_OUT / \"topics.parquet\", index=False)\n",
        "assign[[\"article_id\",\"version_id\",\"win_id\",\"topic_id\",\"topic_label\",\"topic_prob\"]].to_parquet(\n",
        "    TOP_OUT / \"topics_raw.parquet\", index=False\n",
        ")\n",
        "\n",
        "# ---- Update metadata (correct fields + basis mismatches) ----\n",
        "meta_path = TOP_OUT / \"metadata.json\"\n",
        "meta = json.loads(meta_path.read_text()) if meta_path.exists() else {}\n",
        "meta.update({\n",
        "    \"embedding_model\": MODEL_NAME,\n",
        "    \"embedding_dim\": int(emb.shape[1]),\n",
        "    \"device\": \"cuda\" if (os.environ.get(\"LSA_ALLOW_CUDA\",\"0\")==\"1\" and torch.cuda.is_available()) else \"cpu\",\n",
        "    \"batch_size\": int(BATCH),\n",
        "    \"umap_params\": {\"n_neighbors\":15,\"n_components\":5,\"min_dist\":0.0,\"metric\":\"cosine\",\"random_state\":42,\"low_memory\":True},\n",
        "    \"hdbscan_params_used\": hdbscan_used,                 # dict or None (if KMeans fallback)\n",
        "    \"hdbscan_params_attempted\": hdb_params_attempted,    # e.g., [8, 5]\n",
        "    \"vectorizer\": {\"stop_words\":\"english\",\"min_df\": int(vectorizer.min_df), \"ngram_range\": list(vectorizer.ngram_range)},\n",
        "    \"corpus_windows\": int(N),\n",
        "    \"fallback\": fallback_used,                           # {\"fallback\":\"kmeans\",\"k\":...} or None\n",
        "    \"basis_mismatch_windows\": int(_basis_mismatch_count[0])  # windows requested 'text_clean' but fell back to 'text'\n",
        "})\n",
        "meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print({\n",
        "    \"windows\": N,\n",
        "    \"unique_topics_excl_noise\": int(len(set([t for t in topics if t!=-1]))),\n",
        "    \"topics_path\": str(TOP_OUT / \"topics.parquet\"),\n",
        "    \"fallback\": fallback_used,\n",
        "    \"basis_mismatch_windows\": int(_basis_mismatch_count[0]),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVMfr_dvBqhj"
      },
      "outputs": [],
      "source": [
        "# 6.3 — Topic stability & churn metrics: write topic_metrics.parquet (with noise_rate + NaN reassignment when no valid)\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "TOP_OUT  = (BASE_DIR / \"outputs\" / \"bertopic\").resolve()\n",
        "EMB_DIR  = TOP_OUT / \"embeddings\"\n",
        "\n",
        "assign = pd.read_parquet(TOP_OUT / \"topics.parquet\").sort_values([\"article_id\",\"version_id\",\"win_id\"])\n",
        "keys_rows = pd.read_parquet(EMB_DIR / \"embedding_rows.parquet\")  # row_id per (article,version,win)\n",
        "emb = np.load(next(EMB_DIR.glob(\"sbert_*.npy\")))  # (N, d)\n",
        "\n",
        "# Build (article,version,win) -> embedding row_id\n",
        "row_map = {(str(a),int(v),int(w)): int(rid) for a,v,w,rid in keys_rows[[\"article_id\",\"version_id\",\"win_id\",\"row_id\"]].itertuples(index=False)}\n",
        "\n",
        "def _topic_entropy(counts):\n",
        "    total = counts.sum()\n",
        "    if total <= 0:\n",
        "        return np.nan\n",
        "    p = counts[counts>0] / total\n",
        "    return float(-(p * np.log(p)).sum())\n",
        "\n",
        "def _coherence_of_topic(indices):\n",
        "    if len(indices) < 2:\n",
        "        return np.nan\n",
        "    X = emb[indices]  # normalized\n",
        "    c = X.mean(axis=0); n = np.linalg.norm(c); c = c/n if n>0 else c\n",
        "    sims = X @ c\n",
        "    return float(np.mean(sims))\n",
        "\n",
        "# ---- Per-version metrics (add noise_rate) ----\n",
        "ver_rows = []\n",
        "for (art, vid), g in assign.groupby([\"article_id\",\"version_id\"], sort=False):\n",
        "    total_wins = int(g.shape[0])\n",
        "    noise_ct = int((g[\"topic_id\"] == -1).sum())\n",
        "    noise_rate = float(noise_ct / total_wins) if total_wins > 0 else np.nan\n",
        "\n",
        "    g_non = g[g[\"topic_id\"]!=-1]\n",
        "    n_topics = int(g_non[\"topic_id\"].nunique()) if not g_non.empty else 0\n",
        "    counts = g_non[\"topic_id\"].value_counts().sort_index().to_numpy() if not g_non.empty else np.array([0])\n",
        "    ent_excl = _topic_entropy(counts) if n_topics>0 else np.nan\n",
        "    counts_incl = np.append(counts, noise_ct)\n",
        "    ent_incl = _topic_entropy(counts_incl) if counts_incl.sum()>0 else np.nan\n",
        "\n",
        "    coh_vals=[]\n",
        "    for tid, gt in g_non.groupby(\"topic_id\"):\n",
        "        idx = [ row_map[(str(art), int(vid), int(w))] for w in gt[\"win_id\"].tolist() if (str(art), int(vid), int(w)) in row_map ]\n",
        "        if idx:\n",
        "            coh_vals.append(_coherence_of_topic(idx))\n",
        "    coh_mean = float(np.nanmean(coh_vals)) if len(coh_vals)>0 else np.nan\n",
        "    coh_median = float(np.nanmedian(coh_vals)) if len(coh_vals)>0 else np.nan\n",
        "\n",
        "    ver_rows.append({\n",
        "        \"level\": \"doc\",\n",
        "        \"article_id\": str(art), \"version_id\": int(vid),\n",
        "        \"n_topics_doc\": int(n_topics),\n",
        "        \"topic_entropy\": ent_excl,\n",
        "        \"topic_entropy_incl_noise\": ent_incl,\n",
        "        \"coherence_mean\": coh_mean,\n",
        "        \"coherence_median\": coh_median,\n",
        "        \"noise_rate\": noise_rate,  # NEW\n",
        "    })\n",
        "\n",
        "df_ver = pd.DataFrame(ver_rows).sort_values([\"article_id\",\"version_id\"]).reset_index(drop=True)\n",
        "\n",
        "# ---- Adjacent-pair churn & overlap (align by win_id) ----\n",
        "pair_rows = []\n",
        "for art, gA in assign.groupby(\"article_id\", sort=False):\n",
        "    gA = gA.sort_values([\"version_id\",\"win_id\"])\n",
        "    versions = sorted(gA[\"version_id\"].unique().tolist())\n",
        "    for i in range(len(versions)-1):\n",
        "        v1, v2 = int(versions[i]), int(versions[i+1])\n",
        "        A = gA[gA[\"version_id\"]==v1]\n",
        "        B = gA[gA[\"version_id\"]==v2]\n",
        "        merged = pd.merge(A[[\"win_id\",\"topic_id\"]], B[[\"win_id\",\"topic_id\"]],\n",
        "                          on=\"win_id\", suffixes=(\"_a\",\"_b\"))\n",
        "        valid = merged[(merged[\"topic_id_a\"]!=-1) & (merged[\"topic_id_b\"]!=-1)]\n",
        "        if valid.shape[0] == 0:\n",
        "            reassignment = np.nan  # FIX: no valid aligned non-noise windows → NaN, not 0\n",
        "        else:\n",
        "            reassignment = float((valid[\"topic_id_a\"] != valid[\"topic_id_b\"]).sum() / valid.shape[0])\n",
        "\n",
        "        ta = sorted([t for t in A[\"topic_id\"].unique() if t!=-1])\n",
        "        tb = sorted([t for t in B[\"topic_id\"].unique() if t!=-1])\n",
        "        if len(ta)==0 or len(tb)==0:\n",
        "            jaccard = np.nan; births = int(len(tb)); deaths = int(len(ta))\n",
        "        else:\n",
        "            mat = np.zeros((len(ta), len(tb)), dtype=np.int64)\n",
        "            for i_t, t in enumerate(ta):\n",
        "                Wa = set(A.loc[A[\"topic_id\"]==t, \"win_id\"].tolist())\n",
        "                for j_t, u in enumerate(tb):\n",
        "                    Wb = set(B.loc[B[\"topic_id\"]==u, \"win_id\"].tolist())\n",
        "                    mat[i_t, j_t] = len(Wa & Wb)\n",
        "            cost = -mat\n",
        "            ri, cj = linear_sum_assignment(cost)\n",
        "            inter = int(mat[ri, cj].sum())\n",
        "            union = 0\n",
        "            for i_t, j_t in zip(ri, cj):\n",
        "                Wa = set(A.loc[A[\"topic_id\"]==ta[i_t], \"win_id\"].tolist())\n",
        "                Wb = set(B.loc[B[\"topic_id\"]==tb[j_t], \"win_id\"].tolist())\n",
        "                union += len(Wa | Wb)\n",
        "            jaccard = float(inter/union) if union>0 else np.nan\n",
        "            births = int(len(set(tb) - set(tb[j] for j in cj)))\n",
        "            deaths = int(len(set(ta) - set(ta[i] for i in ri)))\n",
        "\n",
        "        va = df_ver[(df_ver.article_id==str(art)) & (df_ver.version_id==v1)].iloc[0]\n",
        "        vb = df_ver[(df_ver.article_id==str(art)) & (df_ver.version_id==v2)].iloc[0]\n",
        "        dn_topics = int((vb[\"n_topics_doc\"] or 0) - (va[\"n_topics_doc\"] or 0))\n",
        "        d_entropy = float((vb[\"topic_entropy\"] - va[\"topic_entropy\"])) if pd.notna(va[\"topic_entropy\"]) and pd.notna(vb[\"topic_entropy\"]) else np.nan\n",
        "        d_coh = float((vb[\"coherence_mean\"] - va[\"coherence_mean\"])) if pd.notna(va[\"coherence_mean\"]) and pd.notna(vb[\"coherence_mean\"]) else np.nan\n",
        "\n",
        "        regularization_cue = (dn_topics < 0) and ( (not pd.isna(d_entropy)) and (d_entropy < 0) ) and ( (not pd.isna(d_coh)) and (d_coh > 0) )\n",
        "\n",
        "        pair_rows.append({\n",
        "            \"level\": \"pair\",\n",
        "            \"article_id\": str(art),\n",
        "            \"from_version\": v1, \"to_version\": v2,\n",
        "            \"reassignment_rate\": reassignment,\n",
        "            \"topic_overlap_jaccard\": jaccard,\n",
        "            \"births\": births, \"deaths\": deaths,\n",
        "            \"delta_n_topics\": dn_topics,\n",
        "            \"delta_entropy\": d_entropy,\n",
        "            \"delta_coherence_mean\": d_coh,\n",
        "            \"regularization_cue\": bool(regularization_cue),\n",
        "        })\n",
        "\n",
        "df_pair = pd.DataFrame(pair_rows).sort_values([\"article_id\",\"from_version\"]).reset_index(drop=True)\n",
        "\n",
        "# ---- Write metrics ----\n",
        "out = pd.concat([df_ver, df_pair], ignore_index=True, sort=False)\n",
        "out.to_parquet(TOP_OUT / \"topic_metrics.parquet\", index=False)\n",
        "\n",
        "# Update metadata (add summary of noise_rate)\n",
        "meta_path = TOP_OUT / \"metadata.json\"\n",
        "meta = json.loads(meta_path.read_text()) if meta_path.exists() else {}\n",
        "try:\n",
        "    mean_noise = float(df_ver[\"noise_rate\"].dropna().mean()) if not df_ver.empty else None\n",
        "except Exception:\n",
        "    mean_noise = None\n",
        "meta.update({\n",
        "    \"metrics_written\": True,\n",
        "    \"articles\": int(assign[\"article_id\"].nunique()),\n",
        "    \"windows\": int(assign.shape[0]),\n",
        "    \"mean_noise_rate\": mean_noise,\n",
        "})\n",
        "meta_path.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print({\n",
        "    \"doc_rows\": int(df_ver.shape[0]),\n",
        "    \"pair_rows\": int(df_pair.shape[0]),\n",
        "    \"mean_noise_rate\": mean_noise,\n",
        "    \"path\": str(TOP_OUT / \"topic_metrics.parquet\")\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtBS8NwLPFBM"
      },
      "outputs": [],
      "source": [
        "# 6.3-safety — ensure metrics fields exist on disk (idempotent)\n",
        "import pandas as pd, json, pathlib\n",
        "p = pathlib.Path(\"outputs/bertopic/topic_metrics.parquet\")\n",
        "if p.exists():\n",
        "    df = pd.read_parquet(p)\n",
        "    changed = False\n",
        "    if \"Topic\" in df.columns and \"noise_rate\" not in df.columns:\n",
        "        total = int(df[\"Count\"].sum()) if \"Count\" in df.columns else None\n",
        "        noise_mask = (df[\"Topic\"] == -1).astype(\"int\")\n",
        "        df[\"noise_rate\"] = (noise_mask if total is None else noise_mask / max(total, 1))\n",
        "        changed = True\n",
        "    if {\"reassignment_events\",\"unique_docs\"}.issubset(df.columns):\n",
        "        if \"reassignment_denominator\" not in df.columns or \"reassignment_rate\" not in df.columns:\n",
        "            denom = df[\"unique_docs\"].where(df[\"unique_docs\"] > 0, 1)\n",
        "            df[\"reassignment_denominator\"] = denom\n",
        "            df[\"reassignment_rate\"] = df[\"reassignment_events\"] / denom\n",
        "            changed = True\n",
        "    if changed:\n",
        "        df.to_parquet(p, index=False)\n",
        "        print(\"6.3-safety: metrics augmented and saved.\")\n",
        "    else:\n",
        "        print(\"6.3-safety: metrics already complete; no change.\")\n",
        "else:\n",
        "    print(\"6.3-safety: topic_metrics.parquet not found (skip).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "514T3LzRF36N"
      },
      "outputs": [],
      "source": [
        "# 6.4 — BERTopic: visuals — topic timeline & coherence (robust)\n",
        "import os, re, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# inline backend\n",
        "try:\n",
        "    ip = get_ipython()\n",
        "    if ip: ip.run_line_magic(\"matplotlib\", \"inline\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\", Path.cwd())).resolve()\n",
        "TOP_OUT  = (BASE_DIR / \"outputs\" / \"bertopic\").resolve()\n",
        "PLOT_DIR = TOP_OUT / \"plots\"\n",
        "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "assign_path  = TOP_OUT / \"topics.parquet\"\n",
        "metrics_path = TOP_OUT / \"topic_metrics.parquet\"\n",
        "\n",
        "if not assign_path.exists():\n",
        "    raise FileNotFoundError(f\"Missing topics assignments at {assign_path}\")\n",
        "assign = pd.read_parquet(assign_path)\n",
        "\n",
        "# Load metrics if present, and keep ONLY per-version rows (version_id not null)\n",
        "if metrics_path.exists():\n",
        "    raw_metrics = pd.read_parquet(metrics_path)\n",
        "    if \"version_id\" in raw_metrics.columns:\n",
        "        mver = raw_metrics.loc[raw_metrics[\"version_id\"].notna()].copy()\n",
        "        # coerce to int safely\n",
        "        mver[\"version_id\"] = mver[\"version_id\"].astype(int)\n",
        "        # ensure required cols exist\n",
        "        for col in [\"coherence_mean\",\"coherence_median\",\"n_topics_doc\",\"topic_entropy\",\"topic_entropy_incl_noise\"]:\n",
        "            if col not in mver.columns: mver[col] = np.nan\n",
        "    else:\n",
        "        mver = pd.DataFrame(columns=[\"article_id\",\"version_id\",\"coherence_mean\",\"n_topics_doc\",\n",
        "                                     \"topic_entropy\",\"topic_entropy_incl_noise\"])\n",
        "else:\n",
        "    mver = pd.DataFrame(columns=[\"article_id\",\"version_id\",\"coherence_mean\",\"n_topics_doc\",\n",
        "                                 \"topic_entropy\",\"topic_entropy_incl_noise\"])\n",
        "\n",
        "# If no n_topics_doc provided, derive a minimal proxy from assignments (excl. noise)\n",
        "if mver.empty or \"n_topics_doc\" not in mver.columns or mver[\"n_topics_doc\"].isna().all():\n",
        "    nt = (assign[assign[\"topic_id\"]!=-1]\n",
        "          .groupby([\"article_id\",\"version_id\"])[\"topic_id\"].nunique()\n",
        "          .rename(\"n_topics_doc\").reset_index())\n",
        "    mver = pd.merge(mver, nt, on=[\"article_id\",\"version_id\"], how=\"outer\", suffixes=(\"\",\"_derived\"))\n",
        "    if \"n_topics_doc_derived\" in mver:\n",
        "        mver[\"n_topics_doc\"] = mver[\"n_topics_doc\"].fillna(mver[\"n_topics_doc_derived\"])\n",
        "        mver.drop(columns=[\"n_topics_doc_derived\"], inplace=True)\n",
        "\n",
        "# helpers\n",
        "def safe_slug(s: str) -> str:\n",
        "    s = str(s).strip().replace(\" \", \"-\")\n",
        "    return re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", s)[:120]\n",
        "\n",
        "SEED = int(os.environ.get(\"LSA_SEED\", \"42\"))\n",
        "rng = np.random.default_rng(SEED)\n",
        "MAX_ARTS = int(os.environ.get(\"LSA_MAX_ARTICLES\", \"10\"))\n",
        "slugs = sorted(assign[\"article_id\"].unique().tolist())[:MAX_ARTS]\n",
        "plots_index = {\"topic_timeline\": [], \"topic_coherence\": [], \"globals\": []}\n",
        "\n",
        "# ---- Global histogram of per-version topic counts (excl. noise)\n",
        "tmp = (assign[assign[\"topic_id\"]!=-1]\n",
        "       .groupby([\"article_id\",\"version_id\"])[\"topic_id\"].nunique())\n",
        "vals = tmp.to_numpy()\n",
        "fig, ax = plt.subplots(figsize=(7,4))\n",
        "if vals.size > 0:\n",
        "    ax.hist(vals, bins=min(20, max(5, int(np.sqrt(vals.size)))), edgecolor=\"black\")\n",
        "    ax.set_title(\"Distribution of per-version topic counts (excl. noise)\")\n",
        "    ax.set_xlabel(\"# unique topics\"); ax.set_ylabel(\"Frequency\")\n",
        "else:\n",
        "    ax.text(0.5,0.5,\"No non-noise topics\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "out = PLOT_DIR / \"topic_counts_hist.png\"\n",
        "plt.savefig(out, dpi=150); plt.show(); plt.close()\n",
        "plots_index[\"globals\"].append(str(out))\n",
        "\n",
        "# ---- Per-article plots\n",
        "for art in slugs:\n",
        "    g  = assign.loc[assign[\"article_id\"]==art].sort_values([\"version_id\",\"win_id\"])\n",
        "    mv = mver.loc[mver[\"article_id\"]==art].dropna(subset=[\"version_id\"]).sort_values(\"version_id\")\n",
        "\n",
        "    # 1) Topic timeline (scatter of windows; color by topic)\n",
        "    if not g.empty:\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        topics = sorted(g[\"topic_id\"].unique().tolist())\n",
        "        cmap = plt.get_cmap(\"tab20\")\n",
        "        colors = {tid: (0.75,0.75,0.75,0.6) if tid==-1 else cmap(len(colors)%20) if 'colors' in locals() else cmap(0)\n",
        "                  for tid in topics}\n",
        "        # rebuild mapping deterministically\n",
        "        colors = {tid: ((0.75,0.75,0.75,0.6) if tid==-1 else plt.get_cmap(\"tab20\")(i % 20))\n",
        "                  for i, tid in enumerate(topics)}\n",
        "\n",
        "        y = g[\"topic_id\"].to_numpy(dtype=float)\n",
        "        yj = y + rng.normal(0, 0.04, size=y.shape)  # jitter\n",
        "        ax.scatter(g[\"version_id\"].to_numpy(int), yj,\n",
        "                   c=[colors[int(t)] for t in g[\"topic_id\"]],\n",
        "                   s=18, alpha=0.85, edgecolors=\"none\")\n",
        "        ax.set_xlabel(\"Version (v1..v4)\")\n",
        "        ax.set_ylabel(\"Topic ID (-1=noise)\")\n",
        "        ax.set_title(f\"Topic timeline — {art}\")\n",
        "        ax.set_xticks(sorted(g[\"version_id\"].unique().tolist()))\n",
        "        ax.grid(True, axis=\"y\", alpha=0.2, linestyle=\":\")\n",
        "        plt.tight_layout()\n",
        "        out1 = PLOT_DIR / f\"topic_timeline_{safe_slug(art)}.png\"\n",
        "        plt.savefig(out1, dpi=150); plt.show(); plt.close()\n",
        "        plots_index[\"topic_timeline\"].append(str(out1))\n",
        "\n",
        "    # 2) Coherence bars per version (with #topics overlay)\n",
        "    if not mv.empty:\n",
        "        # ensure unique per version (drop duplicates if any)\n",
        "        mv = mv.drop_duplicates(subset=[\"article_id\",\"version_id\"])\n",
        "        x = mv[\"version_id\"].astype(int).to_numpy()\n",
        "        y = mv[\"coherence_mean\"].astype(float).to_numpy() if \"coherence_mean\" in mv else np.full_like(x, np.nan, dtype=float)\n",
        "        nt = mv[\"n_topics_doc\"].astype(float).to_numpy() if \"n_topics_doc\" in mv else np.full_like(x, np.nan, dtype=float)\n",
        "\n",
        "        fig, ax1 = plt.subplots(figsize=(7,4))\n",
        "        # bars for coherence (NaNs will simply not render bars)\n",
        "        ax1.bar(x, y, width=0.6, alpha=0.85)\n",
        "        ax1.set_ylim(0, 1.0)\n",
        "        ax1.set_ylabel(\"Coherence (mean)\")\n",
        "        ax1.set_xlabel(\"Version\")\n",
        "        xticks = np.unique(x)\n",
        "        ax1.set_xticks(xticks)\n",
        "        ax1.set_title(f\"Topic coherence — {art}\")\n",
        "\n",
        "        # overlay #topics if available\n",
        "        if np.isfinite(nt).any():\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(x, nt, marker=\"o\", linewidth=1.5, alpha=0.8)\n",
        "            ymax = max(1, int(np.nanmax(nt)) + 1)\n",
        "            ax2.set_ylim(0, ymax)\n",
        "            ax2.set_ylabel(\"# topics\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        out2 = PLOT_DIR / f\"topic_coherence_{safe_slug(art)}.png\"\n",
        "        plt.savefig(out2, dpi=150); plt.show(); plt.close()\n",
        "        plots_index[\"topic_coherence\"].append(str(out2))\n",
        "\n",
        "# manifest\n",
        "(PLOT_DIR / \"plots_index.json\").write_text(json.dumps(plots_index, indent=2), encoding=\"utf-8\")\n",
        "print(\"✓ 6.4 visuals complete. Wrote:\", json.dumps(plots_index, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E-ViMRqaK-q"
      },
      "outputs": [],
      "source": [
        "RUN_TO = 7\n",
        "print({\"RUN_TO\": RUN_TO})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRMjfPgnF66P"
      },
      "outputs": [],
      "source": [
        "# cell 6.Z: bundle Modules 1–6 artifacts safely and stop (unless RUN_TO > 6)\n",
        "import os, json, time, hashlib, zipfile, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Fallback if cell 0.0 wasn't run\n",
        "try:\n",
        "    RUN_TO\n",
        "except NameError:\n",
        "    RUN_TO = int(os.environ.get(\"RUN_TO\", \"6\"))\n",
        "\n",
        "OUTPUTS = Path(\"outputs\")\n",
        "BUNDLES = OUTPUTS / \"bundles\"\n",
        "BUNDLES.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Only bundle these module folders under outputs/\n",
        "module_dirs = [\"lexical\", \"nltk\", \"spacy\", \"transformers\", \"semantic\", \"bertopic\"]\n",
        "\n",
        "stamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_path = BUNDLES / f\"module6_everything_{stamp}.zip\"\n",
        "\n",
        "def _sha256(p: Path, chunk: int = 65536) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(p, \"rb\") as f:\n",
        "        for b in iter(lambda: f.read(chunk), b\"\"):\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "manifest = {\"zip\": str(zip_path), \"files\": []}\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED, compresslevel=6) as z:\n",
        "    for d in module_dirs:\n",
        "        root = OUTPUTS / d\n",
        "        if root.exists() and root.is_dir():\n",
        "            for p in root.rglob(\"*\"):\n",
        "                if p.is_file():\n",
        "                    arc = p.relative_to(OUTPUTS)\n",
        "                    z.write(p, arc)\n",
        "                    manifest[\"files\"].append({\n",
        "                        \"path\": str(arc),\n",
        "                        \"sha256\": _sha256(p),\n",
        "                        \"bytes\": p.stat().st_size\n",
        "                    })\n",
        "\n",
        "# Write a manifest alongside the zip for quick inspection\n",
        "(BUNDLES / f\"manifest_{stamp}.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Bundled {len(manifest['files'])} files → {zip_path}\")\n",
        "\n",
        "if RUN_TO <= 6:\n",
        "    raise SystemExit(0)  # default: stop after bundling\n",
        "else:\n",
        "    print(f\"6.Z: RUN_TO={RUN_TO} → skipping stop; continuing to 7.x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Ly4xk8cBZI"
      },
      "outputs": [],
      "source": [
        "# cell 7.0: pre-7.x cleanup (safe: old zips & pip caches only)\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "bundles = sorted((Path(\"outputs\") / \"bundles\").glob(\"module6_everything_*.zip\"))\n",
        "for z in bundles[:-1]:\n",
        "    try:\n",
        "        z.unlink(); print(\"Deleted old bundle:\", z)\n",
        "    except Exception as e:\n",
        "        print(\"Skip delete:\", z, e)\n",
        "\n",
        "try:\n",
        "    os.system(\"pip cache purge -q\"); print(\"Pip cache purged.\")\n",
        "except Exception as e:\n",
        "    print(\"Pip cache purge skipped:\", e)\n",
        "\n",
        "print(\"Pre-7.x cleanup finished (kept current outputs).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jay4Ev0JRLIn"
      },
      "outputs": [],
      "source": [
        "# cell 7.1 — rapidfuzz: install (CPU-friendly)\n",
        "# ------------------------------------------------\n",
        "# Installs only what's needed here; quiet, wheels-first.\n",
        "import sys, subprocess, pkgutil, json, os, random\n",
        "import importlib.util\n",
        "\n",
        "def need(pname: str) -> bool:\n",
        "    return importlib.util.find_spec(pname) is None\n",
        "\n",
        "to_install = []\n",
        "if need(\"rapidfuzz\"):            to_install.append(\"rapidfuzz==3.*\")\n",
        "if need(\"nltk\"):                 to_install.append(\"nltk>=3.8\")\n",
        "if need(\"pandas\"):               to_install.append(\"pandas>=2.0\")\n",
        "if need(\"numpy\"):                to_install.append(\"numpy>=1.23\")\n",
        "if need(\"matplotlib\"):           to_install.append(\"matplotlib>=3.7\")\n",
        "if need(\"pyarrow\"):              to_install.append(\"pyarrow>=14\")\n",
        "\n",
        "if to_install:\n",
        "    print(\"Installing:\", to_install)\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *to_install])\n",
        "\n",
        "# Ensure NLTK punkt is available (Module 2 should've fetched it, but be safe)\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Create output folders\n",
        "for p in [\n",
        "    \"outputs/rapidfuzz\",\n",
        "    \"outputs/rapidfuzz/plots\",\n",
        "    \"outputs/rapidfuzz/bundles\",\n",
        "]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "# Determinism knobs\n",
        "import numpy as np\n",
        "SEED = int(os.environ.get(\"LSA_SEED\", \"7\"))\n",
        "\n",
        "# Metadata stub\n",
        "meta_path = \"outputs/rapidfuzz/metadata.json\"\n",
        "stub = {\n",
        "    \"module\": 7,\n",
        "    \"seed\": SEED,\n",
        "    \"created\": __import__(\"datetime\").datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"version_order_source\": \"filename_prefix\",\n",
        "}\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(stub, f, indent=2)\n",
        "print(\"✓ 7.1 ready → metadata stub at\", meta_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlWFf411Q_lG"
      },
      "outputs": [],
      "source": [
        "# cell 7.2 — rapidfuzz: paraphrase entropy & repetition (crash-safe, blocked, thread-capped)\n",
        "# -----------------------------------------------------------------------------------------\n",
        "import os\n",
        "\n",
        "import re, json, math, gc\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rapidfuzz import distance, fuzz, process\n",
        "import nltk\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "SIM_METRIC = os.environ.get(\"LSA_RAPIDFUZZ_SIM\", \"levenshtein_normalized\")  # or \"token_set_ratio\"\n",
        "SAFE_MODE = int(os.environ.get(\"LSA_RF_SAFE_MODE\", \"1\"))  # 1=skip sentence-level table; 0=store it\n",
        "BLOCK = int(os.environ.get(\"LSA_RF_BLOCK\", \"64\"))         # blocked similarity batch\n",
        "TAU = float(os.environ.get(\"LSA_RAPIDFUZZ_TAU\", \"0.15\"))\n",
        "INTRA_WINDOW_M = int(os.environ.get(\"LSA_RAPIDFUZZ_INTRA_M\", \"12\"))\n",
        "CANDIDATE_POLICY = os.environ.get(\"LSA_RAPIDFUZZ_CANDIDATE\", \"same_win\")  # or \"same_win±1\"\n",
        "\n",
        "if SIM_METRIC == \"token_set_ratio\":\n",
        "    SIM_SCORER = fuzz.token_set_ratio      # 0..100\n",
        "    SIM_SCALE  = \"0..100\"\n",
        "    THETA_SAME_RAW  = float(os.environ.get(\"LSA_RAPIDFUZZ_THETA_SAME\",  \"90\"))\n",
        "    THETA_CROSS_RAW = float(os.environ.get(\"LSA_RAPIDFUZZ_THETA_CROSS\", \"95\"))\n",
        "    to_unit = lambda x: np.asarray(x, dtype=np.float64) / 100.0\n",
        "else:\n",
        "    SIM_SCORER = distance.Levenshtein.normalized_similarity  # 0..1\n",
        "    SIM_SCALE  = \"0..1\"\n",
        "    ts = float(os.environ.get(\"LSA_RAPIDFUZZ_THETA_SAME\",  \"0.90\"))\n",
        "    tc = float(os.environ.get(\"LSA_RAPIDFUZZ_THETA_CROSS\", \"0.95\"))\n",
        "    THETA_SAME_RAW  = ts/100.0 if ts > 1.0 else ts\n",
        "    THETA_CROSS_RAW = tc/100.0 if tc > 1.0 else tc\n",
        "    to_unit = lambda x: np.asarray(x, dtype=np.float64)\n",
        "\n",
        "# ---------------- Inputs ----------------\n",
        "WIN_PATH = \"outputs/nltk/fw_burstiness_windows.parquet\"\n",
        "if not os.path.exists(WIN_PATH):\n",
        "    raise FileNotFoundError(f\"Missing {WIN_PATH}. Run Module 2 first.\")\n",
        "win = pd.read_parquet(WIN_PATH).sort_values([\"article_id\",\"version_id\",\"win_id\"]).reset_index(drop=True)\n",
        "\n",
        "# texts: prefer in-memory DOCS; else load from inputs/\n",
        "def _sanitize_slug(s): return re.sub(r\"[^A-Za-z0-9._-]+\", \"-\", str(s).strip().lower()).strip(\"-\")\n",
        "def _read_text(fp):\n",
        "    with open(fp,\"rb\") as fh: return fh.read().decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "TEXTS = {}\n",
        "if \"DOCS\" in globals() and isinstance(globals()[\"DOCS\"], (list, tuple)):\n",
        "    for d in DOCS:\n",
        "        art = str(d.get(\"article_id\")); vid = int(d.get(\"version_id\"))\n",
        "        TEXTS[(art,vid)] = {\"text\": d.get(\"text\",\"\") or \"\", \"text_clean\": d.get(\"text_clean\") if isinstance(d.get(\"text_clean\"), str) else None}\n",
        "if not TEXTS:\n",
        "    for root in [\"inputs\",\"data\",\"content/inputs\",\"/content/inputs\"]:\n",
        "        if os.path.isdir(root):\n",
        "            for dirpath,_,files in os.walk(root):\n",
        "                for fn in files:\n",
        "                    m = re.match(r\"^(\\d{2})-(.+?)\\.(txt|md)$\", fn)\n",
        "                    if not m: continue\n",
        "                    vid = int(m.group(1)); art = _sanitize_slug(m.group(2))\n",
        "                    TEXTS[(art,vid)] = {\"text\": _read_text(os.path.join(dirpath,fn)), \"text_clean\": None}\n",
        "\n",
        "need = [(str(a),int(v)) for a,v in win[[\"article_id\",\"version_id\"]].drop_duplicates().itertuples(index=False) if (str(a),int(v)) not in TEXTS]\n",
        "if need:\n",
        "    raise RuntimeError(f\"Missing raw text for: {need[:4]} … (ensure inputs/01-<slug>.txt .. 04-<slug>.txt or rerun earlier modules)\")\n",
        "\n",
        "# sentence segmentation (NLTK Punkt) on the SAME basis as M2\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "@dataclass\n",
        "class SentRec:\n",
        "    sid:int; start:int; end:int; text:str  # lowercased, trimmed\n",
        "\n",
        "SENTS = {}  # (art,vid,basis)->list[SentRec]\n",
        "def get_text(art, vid, basis):\n",
        "    entry = TEXTS[(art,vid)]\n",
        "    return (entry[\"text_clean\"] if basis==\"text_clean\" and entry.get(\"text_clean\") else entry[\"text\"])\n",
        "\n",
        "def ensure_sentences(art, vid, basis):\n",
        "    key = (art,vid,basis)\n",
        "    if key in SENTS: return\n",
        "    t = get_text(art, vid, basis)\n",
        "    spans = list(tokenizer.span_tokenize(t))\n",
        "    SENTS[key] = [SentRec(i,a,b,t[a:b].strip().lower()) for i,(a,b) in enumerate(spans)]\n",
        "\n",
        "def window_sentences(row):\n",
        "    art = str(row.article_id); vid = int(row.version_id); basis = row.span_basis if isinstance(row.span_basis,str) else \"text\"\n",
        "    ensure_sentences(art,vid,basis)\n",
        "    recs = SENTS[(art,vid,basis)]\n",
        "    s0, s1 = int(row.sent_start_index), int(row.sent_end_index)\n",
        "    if not recs or s1 < s0: return []\n",
        "    s0 = max(0,s0); s1 = min(s1,len(recs)-1)\n",
        "    return recs[s0:s1+1]\n",
        "\n",
        "def candidate_texts_same_win(art, vid, wid, basis):\n",
        "    out=[]\n",
        "    versions = sorted(win.loc[win.article_id==art, \"version_id\"].unique().astype(int))\n",
        "    for v2 in versions:\n",
        "        if v2==vid: continue\n",
        "        wids = [wid] if CANDIDATE_POLICY==\"same_win\" else [wid-1, wid, wid+1]\n",
        "        for wj in wids:\n",
        "            wdf = win[(win.article_id==art)&(win.version_id==v2)&(win.win_id==wj)].head(1)\n",
        "            if wdf.empty: continue\n",
        "            ensure_sentences(art,v2,basis)\n",
        "            for rr in wdf.itertuples(index=False):\n",
        "                out.extend([s.text for s in window_sentences(rr)])\n",
        "    return out\n",
        "\n",
        "# blocked similarity helpers (keeps memory flat)\n",
        "def blocked_row_sims(q_text, cand_texts, block=BLOCK):\n",
        "    \"\"\"Return all similarity scores (raw scale) for a single query vs candidate list, computed in small blocks.\"\"\"\n",
        "    if not cand_texts: return np.empty((0,), dtype=np.float64)\n",
        "    scores = []\n",
        "    i = 0\n",
        "    while i < len(cand_texts):\n",
        "        chunk = cand_texts[i:i+block]\n",
        "        mat = process.cdist([q_text], chunk, scorer=SIM_SCORER, workers=1)\n",
        "        scores.append(mat.ravel().astype(np.float64))\n",
        "        i += block\n",
        "    return np.concatenate(scores, axis=0) if scores else np.empty((0,), dtype=np.float64)\n",
        "\n",
        "def intra_version_max_blocked(texts, M):\n",
        "    \"\"\"Max similarity within ±M neighbors per sentence, computed in blocks.\"\"\"\n",
        "    n = len(texts)\n",
        "    out = np.zeros((n,), dtype=np.float32)\n",
        "    for i in range(n):\n",
        "        lo, hi = max(0, i-M), min(n, i+M+1)\n",
        "        cands = texts[lo:i] + texts[i+1:hi]\n",
        "        if not cands:\n",
        "            continue\n",
        "        q = re.sub(r\"\\s+\",\" \", texts[i]).strip()\n",
        "        cands_norm = [re.sub(r\"\\s+\",\" \", t).strip() for t in cands]\n",
        "        if q in set(cands_norm):\n",
        "            out[i] = 100.0 if SIM_SCALE==\"0..100\" else 1.0\n",
        "            continue\n",
        "        smax = 0.0\n",
        "        j = 0\n",
        "        while j < len(cands):\n",
        "            chunk = cands[j:j+BLOCK]\n",
        "            mat = process.cdist([texts[i]], chunk, scorer=SIM_SCORER, workers=1)\n",
        "            smax = max(smax, float(np.max(mat)))\n",
        "            j += BLOCK\n",
        "        out[i] = smax\n",
        "    return out\n",
        "\n",
        "def softmax_stable(x, tau):\n",
        "    if x.size == 0: return x\n",
        "    z = (x / max(tau, 1e-6)).astype(np.float64)\n",
        "    z -= z.max(axis=-1, keepdims=True)\n",
        "    e = np.exp(z); d = e.sum(axis=-1, keepdims=True); d[d==0.0]=1.0\n",
        "    return e/d\n",
        "\n",
        "# --- main ---\n",
        "rows_win, rows_ver = [], []\n",
        "if not SAFE_MODE:\n",
        "    sent_entropy_rows = []\n",
        "\n",
        "windows_with_cands = 0\n",
        "\n",
        "for (art, vid, basis), g in win.groupby([\"article_id\",\"version_id\",\"span_basis\"], dropna=False):\n",
        "    art = str(art); vid = int(vid); basis = basis if isinstance(basis,str) else \"text\"\n",
        "    ensure_sentences(art, vid, basis)\n",
        "    ver_sents = SENTS[(art,vid,basis)]\n",
        "    ver_texts = [s.text for s in ver_sents]\n",
        "\n",
        "    # intra-version repetition (blocked)\n",
        "    intra_max = intra_version_max_blocked(ver_texts, INTRA_WINDOW_M)\n",
        "\n",
        "    H_accum, med_accum = [], []\n",
        "\n",
        "    for r in g.sort_values(\"win_id\").itertuples(index=False):\n",
        "        sents = window_sentences(r)\n",
        "        n_s = len(sents)\n",
        "        sent_texts = [s.text for s in sents]\n",
        "        cand_texts = candidate_texts_same_win(art, vid, int(r.win_id), basis)\n",
        "        n_c = len(cand_texts)\n",
        "        if n_c > 0: windows_with_cands += 1\n",
        "\n",
        "        if n_s == 0:\n",
        "            rows_win.append({\n",
        "                \"article_id\": art, \"version_id\": vid, \"version_tag\": f\"v{vid}\", \"doc_id\": f\"{art}__v{vid}\",\n",
        "                \"win_id\": int(r.win_id), \"win_label\": r.win_label, \"span_basis\": basis,\n",
        "                \"char_start\": int(r.char_start), \"char_end\": int(r.char_end),\n",
        "                \"sent_start_index\": int(r.sent_start_index), \"sent_end_index\": int(r.sent_end_index),\n",
        "                \"is_partial_tail\": bool(getattr(r, \"is_partial_tail\", False)),\n",
        "                \"n_sentences_win\": 0, \"n_candidates_win\": 0,\n",
        "                \"paraphrase_entropy_win\": np.nan, \"median_crossver_sim_win\": np.nan, \"max_crossver_sim_win\": np.nan,\n",
        "                \"repeat_within_rate_win\": np.nan, \"boilerplate_cross_rate_win\": np.nan,\n",
        "                \"repeat_run_max_len_win\": 0, \"repeat_exact_count_win\": 0, \"repeat_fuzzy_count_win\": 0,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        H_vals, med_sims, max_sims, boiler_flags = [], [], [], []\n",
        "\n",
        "        for i, q in enumerate(sent_texts):\n",
        "            row_raw = blocked_row_sims(q, cand_texts, BLOCK) if n_c else np.empty((0,), dtype=np.float64)\n",
        "\n",
        "            if row_raw.size >= 2:\n",
        "                sims01 = to_unit(row_raw)\n",
        "                P = softmax_stable(sims01, TAU)\n",
        "                denom = math.log(max(row_raw.size, 2))\n",
        "                H = float(-(P * np.log(np.maximum(P, 1e-12))).sum() / denom)\n",
        "            else:\n",
        "                H = np.nan\n",
        "            H_vals.append(H)\n",
        "\n",
        "            if row_raw.size > 0:\n",
        "                med_sims.append(float(np.median(row_raw)))\n",
        "                max_sims.append(float(np.max(row_raw)))\n",
        "                boiler_flags.append(bool(np.any(row_raw >= THETA_CROSS_RAW)))\n",
        "            else:\n",
        "                med_sims.append(np.nan)\n",
        "                max_sims.append(np.nan)\n",
        "                boiler_flags.append(False)\n",
        "\n",
        "            if not SAFE_MODE:\n",
        "                if 'sent_entropy_rows' in globals():\n",
        "                    sent_entropy_rows.append({\"article_id\": art, \"version_id\": vid, \"win_id\": int(r.win_id), \"sid\": i, \"H_s\": H})\n",
        "\n",
        "        # repetition inside this window (use precomputed doc-wide intra_max slice)\n",
        "        s0, s1 = int(r.sent_start_index), int(r.sent_end_index)\n",
        "        intra_slice = intra_max[s0:s1+1] if s1 >= s0 else np.array([], dtype=np.float32)\n",
        "        repeat_within = (intra_slice >= THETA_SAME_RAW) if intra_slice.size else np.array([], dtype=bool)\n",
        "\n",
        "        norm = [re.sub(r\"\\s+\",\" \", t).strip() for t in sent_texts]\n",
        "        counts = Counter(norm)\n",
        "        exact = sum(1 for t in norm if counts[t] > 1)\n",
        "        fuzzy = max(int(np.sum(repeat_within)) - exact, 0)\n",
        "\n",
        "        # longest consecutive repeat run (window)\n",
        "        run_max = 0; cur = 0\n",
        "        for flag in repeat_within:\n",
        "            cur = (cur + 1) if flag else 0\n",
        "            run_max = max(run_max, cur)\n",
        "\n",
        "        rows_win.append({\n",
        "            \"article_id\": art, \"version_id\": vid, \"version_tag\": f\"v{vid}\", \"doc_id\": f\"{art}__v{vid}\",\n",
        "            \"win_id\": int(r.win_id), \"win_label\": r.win_label, \"span_basis\": basis,\n",
        "            \"char_start\": int(r.char_start), \"char_end\": int(r.char_end),\n",
        "            \"sent_start_index\": int(r.sent_start_index), \"sent_end_index\": int(r.sent_end_index),\n",
        "            \"is_partial_tail\": bool(getattr(r, \"is_partial_tail\", False)),\n",
        "            \"n_sentences_win\": int(n_s), \"n_candidates_win\": int(n_c),\n",
        "            \"paraphrase_entropy_win\": float(np.nanmean(H_vals)) if H_vals else np.nan,\n",
        "            \"median_crossver_sim_win\": float(np.nanmedian(med_sims)) if med_sims else np.nan,\n",
        "            \"max_crossver_sim_win\": float(np.nanmax(max_sims)) if max_sims else np.nan,\n",
        "            \"repeat_within_rate_win\": (float(np.mean(repeat_within)) if repeat_within.size else np.nan),\n",
        "            \"boilerplate_cross_rate_win\": (float(np.mean(boiler_flags)) if boiler_flags else np.nan),\n",
        "            \"repeat_run_max_len_win\": int(run_max),\n",
        "            \"repeat_exact_count_win\": int(exact),\n",
        "            \"repeat_fuzzy_count_win\": int(fuzzy),\n",
        "        })\n",
        "\n",
        "        H_accum.extend([h for h in H_vals if not np.isnan(h)])\n",
        "        med_accum.extend([m for m in med_sims if not np.isnan(m)])\n",
        "\n",
        "    # version-level aggregates (robust run-length computation; avoids empty max())\n",
        "    flags = (intra_max >= THETA_SAME_RAW) if intra_max.size else np.array([], dtype=bool)\n",
        "    if flags.any():\n",
        "        b = np.r_[False, flags, False]\n",
        "        starts = np.flatnonzero(b[1:] & ~b[:-1])\n",
        "        ends   = np.flatnonzero(~b[1:] & b[:-1])\n",
        "        run_max_ver = int((ends - starts).max()) if starts.size else 0\n",
        "    else:\n",
        "        run_max_ver = 0\n",
        "\n",
        "    rows_ver.append({\n",
        "        \"article_id\": art, \"version_id\": vid, \"version_tag\": f\"v{vid}\", \"doc_id\": f\"{art}__v{vid}\",\n",
        "        \"paraphrase_entropy_ver\": float(np.nanmedian(H_accum)) if H_accum else np.nan,\n",
        "        \"median_crossver_sim_ver\": float(np.nanmedian(med_accum)) if med_accum else np.nan,\n",
        "        \"p90_crossver_sim_ver\": float(np.nanpercentile(med_accum,90)) if med_accum else np.nan,\n",
        "        \"repeat_within_rate_ver\": float(np.mean(flags)) if flags.size else np.nan,\n",
        "        \"boilerplate_cross_rate_ver\": np.nan,\n",
        "        \"repeat_run_max_len_ver\": run_max_ver\n",
        "    })\n",
        "\n",
        "    # housekeeping\n",
        "    del ver_sents, ver_texts, intra_max\n",
        "    gc.collect()\n",
        "\n",
        "# ---------------- DataFrames & dtypes ----------------\n",
        "df_win = pd.DataFrame(rows_win)\n",
        "df_ver = pd.DataFrame(rows_ver)\n",
        "if not SAFE_MODE:\n",
        "    df_sentH = pd.DataFrame(sent_entropy_rows)\n",
        "\n",
        "for c in [\"version_id\",\"win_id\",\"char_start\",\"char_end\",\"sent_start_index\",\"sent_end_index\",\n",
        "          \"n_sentences_win\",\"n_candidates_win\",\"repeat_run_max_len_win\",\n",
        "          \"repeat_exact_count_win\",\"repeat_fuzzy_count_win\"]:\n",
        "    if c in df_win.columns: df_win[c] = pd.to_numeric(df_win[c], errors=\"coerce\").astype(\"Int64\").astype(\"int64\")\n",
        "for c in [\"version_id\",\"repeat_run_max_len_ver\"]:\n",
        "    if c in df_ver.columns: df_ver[c] = pd.to_numeric(df_ver[c], errors=\"coerce\").astype(\"Int64\").astype(\"int64\")\n",
        "\n",
        "# ---------------- Save ----------------\n",
        "os.makedirs(\"outputs/rapidfuzz\", exist_ok=True)\n",
        "out_win = \"outputs/rapidfuzz/paraphrase_entropy.parquet\"\n",
        "out_ver = \"outputs/rapidfuzz/paraphrase_entropy_doc.parquet\"\n",
        "df_win.to_parquet(out_win, index=False)\n",
        "df_ver.to_parquet(out_ver, index=False)\n",
        "extras = []\n",
        "if not SAFE_MODE:\n",
        "    out_sent = \"outputs/rapidfuzz/paraphrase_entropy_sentences.parquet\"\n",
        "    df_sentH.to_parquet(out_sent, index=False)\n",
        "    extras.append(out_sent)\n",
        "\n",
        "# ---------------- Metadata ----------------\n",
        "meta_path = \"outputs/rapidfuzz/metadata.json\"\n",
        "meta = {}\n",
        "if os.path.exists(meta_path):\n",
        "    try:\n",
        "        with open(meta_path, \"r\") as f: meta = json.load(f)\n",
        "    except Exception:\n",
        "        meta = {}\n",
        "windows_total = int(len(df_win))\n",
        "meta.update({\n",
        "    \"version_order_source\": \"filename_prefix\",\n",
        "    \"sim_metric\": SIM_METRIC,\n",
        "    \"sim_scale\": SIM_SCALE,\n",
        "    \"theta_same_effective\": THETA_SAME_RAW,\n",
        "    \"theta_cross_effective\": THETA_CROSS_RAW,\n",
        "    \"tau_softmax\": TAU,\n",
        "    \"candidate_policy\": CANDIDATE_POLICY,\n",
        "    \"intra_window_M\": INTRA_WINDOW_M,\n",
        "    \"blocked_batch\": BLOCK,\n",
        "    \"safe_mode\": bool(SAFE_MODE),\n",
        "    \"articles\": int(df_win[\"article_id\"].nunique()),\n",
        "    \"versions_per_article_min\": int(df_win.groupby(\"article_id\")[\"version_id\"].nunique().min()),\n",
        "    \"versions_per_article_max\": int(df_win.groupby(\"article_id\")[\"version_id\"].nunique().max()),\n",
        "    \"windows_total\": windows_total,\n",
        "    \"windows_with_candidates\": int((df_win[\"n_candidates_win\"]>0).sum()),\n",
        "})\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\": \"7.2\",\n",
        "    \"rows_win\": len(df_win),\n",
        "    \"rows_ver\": len(df_ver),\n",
        "    \"saved\": [out_win, out_ver] + extras,\n",
        "    \"sim_metric\": SIM_METRIC, \"sim_scale\": SIM_SCALE,\n",
        "    \"theta_same_effective\": THETA_SAME_RAW, \"theta_cross_effective\": THETA_CROSS_RAW,\n",
        "    \"blocked_batch\": BLOCK, \"safe_mode\": bool(SAFE_MODE)\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCRX6pLCtBZQ"
      },
      "outputs": [],
      "source": [
        "# cell 7.2b — rapidfuzz: finalize doc-level rates & metadata (thresholds + boilerplate_ver)\n",
        "\n",
        "import os, json, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "WIN_FP = \"outputs/rapidfuzz/paraphrase_entropy.parquet\"\n",
        "DOC_FP = \"outputs/rapidfuzz/paraphrase_entropy_doc.parquet\"\n",
        "META_FP = \"outputs/rapidfuzz/metadata.json\"\n",
        "\n",
        "assert os.path.exists(WIN_FP), f\"Missing {WIN_FP}. Run 7.2 first.\"\n",
        "\n",
        "dfw = pd.read_parquet(WIN_FP).copy()\n",
        "\n",
        "# ----- (A) Recompute version-level repetition rates from window table -----\n",
        "def _wmean(values: pd.Series, weights: pd.Series) -> float:\n",
        "    v = values.astype(float)\n",
        "    w = weights.astype(float)\n",
        "    m = np.isfinite(v) & np.isfinite(w) & (w > 0)\n",
        "    if not m.any():\n",
        "        return float(\"nan\")\n",
        "    return float(np.sum(v[m] * w[m]) / np.sum(w[m]))\n",
        "\n",
        "grp_keys = [\"article_id\",\"version_id\",\"version_tag\",\"doc_id\"]\n",
        "if \"n_sentences_win\" not in dfw:\n",
        "    dfw[\"n_sentences_win\"] = 1\n",
        "\n",
        "agg_rows = []\n",
        "for _, g in dfw.groupby(grp_keys, sort=True):\n",
        "    w = g[\"n_sentences_win\"].fillna(1)\n",
        "    rep_within = _wmean(g[\"repeat_within_rate_win\"].fillna(0.0), w)\n",
        "    boiler = _wmean(g[\"boilerplate_cross_rate_win\"].fillna(0.0), w)\n",
        "    agg_rows.append({\n",
        "        \"article_id\": g[\"article_id\"].iloc[0],\n",
        "        \"version_id\": int(g[\"version_id\"].iloc[0]),\n",
        "        \"version_tag\": g[\"version_tag\"].iloc[0],\n",
        "        \"doc_id\": g[\"doc_id\"].iloc[0],\n",
        "        \"repeat_within_rate_ver\": rep_within,\n",
        "        \"boilerplate_cross_rate_ver\": boiler,\n",
        "        \"sentences_total_ver\": int(np.sum(w)),\n",
        "    })\n",
        "agg = pd.DataFrame(agg_rows)\n",
        "\n",
        "# Merge into existing doc table if present\n",
        "if os.path.exists(DOC_FP):\n",
        "    dfd = pd.read_parquet(DOC_FP).copy()\n",
        "    dfd = dfd.drop(columns=[\"repeat_within_rate_ver\",\"boilerplate_cross_rate_ver\",\"sentences_total_ver\"],\n",
        "                   errors=\"ignore\").merge(agg, on=grp_keys, how=\"left\")\n",
        "else:\n",
        "    # If 7.2 didn't create a doc table, seed one from window aggregates\n",
        "    dfd = agg.copy()\n",
        "    # Optional: initialize expected columns so schema matches downstream joins\n",
        "    for col in [\"paraphrase_entropy_ver\",\"median_crossver_sim_ver\",\"p90_crossver_sim_ver\",\"repeat_run_max_len_ver\"]:\n",
        "        if col not in dfd.columns:\n",
        "            dfd[col] = np.nan\n",
        "\n",
        "# Persist\n",
        "os.makedirs(os.path.dirname(DOC_FP), exist_ok=True)\n",
        "dfd.to_parquet(DOC_FP, index=False)\n",
        "\n",
        "# ----- (B) Patch metadata with explicit thresholds & coverage -----\n",
        "meta = {}\n",
        "if os.path.exists(META_FP):\n",
        "    with open(META_FP, \"r\") as f:\n",
        "        try:\n",
        "            meta = json.load(f)\n",
        "        except Exception:\n",
        "            meta = {}\n",
        "\n",
        "sim_scale = str(meta.get(\"sim_scale\") or \"0..1\").strip()\n",
        "# If thresholds already present, keep them; else set sensible defaults on the recorded scale.\n",
        "if \"theta_same\" not in meta or meta.get(\"theta_same\") in (None, \"\", \"null\"):\n",
        "    meta[\"theta_same\"]  = 0.90 if sim_scale == \"0..1\" else 90.0\n",
        "if \"theta_cross\" not in meta or meta.get(\"theta_cross\") in (None, \"\", \"null\"):\n",
        "    meta[\"theta_cross\"] = 0.95 if sim_scale == \"0..1\" else 95.0\n",
        "\n",
        "meta[\"repeat_thresholds_applied\"] = True\n",
        "meta[\"candidate_policy\"] = meta.get(\"candidate_policy\") or \"same_win\"\n",
        "\n",
        "# Coverage counts\n",
        "meta[\"windows_total\"] = int(len(dfw))\n",
        "meta[\"windows_with_candidates\"] = int((dfw[\"n_candidates_win\"].fillna(0) > 0).sum())\n",
        "meta[\"windows_no_candidates\"] = int(meta[\"windows_total\"] - meta[\"windows_with_candidates\"])\n",
        "meta[\"updated_at\"] = int(time.time())\n",
        "\n",
        "with open(META_FP, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print({\n",
        "    \"cell_id\": \"7.2b\",\n",
        "    \"doc_rows\": len(dfd),\n",
        "    \"windows_total\": meta[\"windows_total\"],\n",
        "    \"windows_with_candidates\": meta[\"windows_with_candidates\"],\n",
        "    \"theta_same\": meta[\"theta_same\"],\n",
        "    \"theta_cross\": meta[\"theta_cross\"],\n",
        "    \"sim_scale\": sim_scale,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER1UbvQr4--Q"
      },
      "outputs": [],
      "source": [
        "# ===== cell 7.X1 — disk auditor: find big dirs/files quickly =====\n",
        "import os, sys, math, time\n",
        "from pathlib import Path\n",
        "\n",
        "def hsize(n):  # human friendly\n",
        "    for u in ['B','KB','MB','GB','TB']:\n",
        "        if n < 1024: return f\"{n:,.1f} {u}\"\n",
        "        n /= 1024\n",
        "    return f\"{n:.1f} PB\"\n",
        "\n",
        "roots = [\n",
        "    Path.cwd() / \"outputs\",\n",
        "    Path.home() / \".cache\" / \"huggingface\",\n",
        "    Path.home() / \".cache\" / \"pip\",\n",
        "    Path.home() / \"nltk_data\",\n",
        "]\n",
        "\n",
        "def dir_size(p: Path) -> int:\n",
        "    total = 0\n",
        "    if not p.exists(): return 0\n",
        "    for root, _, files in os.walk(p, followlinks=False):\n",
        "        for f in files:\n",
        "            try:\n",
        "                total += (Path(root)/f).stat().st_size\n",
        "            except Exception:\n",
        "                pass\n",
        "    return total\n",
        "\n",
        "print(\"Scanning…\")\n",
        "totals = []\n",
        "for r in roots:\n",
        "    sz = dir_size(r)\n",
        "    totals.append((r, sz))\n",
        "totals.sort(key=lambda x: x[1], reverse=True)\n",
        "for r, sz in totals:\n",
        "    print(f\"{str(r):<60} {hsize(sz)}\")\n",
        "\n",
        "# Top 25 biggest files under your project tree\n",
        "big_files = []\n",
        "for root, _, files in os.walk(Path.cwd(), followlinks=False):\n",
        "    for f in files:\n",
        "        p = Path(root)/f\n",
        "        try:\n",
        "            s = p.stat().st_size\n",
        "        except Exception:\n",
        "            continue\n",
        "        big_files.append((s, p))\n",
        "big_files.sort(reverse=True)\n",
        "print(\"\\nTop 25 largest files under project:\")\n",
        "for s, p in big_files[:25]:\n",
        "    print(f\"{hsize(s):>10}  {p}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJTCkUMoRFO9"
      },
      "outputs": [],
      "source": [
        "# cell 7.3 — rapidfuzz: visuals — entropy & repetition (fixed: single render per plot)\n",
        "\n",
        "import os, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PLOTS_DIR = \"outputs/rapidfuzz/plots\"\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Load window- and version-level metrics (window file is required) ----\n",
        "win_fp = \"outputs/rapidfuzz/paraphrase_entropy.parquet\"\n",
        "doc_fp = \"outputs/rapidfuzz/paraphrase_entropy_doc.parquet\"\n",
        "\n",
        "assert os.path.exists(win_fp), f\"Missing {win_fp}. Run 7.2 first.\"\n",
        "dfw = pd.read_parquet(win_fp).copy()\n",
        "dfw[\"version_id\"] = dfw[\"version_id\"].astype(int)\n",
        "\n",
        "dfd = pd.read_parquet(doc_fp).copy() if os.path.exists(doc_fp) else None\n",
        "\n",
        "# Helper: expand a window mean to sentence-level weights to approximate a ridgeline\n",
        "def expand_entropy_rows(g: pd.DataFrame) -> np.ndarray:\n",
        "    if \"n_sentences_win\" in g and g[\"n_sentences_win\"].notna().any():\n",
        "        n = g[\"n_sentences_win\"].fillna(1).clip(lower=1).astype(int).to_numpy()\n",
        "    else:\n",
        "        n = np.ones(len(g), dtype=int)\n",
        "    vals = g[\"paraphrase_entropy_win\"].astype(float).to_numpy()\n",
        "    # Repeat each window’s entropy by its sentence count\n",
        "    return np.repeat(vals, n)\n",
        "\n",
        "plots_index = {\"entropy_ridge\": [], \"repetition_heatmaps\": []}\n",
        "\n",
        "# =========================\n",
        "# 1) Global entropy ridgeline by version (single figure)\n",
        "# =========================\n",
        "fig, ax = plt.subplots(figsize=(9, 5.5))\n",
        "versions = sorted(dfw[\"version_id\"].unique())\n",
        "offset = 1.2  # vertical spacing between ridges\n",
        "\n",
        "for i, vid in enumerate(versions, start=1):\n",
        "    vals = expand_entropy_rows(dfw[dfw[\"version_id\"] == vid])\n",
        "    vals = vals[np.isfinite(vals)]\n",
        "    if vals.size == 0:\n",
        "        continue\n",
        "    y = i * offset\n",
        "    # Histogram \"ridgeline\"\n",
        "    hist, bins = np.histogram(vals, bins=30, range=(0.0, 1.0), density=True)\n",
        "    hist = np.minimum(hist, np.percentile(hist, 99))  # clip very tall bars\n",
        "    ax.fill_between(bins[:-1], y, y + hist / hist.max() * 0.9, alpha=0.45)\n",
        "    ax.plot([np.median(vals)], [y + 0.95], marker=\"|\", markersize=14, lw=0,\n",
        "            label=f\"v{vid} median={np.median(vals):.2f}\")\n",
        "\n",
        "ax.set_title(\"Entropy ridge by version\")\n",
        "ax.set_xlabel(\"Paraphrase entropy (sentence-level, approximated)\")\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_yticks([i * offset + 0.95 for i in range(1, len(versions)+1)])\n",
        "ax.set_yticklabels([f\"v{vid}\" for vid in versions])\n",
        "ax.grid(alpha=0.2, axis=\"x\")\n",
        "ax.legend(loc=\"lower left\", ncols=min(len(versions), 4), fontsize=9, frameon=False)\n",
        "\n",
        "ridge_path = os.path.join(PLOTS_DIR, \"entropy_ridge.png\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(ridge_path, dpi=150)\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "plots_index[\"entropy_ridge\"].append(ridge_path)\n",
        "\n",
        "# =========================\n",
        "# 2) Repetition heatmap(s) — one per article (capped to 10)\n",
        "# =========================\n",
        "slugs = sorted(dfw[\"article_id\"].unique())[:10]\n",
        "for slug in slugs:\n",
        "    g = dfw[dfw[\"article_id\"] == slug].copy()\n",
        "    # Build a coarse sentence index by expanding windows by their sentence counts\n",
        "    # and painting each expanded position with that window’s repeat rate.\n",
        "    rows = []\n",
        "    for vid, gv in g.groupby(\"version_id\", sort=True):\n",
        "        if \"n_sentences_win\" in gv:\n",
        "            counts = gv[\"n_sentences_win\"].fillna(1).clip(lower=1).astype(int).to_numpy()\n",
        "        else:\n",
        "            counts = np.ones(len(gv), dtype=int)\n",
        "        vals = gv[\"repeat_within_rate_win\"].fillna(0.0).to_numpy()\n",
        "        seq = np.concatenate([np.full(c, v, dtype=float) for v, c in zip(vals, counts)]) if len(vals) else np.array([], float)\n",
        "        rows.append(seq)\n",
        "\n",
        "    # Pad to the same length for plotting\n",
        "    max_len = max((len(r) for r in rows), default=0)\n",
        "    if max_len == 0:\n",
        "        # Nothing to plot; skip gracefully\n",
        "        continue\n",
        "    M = np.vstack([\n",
        "        np.pad(r, (0, max_len - len(r)), constant_values=np.nan) for r in rows\n",
        "    ])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 3.6))\n",
        "    im = ax.imshow(np.nan_to_num(M, nan=0.0), aspect=\"auto\", vmin=0.0, vmax=1.0, cmap=\"viridis\")\n",
        "    ax.set_title(f\"Repetition heatmap — {slug}\")\n",
        "    ax.set_ylabel(\"Version (v1..v4)\")\n",
        "    ax.set_xlabel(\"Sentence index (approx.)\")\n",
        "    ax.set_yticks(range(len(rows)))\n",
        "    ax.set_yticklabels([f\"v{v}\" for v in sorted(g[\"version_id\"].unique())])\n",
        "    cbar = fig.colorbar(im, ax=ax)\n",
        "    cbar.set_label(\"repeat_within_rate (0..1)\")\n",
        "\n",
        "    heat_path = os.path.join(PLOTS_DIR, f\"repetition_heatmap_{slug}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(heat_path, dpi=150)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    plots_index[\"repetition_heatmaps\"].append(heat_path)\n",
        "\n",
        "# Manifest\n",
        "with open(os.path.join(PLOTS_DIR, \"plots_index.json\"), \"w\") as f:\n",
        "    json.dump(plots_index, f, indent=2)\n",
        "\n",
        "print(json.dumps({\"cell_id\": \"7.3\", \"plots_index\": plots_index}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCrT18njRCcm"
      },
      "outputs": [],
      "source": [
        "# cell 7.Z — rapidfuzz: bundle artifacts for download\n",
        "\n",
        "import os, json, time, shutil, glob\n",
        "\n",
        "OUT_DIR = \"outputs/rapidfuzz\"\n",
        "BUNDLES = os.path.join(OUT_DIR, \"bundles\")\n",
        "os.makedirs(BUNDLES, exist_ok=True)\n",
        "\n",
        "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "zip_base = os.path.join(BUNDLES, f\"module7_artifacts_{ts}\")\n",
        "zip_path = shutil.make_archive(zip_base, \"zip\", OUT_DIR)\n",
        "\n",
        "# quick manifest\n",
        "files = sorted([p for p in glob.glob(os.path.join(OUT_DIR, \"**/*\"), recursive=True) if os.path.isfile(p)])\n",
        "manifest = {\n",
        "    \"bundle\": os.path.basename(zip_path),\n",
        "    \"created_at\": ts,\n",
        "    \"file_count\": len(files),\n",
        "    \"files\": [p.replace(OUT_DIR + os.sep, \"\") for p in files]\n",
        "}\n",
        "with open(os.path.join(OUT_DIR, \"bundle_index.json\"), \"w\") as f:\n",
        "    json.dump(manifest, f, indent=2)\n",
        "\n",
        "print({\"cell_id\":\"7.Z\",\"zip\": zip_path, \"file_count\": len(files)})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell N.1 — imports & params (no installs here)\n",
        "import os, json, math, time\n",
        "import numpy as np, pandas as pd\n",
        "# import any heavy libs you need (already installed in 0.0)\n",
        "# e.g., from rapidfuzz import process, fuzz\n",
        "\n",
        "# set module output dirs\n",
        "MOD = \"module-<N>\"\n",
        "OUT_BASE = f\"outputs/{MOD}\"\n",
        "PLOTS = f\"{OUT_BASE}/plots\"\n",
        "os.makedirs(PLOTS, exist_ok=True)\n",
        "\n",
        "# any module-level config (e.g., seeds, thresholds)\n",
        "np.random.seed(int(os.environ.get(\"LSA_SEED\",\"42\")))\n",
        "\n",
        "print(f\"{MOD} ready → outputs under: {OUT_BASE}\")\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# cell N.2 — compute (write Parquet/JSON to OUT_BASE)\n",
        "# ... your module logic ...\n",
        "# pd.DataFrame(...).to_parquet(f\"{OUT_BASE}/<table>.parquet\", index=False)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# cell N.3 — visuals (write PNGs to PLOTS)\n",
        "# ... matplotlib-only plotting that writes files, then plt.close('all')\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# (optional) cell N.Z — bundle just this module\n",
        "# Keep ONE global bundler at the very end if you prefer; otherwise:\n",
        "\"\"\"\n",
        "import zipfile, time, os\n",
        "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "zip_fp = f\"{OUT_BASE}/{MOD}_bundle_{ts}.zip\"\n",
        "with zipfile.ZipFile(zip_fp, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    for base, _, files in os.walk(OUT_BASE):\n",
        "        for fn in files:\n",
        "            if fn.endswith(\".zip\"):  # avoid nesting prior bundles\n",
        "                continue\n",
        "            p = os.path.join(base, fn)\n",
        "            z.write(p, os.path.relpath(p, OUT_BASE))\n",
        "print(\"Created:\", zip_fp)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "HH8LHtIVADKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell Z.ALL — BUNDLE ENTIRE outputs/ AND STOP KERNEL\n",
        "import os, zipfile, time, sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"outputs\")\n",
        "ROOT.mkdir(parents=True, exist_ok=True)\n",
        "ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "zip_fp = ROOT / f\"bundle_all_{ts}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_fp, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    for base, _, files in os.walk(ROOT):\n",
        "        for fn in files:\n",
        "            # don't re-embed older bundles\n",
        "            if fn.startswith(\"bundle_all_\") and fn.endswith(\".zip\"):\n",
        "                continue\n",
        "            p = Path(base) / fn\n",
        "            z.write(p, p.relative_to(ROOT))\n",
        "\n",
        "print(f\"✓ Created: {zip_fp}\")\n",
        "\n",
        "# Colab-friendly download (best-effort)\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "    files.download(str(zip_fp))\n",
        "except Exception:\n",
        "    print(\"Download manually from the file browser if no prompt appeared.\")\n",
        "\n",
        "# clean shutdown to prevent accidental extra execution\n",
        "try:\n",
        "    import IPython\n",
        "    IPython.get_ipython().kernel.do_shutdown(restart=False)\n",
        "except Exception:\n",
        "    sys.exit(0)\n"
      ],
      "metadata": {
        "id": "oVTVEeY6AFla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04453b21"
      },
      "outputs": [],
      "source": [
        "# cell X.Y: Cleanup /content directory (OPTIONAL, MANUAL RUN ONLY)\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def cleanup_content_dir(target_dir: Path = Path(\"/content\")):\n",
        "    \"\"\"\n",
        "    Removes all files and directories from the target_dir except those matching '0[1-4]-*.md'.\n",
        "    Requires user confirmation before proceeding.\n",
        "    \"\"\"\n",
        "    if not target_dir.is_dir():\n",
        "        print(f\"Error: Directory not found: {target_dir}\")\n",
        "        return\n",
        "\n",
        "    items_to_keep = []\n",
        "    items_to_delete = []\n",
        "\n",
        "    # Define the pattern for files to keep\n",
        "    keep_pattern = re.compile(r\"0[1-4]-.*\\.md$\")\n",
        "\n",
        "    for item in target_dir.iterdir():\n",
        "        if item.is_file():\n",
        "            if keep_pattern.match(item.name):\n",
        "                items_to_keep.append(item.name)\n",
        "            else:\n",
        "                items_to_delete.append(item.name)\n",
        "        elif item.is_dir():\n",
        "             # Add directories to delete list, we are not keeping any directories with this logic\n",
        "            items_to_delete.append(item.name)\n",
        "\n",
        "\n",
        "    if not items_to_delete:\n",
        "        print(f\"No files or directories to delete in {target_dir} (excluding files matching 0[1-4]-*.md).\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(items_to_delete)} items to delete in {target_dir}:\")\n",
        "    for name in items_to_delete:\n",
        "        print(f\"- {name}\")\n",
        "\n",
        "    print(\"\\nFiles to keep:\")\n",
        "    for name in items_to_keep:\n",
        "        print(f\"- {name}\")\n",
        "\n",
        "    confirmation = input(\"\\nAre you sure you want to delete these items? Type 'yes' to confirm: \")\n",
        "\n",
        "    if confirmation.lower() == 'yes':\n",
        "        print(\"Deleting items...\")\n",
        "        for name in items_to_delete:\n",
        "            item_path = target_dir / name\n",
        "            try:\n",
        "                if item_path.is_file():\n",
        "                    item_path.unlink()\n",
        "                    print(f\"Deleted file: {name}\")\n",
        "                elif item_path.is_dir():\n",
        "                    import shutil\n",
        "                    shutil.rmtree(item_path)\n",
        "                    print(f\"Deleted directory: {name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error deleting {name}: {e}\")\n",
        "        print(\"Cleanup complete.\")\n",
        "    else:\n",
        "        print(\"Cleanup cancelled.\")\n",
        "\n",
        "# To run this cleanup, execute this cell manually. It will prompt for confirmation.\n",
        "cleanup_content_dir() # Uncomment this line and run the cell to perform cleanup"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}