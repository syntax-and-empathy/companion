---
collected_date: '2025-11-17T18:22:02.426259'
created_date: '2025-11-17T18:11:02.995112'
duration: '[00:08:06]'
language: en
model: medium
source: transcribe
title: 18-02-12.m4a
---

In early September, I set out to make the third generation of the tool for detecting contributions between myself and AI to my articles. It's become less about actually knowing who's writing what, and more of just an ongoing learning experience, if not experiment. Trying a variety of different things, seeing what works, what doesn't. So it's been interesting. Version two was sloppy, so unsurprisingly, it came out messy. I wasn't very serious about it. So version three, I got serious. I made a lexical semantic expert, I think it called it an orchestrator. I made Python engineers and Python code reviewers, both of which were specialized for those specific tasks. So I had the orchestrator put together the entire plan, did the research, brought that back, made a roadmap, spelled out all the technologies, the Python libraries, what informed what. I gave it some criteria that may have contributed to some of the problems we experienced. I really don't know. And so we went off to the races. The first engineer was rocky start, it had a little to go on other than the roadmap. What I was doing was taking the output from, I gave the engineer the information from the orchestrator and I gave the code reviewer all the output from the Python, so it was orchestrator, developer, reviewer. There would be loops between the developer and the reviewer until they both agreed that the code and the feedback was good enough and then it would go into a cell. So we had to get like four, five, I think it was the third module when we were at the point where we had everything loaded and started seeing visualizations. It was pretty rocky. At some point the first engineer was just clearly decaying. The notebook was massive and there were so many rewrites just to start getting it correct that its context window was shot, it had nothing to go on other than that. But what I started doing was I had them giving me feedback after every module and then I would put that into the project in ChatGPT. And if I didn't mention it, ChatGPT was the only thing I used for this. The three roles were in different sessions, that's important. So there was no contamination other than the information that was being passed between them. So first engineer started to decay, so I retired him, made a new one, same prompt, and had it first do a review of the Python and then go through and start fixing it, start looking at the things that were outstanding in the notes but didn't seem serious enough to worry about. And it was a mess. I imagine it fixed some stuff but it seemed like it was doing as much damage as it was good. So I fired that guy, got another engineer going and I put that role that I fired and the first engineer that retired all into the project as well. So now I've got the notes on several modules and two Python engineers and they're varying journeys on this project. So second engineer, I mean it had the two roles to draw from, it had probably four or five modules of information, maybe six or seven along the way, and so it was going really strong. There was a point where I don't remember the details but something particularly nasty happened. There was just some really bad error that was concerning and I didn't want to lose the engineer to it so I spun up another one to investigate. And he was about as useful as the first one for whatever reason despite the fact he had that much more information to go on. I think we finally got past the bump and so I fired him, went back to the other engineer and we kept going. I had to replace one of the code reviewers at some point but then we came up to module six, module seven and that's when it got just problematic. We started having multiple crashes of the Jupyter Notebook kernel, started getting errors from Colab that we had almost consumed all our disk space. I mean just every kind of significant issue I can think of off the top of my head. And so I got to the point where I was worried that I was going to have to start over to disentangle all of the visualizations from the data being created and that wouldn't have saved that much anyway because all the heavy lifting was the data. So I might have had to split the data generation across two notebooks and maybe had the visualizations in the second. But I couldn't just cut them in half or whatever because a lot of the information being generated was used by other things downstream to then do their mathematics and whatnot. And there's just a point where trying to transfer x number of files is sketchy because then you start having, oh they wrote the wrong folder, it's looking for the wrong name because you have them fixing these things and they change stuff like that.

So I set that aside for I don't know a while. I started on it in early September. I think that it was early October that I decided to set that aside. And then the preview release of Claude Code came out for web and I've already published an article about that so I should link to that but everything turned around then. It was really impressive and I guess I have a whole article on that. So I think I'm gonna gonna stop here.
