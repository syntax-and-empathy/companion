Speaker 1: So with the tests on the categories of hallucinations, which is an interesting thing, it's not just hallucinations anymore. We're actually starting to categorize them because they're becoming so common. I used an overly complicated role, admittedly. I had to work on the newest Minion Maker. I mean, it's really new, but after pushing it in ChatGPT 5, who's not doing well, pushing it to make the experiments increasingly complex before employing them with the confabulation test, I really made it go like thirty-four times for complexity before really running them because they were not very complicated.

Speaker 1: With the sycophancy test, I was a little more impatient. I thought that they would phone it in a lot faster than they did, but in the end, basically the same test with three levels of complexity, and everyone caved eventually. First ChatGPT 5, all along Gemini fell in the trap for the second time, and Claude was it for the third. These are not complicated—granted, they are designed to cause the different types of hallucinations—but missing pieces of data, I mean, that's not uncommon. That’s maybe not common, but it’s not something that doesn’t happen when you’re doing real sophisticated research, and when you’re doing research, you’re gathering data. So how’s it gonna do with that, right? Probably not good.

Speaker 1: My experience has been not good at all. I’ve actually had—anyway, I’m not gonna get into that. With the sycophancy test, it got to the point where even in the same session, I could flip it from sycophant to consciously examining the results and, in at least ChatGPT 5’s case, flipping back in the same session and doing it again, which there was not—I mean, all of this was easily in context window for all of these. I did not go over—I can’t believe I went over five thousand tokens, and I’m just really throwing a big number out there. These are short scripts, actually. Reminder to myself: I should actually go and measure the tokens in these sessions, ’cause these are tiny.

Speaker 1: Granted, they’re chatting interfaces, and there’s no raging bull, there’s no contextual—in the context engineering—nothing, right? It’s just not even complicated prompts. So there’s a factor in there being basic prompts, so I think that evens out, because it’s not complicated situations either. I used Sonnet 4 because I didn’t want to give Sonnet 5 that advantage, since it’s the latest and theoretically greatest. It has not been doing me any favors with Claude code, but yeah, Bot is managing to do pretty good, which isn’t surprising given the history. But this is, I think, as bad if not worse when you consider the levels of improvements that are theoretically happening with these models and being tested in all these big fancy tests that don’t really mirror average use cases. I’m sure somebody needs that fancy crap, but not anyone I work with, and I work in a fairly sophisticated company, so that’s concerning.